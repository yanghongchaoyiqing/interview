'use strict';(function(){const indexCfg={cache:true};indexCfg.doc={id:'id',field:['title','content'],store:['title','href'],};const index=FlexSearch.create('balance',indexCfg);window.bookSearchIndex=index;index.add({'id':0,'href':'/interview/docs/architecture/','title':"Architecture",'content':""});index.add({'id':1,'href':'/interview/docs/architecture/concurrent/','title':"Concurrent",'content':""});index.add({'id':2,'href':'/interview/docs/architecture/design/','title':"Design",'content':""});index.add({'id':3,'href':'/interview/docs/architecture/distributed/','title':"Distributed",'content':""});index.add({'id':4,'href':'/interview/docs/basic/','title':"Basic",'content':"计算机基础 "});index.add({'id':5,'href':'/interview/docs/basic/database/','title':"Database",'content':"数据库 "});index.add({'id':6,'href':'/interview/docs/fromwork/','title':"Fromwork",'content':""});index.add({'id':7,'href':'/interview/docs/fromwork/mybatis/','title':"Mybatis",'content':""});index.add({'id':8,'href':'/interview/docs/java/','title':"Java",'content':"Java "});index.add({'id':9,'href':'/interview/docs/java/concurrent/','title':"Concurrent",'content':""});index.add({'id':10,'href':'/interview/docs/java/gc/','title':"Gc",'content':""});index.add({'id':11,'href':'/interview/docs/java/jvm/','title':"Jvm",'content':""});index.add({'id':12,'href':'/interview/docs/leetcode/','title':"Leetcode",'content':"LeetCode 包含头条常见的面试题\n"});index.add({'id':13,'href':'/interview/docs/offer/','title':"Offer",'content':"剑指Offer 包含 剑指Offer 一直 60 道算法题目\n常用技巧  异或运算 删除链表节点时，可通过复制下一个节点的方式减少遍历 保存计算结果来减少重复计算，优化时间效率 分治的思想 滑动窗口 数学建模  "});index.add({'id':14,'href':'/interview/docs/fromwork/spring/aop/','title':"AOP",'content':"AOP AOP 的存在价值 在传统 OOP 编程里以对象为核心，整个软件系统由系列相互依赖的对象所组成，而这些对象将被抽象成一个一个的类，并允许使用类继承来管理类与类之间一般到特殊的关系。随着软件规模的增大，应用的逐渐升级，慢慢出现了一些 OOP 很难解决的问题。\n我们可以通过分析、抽象出一系列具有一定属性与行为的对象，并通过这些对象之间的协作来形成一个完整的软件功能。由于对象可以继承，因此我们可以把具有相同功能或相同特性的属性抽象到一个层次分明的类结构体系中。随着软件规范的不断扩大，专业化分工越来越系列，以及 OOP 应用实践的不断增多，随之也暴露出了一些 OOP 无法很好解决的问题。\n现在假设系统中有 3 段完全相似的代码，这些代码通常会采用“复制”、“粘贴”方式来完成，通过这种“复制”、“粘贴”方式开发出来的软件如图 1 所示。\n看到如上图所示的示意图，可能有的读者已经发现了这种做法的不足之处：如果有一天，上图中的深色代码段需要修改，那是不是要打开 3 个地方的代码进行修改？如果不是 3 个地方包含这段代码，而是 100 个地方，甚至是 1000 个地方包含这段代码段，那会是什么后果？\n为了解决这个问题，我们通常会采用将如上图所示的深色代码部分定义成一个方法，然后在 3 个代码段中分别调用该方法即可。在这种方式下，软件系统的结构如下图所示。\n"});index.add({'id':15,'href':'/interview/docs/java/concurrent/AQS/','title':"AQS",'content':"AQS AQS 提供一个框架，用于实现依赖于先进先出（FIFO）等待队列 的阻塞锁和相关同步器（信号量，事件等）。对于大多数依赖单个原子 int 值表示状态的同步器，该类可以作为十分有用的基类。子类必须定义所有的protected方法（包括tryAcquire、tryRelease），来改变这个状态，并且定义哪些状态代表来对象被使用和被释放。鉴于这些，该类中其他的方法用来实现队列和阻塞的机制。子类可以维护其他状态字段，但是只有使用 getState 、setState以及 compareAndSetState 来原子的操作状态值。\n子类需要定义非 public 的内部工具类用于实现其内部类的同步属性。AbstractQueuedSynchronizer 类不实现任何同步接口，相反，它定义了诸如acquireInterruptibly之类的方法，可以被具体的锁和相关的同步器适当地调用，以实现它们的公共方法。\n该类支持默认的独占模式和共享模式。当一个线程处在独占模式下，其他试图 acquire 的线程都无法成功。共享模式可以同时被多个线程 acquire成功。在具体的应用场景中该类无法理解这些区别，当共享模式 acquire 成功之后，下一个线程（如果有一个存在）必须判定是否能够acquire。线程等待在不同的模式里但是会共享同一个FIFO队列。通常来说，子类只需要支持其中一种模式，但是如果都支持，可以参照ReadWriteLock。子类不需要定义不支持模式的方法。\n该类定义AbstractQueuedSynchronizer.ConditionObject内部类，可以被子类使用的 Condition 实现，来支持独占模式 isHeldExclusively 判定当前线程的同步是否是独占模式，可用通过release方法与 getState 方法来完全释放当前对象，在将保存的状态值调用acquire，最终将此对象恢复到其先前获取的状态。AbstractQueuedSynchronizer没有方法来创建 Condition，所以如果无法满足这个约束，则不要使用它。AbstractQueuedSynchronizer.ConditionObject 的行为与具体的同步器实现有关。\n该类为内部队列提供检查，检测和监视方法，以及 在condition objects上的类似方法。 这些方法可以根据需要使用 AbstractQueuedSynchronizer 用于它们的同步机制。该类的序列化仅存储 atomic int 的状态值，因此反序列化对象的线程队列为空。\n使用 为了使用该类去创建一个同步器，需要重新定义以下方法，并使用 getState, setState, compareAndSetState 方法来改变同步状态。\n tryAcquire tryRelease tryAcquireShared tryReleaseShared isHeldExclusively  上述所有方法默认实现都会抛出 UnsupportedOperationException。这个方法的具体实现必须保证内部的线程安全，并且应该快速并且不会阻塞。所有其他方法均为 final，因为他们不能独立变化。\n也许你发现一些继承自 AbstractOwnableSynchronizer 的方法非常有助于线程保持拥有其独占同步器。同时我们也鼓励使用他们，有助于监控和诊断工具判定哪些线程持有来锁。\nReentrantLock  公平锁相比与非公平锁在 tryAcquire中会多判定一个 hasQueuedPredecessors，如果为 false（队列头为当前线程\u0026ndash;已获取锁 or 队列为空）并且成功修改状态值，则可以认为获取锁成功，这样才是重入，不然加到队尾就会有麻烦。\n ReentrantLock 中通过两个子类 FairSync 和 NoFairSync 继承 AQS 来实现锁。在Lock方法中，直接调用 AQS 的 acquire，acquire会调用 NoFairSync 中的tryAcquire来尝试让当前线程直接获取锁。如果失败则会创建链表节点，将当前线程加入队列，并park。当release方法被调用后，会寻找队列下一个节点进行 unpark，这样他就有机会在acquireQueued中获取锁。\n 公平和非公平就体现在 tryAcquire 方法中，FairSync会判定当前线程是否已获取锁 or 队列为空，在这样的情况下才会尝试获取锁。而NoFairSync会直接来获取锁。\n Condition Condition 因子将 Object monitor 方法（wait, notify and notifyAll）拆分为不同的对象，通过将它们与 Lock 相结合来实现每个对象具有多个等待集的效果。任何 Lock 可以替代 synchronized 关键字的地方，都可以用Condition 来替换Object monitor 方法。\nConditions（也称为 条件队列 或者 条件变量）提供了一种方法 \u0026ndash; 让线程暂停执行，直到其他线程基于某种条件唤醒。在多个线程中访问一些共享的状态信息，是需要进行保护的，所以 Lock 与 Condition 有某种形式的关联。Condition提供的关键属性是它以原子方式释放关联的锁并挂起当前线程，就像Object.wait一样。\nCondition 本质上是绑定到 Lock。可以通过 Lock.newCondition() 来获取一个 Condition 实例。\nCondition 的实现可以提供相比于 Object monitor方法不一样的行为和语义，比如：被通知调起的顺序、在通知时不需要持有锁。如果实现类提供了不一样的语义，必须在文档中进行说明。\nCondition 实例只是普通的对象，可以用在同步语句中，并且有他们自己的 Object monitor的wait和 notification 方法。获取 Condition 对象的 Object monitor 或者使用其 monitor 方法，与Lock 中使用 Condition 的 wait 或者 signal 方法没有任何关系。为了避免混淆，不建议使用 Condition 的 Object monitor 方法，除非在它自己的实现里。\n实现类需要注意  虚假唤醒（spurious wakeup）：开发者最好将条件 wait 方法放在循环中 Condition 有3中 wait 形式（interruptible, non-interruptible, and timed），在不同平台的底层实现可能不同。因此，不需要对三种 wait 定义一致的语义，也不需要支持中断形式的线程暂停。  AbstractQueuedSynchronizer.ConditionObject /** First node of condition queue. */ private transient Node firstWaiter; /** Last node of condition queue. */ private transient Node lastWaiter; 在 ConditionObject 的内部维护了一个队列：条件队列，与 AbstractQueuedSynchronizer 里的 等待队列 不同。\n基本上，把这张图看懂，你也就知道 condition 的处理流程了。\n 条件队列和等待队列的节点，都是 Node 的实例，因为条件队列的节点是需要转移到等待队列中去的； 我们知道一个 ReentrantLock 实例可以通过多次调用 newCondition() 来产生多个 Condition 实例，这里对应 condition1 和 condition2。注意，ConditionObject 只有两个属性 firstWaiter 和 lastWaiter； 每个 condition 有一个关联的条件队列，如线程 1 调用 condition1.await() 方法即可将当前线程 1 包装成 Node 后加入到条件队列中，然后阻塞在这里，不继续往下执行，条件队列是一个单向链表； 调用condition1.signal() 触发一次唤醒，此时唤醒的是队头，会将condition1 对应的条件队列的 firstWaiter（队头） 移到等待队列的队尾，等待获取锁，获取锁后 await 方法才能返回，继续往下执行。  上面的 2-\u0026gt;3-\u0026gt;4 描述了一个最简单的流程，没有考虑中断、signalAll、还有带有超时参数的 await 方法等，不过把这里弄懂是这节的主要目的。\n"});index.add({'id':16,'href':'/interview/docs/java/collection/BlockQueue/','title':"BlockingQueue",'content':"BlockingQueue BlockingQueue 支持当获取队列元素但是队列为空时，会阻塞等待队列中有元素再返回；也支持添加元素时，如果队列已满，那么等到队列可以放入新元素时再放入。\n其提供了4种类型的方法：\n    Throws exception Special value Blocks Times out     Insert add(e) offer(e) put(e) offer(e, time, unit)   Remove remove() poll() take() poll(time, unit)   Examine element() peek() not applicable not applicable    BlockingQueue不接受 null 元素。所有实现应当抛出 NullPointerException 在所有的 add,put以及offer方法上。null被用来标记poll失败。\n在任意时刻，当有界BlockingQueue 队列元素放满之后，所有的元素都将在放入的时候阻塞。无界BlockingQueue 没有任何容量限制，容量大小始终是Integer.MAX_VALUE。\nBlockingQueue的实现是用于 生产者-消费者 的队列，同时也支持 Collection 接口。所以可通过remove(x)来移除队列里的一个元素。通常情况下，这样的操作效率不是很好，只在诸如队列消息被取消的情况下才会偶尔使用。\nBlockingQueue 的实现都是线程安全的。所有 queue 的方法都需要通过内部锁机制或者其他形式来进行并发控制来实现其原子操作。然而，Collection 接口的方法，比如：addAll, containsAll, retainAll 以及 removeAll 都没有必要进行原子操作，除非实现类有特别说明。所以对于addAll(c)有可能在添加部分c元素后抛出异常。\nBlockingQueue 本质上不支持任何的 close 或者 shutdown 操作，来表明不会有新的元素添加。如果需要这些特性，得实现类来支持。\nArrayBlockingQueue ArrayBlockingQueue 是底层由数组存储的有界队列。遵循FIFO，所以在队首的元素是在队列中等待时间最长的，而在队尾的则是最短时间的元素。新元素被插入到队尾，队列的取出 操作队首元素。\n这是一个经典的有界缓存，由一个长度确定的数组持有所有由生产者插入、由消费者取出的元素。一旦创建，整个队列的容量将不会改变。尝试向一个已满的队列 put 将会导致调用被阻塞，同样的向一个空队列 take 也会阻塞。\n该队列支持队等待的生产者和消费者实施可选的公平策略。默认情况下，是非公平策略。可以通过构造函数来指定是否进行公平策略。一般情况下公平策略会减小吞吐量，但是也会降低可变性以及防止饥饿效应。\n实现 ArrayBlockingQueue 内部使用了 ReentrantLock 以及两个 Condition 来实现。\n/** Main lock guarding all access */ final ReentrantLock lock; /** Condition for waiting takes */ private final Condition notEmpty; /** Condition for waiting puts */ private final Condition notFull; PUT 方法也很简单，就是 Condition 的应用。\npublic void put(E e) throws InterruptedException { checkNotNull(e); final ReentrantLock lock = this.lock; lock.lockInterruptibly(); try { //队列已满，wait 在 condition 上 while (count == items.length) notFull.await(); enqueue(e); } finally { lock.unlock(); } } take 方法也同样的。\npublic E take() throws InterruptedException { final ReentrantLock lock = this.lock; lock.lockInterruptibly(); try { //队列为空，wait 在 condition 上 while (count == 0) notEmpty.await(); return dequeue(); } finally { lock.unlock(); } } "});index.add({'id':17,'href':'/interview/docs/java/collection/Concurrenthashmap/','title':"ConcurrentHashmap",'content':"ConcurrentHashmap JDK1.7 ConcurrentHashMap的锁分段技术：假如容器里有多把锁，每一把锁用于锁容器其中一部分数据，那么当多线程访问容器里不同数据段的数据时，线程间就不会存在锁竞争，从而可以有效的提高并发访问效率，这就是ConcurrentHashMap所使用的锁分段技术。首先将数据分成一段一段的存储，然后给每一段数据配一把锁，当一个线程占用锁访问其中一个段数据的时候，其他段的数据也能被其他线程访问。\n ConcurrentHashMap不允许Key或者Value的值为NULL。ConcurrentMaps中不允许空值的主要原因是，在非并发映射中几乎不能容忍的模糊性是无法容纳的。主要的一点是如果map.get（key）返回null，则无法检测 key 是否显式映射为 null 或者 key 未映射。 在非并发映射中，您可以通过 map.contains（key） 进行检查，但在并发映射中，映射可能在调用之间发生了变化。\n Segment类 Put 将一个HashEntry放入到该Segment中，使用自旋机制，减少了加锁的可能性。\nfinal V put(K key, int hash, V value, boolean onlyIfAbsent) { HashEntry\u0026lt;K,V\u0026gt; node = tryLock() ? null : scanAndLockForPut(key, hash, value); //如果加锁失败，则调用该方法 V oldValue; try { HashEntry\u0026lt;K,V\u0026gt;[] tab = table; int index = (tab.length - 1) \u0026amp; hash; //同hashMap相同的哈希定位方式 HashEntry\u0026lt;K,V\u0026gt; first = entryAt(tab, index); for (HashEntry\u0026lt;K,V\u0026gt; e = first;;) { if (e != null) { //若不为null，则持续查找，知道找到key和hash值相同的节点，将其value更新 K k; if ((k = e.key) == key || (e.hash == hash \u0026amp;\u0026amp; key.equals(k))) { oldValue = e.value; if (!onlyIfAbsent) { e.value = value; ++modCount; } break; } e = e.next; } else { //若头结点为null if (node != null) //在遍历key对应节点链时没有找到相应的节点 node.setNext(first); //当前修改并不需要让其他线程知道，在锁退出时修改自然会 //更新到内存中,可提升性能 else node = new HashEntry\u0026lt;K,V\u0026gt;(hash, key, value, first); int c = count + 1; if (c \u0026gt; threshold \u0026amp;\u0026amp; tab.length \u0026lt; MAXIMUM_CAPACITY) rehash(node); //如果超过阈值，则进行rehash操作 else setEntryAt(tab, index, node); ++modCount; count = c; oldValue = null; break; } } } finally { unlock(); } return oldValue; } scanAndLockForPut 该操作持续查找key对应的节点链中是否已存在该节点，如果没有找到已存在的节点，则预创建一个新节点，并且尝试n次，直到尝试次数超出限制，才真正进入等待状态，即所谓的 自旋等待。\nprivate HashEntry\u0026lt;K,V\u0026gt; scanAndLockForPut(K key, int hash, V value) { //根据hash值找到segment中的HashEntry节点 HashEntry\u0026lt;K,V\u0026gt; first = entryForHash(this, hash); //首先获取头结点 HashEntry\u0026lt;K,V\u0026gt; e = first; HashEntry\u0026lt;K,V\u0026gt; node = null; int retries = -1; // negative while locating node while (!tryLock()) { //持续遍历该哈希链 HashEntry\u0026lt;K,V\u0026gt; f; // to recheck first below if (retries \u0026lt; 0) { if (e == null) { if (node == null) //若不存在要插入的节点，则创建一个新的节点 node = new HashEntry\u0026lt;K,V\u0026gt;(hash, key, value, null); retries = 0; } else if (key.equals(e.key)) retries = 0; else e = e.next; } else if (++retries \u0026gt; MAX_SCAN_RETRIES) { //尝试次数超出限制，则进行自旋等待 lock(); break; } /*当在自旋过程中发现节点链的链头发生了变化，则更新节点链的链头， 并重置retries值为－1，重新为尝试获取锁而自旋遍历*/ else if ((retries \u0026amp; 1) == 0 \u0026amp;\u0026amp; (f = entryForHash(this, hash)) != first) { e = first = f; // re-traverse if entry changed retries = -1; } } return node; } remove 用于移除某个节点，返回移除的节点值。\nfinal V remove(Object key, int hash, Object value) { if (!tryLock()) scanAndLock(key, hash); V oldValue = null; try { HashEntry\u0026lt;K,V\u0026gt;[] tab = table; int index = (tab.length - 1) \u0026amp; hash; //根据这种哈希定位方式来定位对应的HashEntry HashEntry\u0026lt;K,V\u0026gt; e = entryAt(tab, index); HashEntry\u0026lt;K,V\u0026gt; pred = null; while (e != null) { K k; HashEntry\u0026lt;K,V\u0026gt; next = e.next; if ((k = e.key) == key || (e.hash == hash \u0026amp;\u0026amp; key.equals(k))) { V v = e.value; if (value == null || value == v || value.equals(v)) { if (pred == null) setEntryAt(tab, index, next); else pred.setNext(next); ++modCount; --count; oldValue = v; } break; } pred = e; e = next; } } finally { unlock(); } return oldValue; } Clear 要首先对整个segment加锁，然后将每一个HashEntry都设置为null。\nfinal void clear() { lock(); try { HashEntry\u0026lt;K,V\u0026gt;[] tab = table; for (int i = 0; i \u0026lt; tab.length ; i++) setEntryAt(tab, i, null); ++modCount; count = 0; } finally { unlock(); } } Put public V put(K key, V value) { Segment\u0026lt;K,V\u0026gt; s; if (value == null) throw new NullPointerException(); int hash = hash(key); //求出key的hash值 int j = (hash \u0026gt;\u0026gt;\u0026gt; segmentShift) \u0026amp; segmentMask; //求出key在segments数组中的哪一个segment中 if ((s = (Segment\u0026lt;K,V\u0026gt;)UNSAFE.getObject (segments, (j \u0026lt;\u0026lt; SSHIFT) + SBASE)) == null) s = ensureSegment(j); //使用unsafe操作取出该segment return s.put(key, hash, value, false); //向segment中put元素 } Get public V get(Object key) { Segment\u0026lt;K,V\u0026gt; s; HashEntry\u0026lt;K,V\u0026gt;[] tab; int h = hash(key); //找出对应的segment的位置 long u = (((h \u0026gt;\u0026gt;\u0026gt; segmentShift) \u0026amp; segmentMask) \u0026lt;\u0026lt; SSHIFT) + SBASE; if ((s = (Segment\u0026lt;K,V\u0026gt;)UNSAFE.getObjectVolatile(segments, u)) != null \u0026amp;\u0026amp; (tab = s.table) != null) { //使用Unsafe获取对应的Segmen for (HashEntry\u0026lt;K,V\u0026gt; e = (HashEntry\u0026lt;K,V\u0026gt;) UNSAFE.getObjectVolatile (tab, ((long)(((tab.length - 1) \u0026amp; h)) \u0026lt;\u0026lt; TSHIFT) + TBASE); e != null; e = e.next) { //找出对应的HashEntry，从头开始遍历 K k; if ((k = e.key) == key || (e.hash == h \u0026amp;\u0026amp; key.equals(k))) return e.value; } } return null; } Size 求出所有的HashEntry的数目，先尝试的遍历查找、计算2遍，如果两遍遍历过程中整个Map没有发生修改（即两次所有Segment实例中modCount值的和一致），则可以认为整个查找、计算过程中Map没有发生改变。否则,需要对所有segment实例进行加锁、计算、解锁，然后返回。\npublic int size() { final Segment\u0026lt;K,V\u0026gt;[] segments = this.segments; int size; boolean overflow; // true if size overflows 32 bits long sum; // sum of modCounts long last = 0L; // previous sum int retries = -1; // first iteration isn't retry try { for (;;) { if (retries++ == RETRIES_BEFORE_LOCK) { for (int j = 0; j \u0026lt; segments.length; ++j) ensureSegment(j).lock(); // force creation } sum = 0L; size = 0; overflow = false; for (int j = 0; j \u0026lt; segments.length; ++j) { Segment\u0026lt;K,V\u0026gt; seg = segmentAt(segments, j); if (seg != null) { sum += seg.modCount; int c = seg.count; if (c \u0026lt; 0 || (size += c) \u0026lt; 0) overflow = true; } } if (sum == last) break; last = sum; } } finally { if (retries \u0026gt; RETRIES_BEFORE_LOCK) { for (int j = 0; j \u0026lt; segments.length; ++j) segmentAt(segments, j).unlock(); } } return overflow ? Integer.MAX_VALUE : size; } JDK1.8 在JDK1.8中对ConcurrentHashmap做了两个改进：\n  取消segments字段，直接采用transient volatile HashEntry\u0026lt;K,V\u0026gt;[] table保存数据，采用table数组元素作为锁，从而实现了对每一行数据进行加锁，进一步减少并发冲突的概率。\n  将原先 table数组＋单向链表 的数据结构，变更为 table数组＋单向链表＋红黑树 的结构。对于 hash 表来说，最核心的能力在于将 key hash 之后能均匀的分布在数组中。如果 hash 之后散列的很均匀，那么 table 数组中的每个队列长度主要为 0 或者 1 。但实际情况并非总是如此理想，虽然 ConcurrentHashMap 类默认的加载因子为 0.75，但是在数据量过大或者运气不佳的情况下，还是会存在一些队列长度过长的情况，如果还是采用单向列表方式，那么查询某个节点的时间复杂度为 $$O(n)$$；因此，对于个数超过 8 (默认值)的链表，jdk1.8 中采用了红黑树的结构，那么查询的时间复杂度可以降低到 $$O(logN)$$，可以改进性能。\n  Put final V putVal(K key, V value, boolean onlyIfAbsent) { if (key == null || value == null) throw new NullPointerException(); // 得到 hash 值 int hash = spread(key.hashCode()); // 用于记录相应链表的长度 int binCount = 0; for (Node\u0026lt;K,V\u0026gt;[] tab = table;;) { Node\u0026lt;K,V\u0026gt; f; int n, i, fh; // 如果数组\u0026quot;空\u0026quot;，进行数组初始化 if (tab == null || (n = tab.length) == 0) // 初始化数组，后面会详细介绍 tab = initTable(); // 找该 hash 值对应的数组下标，得到第一个节点 f else if ((f = tabAt(tab, i = (n - 1) \u0026amp; hash)) == null) { // 如果数组该位置为空， // 用一次 CAS 操作将这个新值放入其中即可，这个 put 操作差不多就结束了，可以拉到最后面了 // 如果 CAS 失败，那就是有并发操作，进到下一个循环就好了 if (casTabAt(tab, i, null, new Node\u0026lt;K,V\u0026gt;(hash, key, value, null))) break; // no lock when adding to empty bin } // hash 居然可以等于 MOVED，这个需要到后面才能看明白，不过从名字上也能猜到，肯定是因为在扩容 else if ((fh = f.hash) == MOVED) // 帮助数据迁移，这个等到看完数据迁移部分的介绍后，再理解这个就很简单了 tab = helpTransfer(tab, f); else { // 到这里就是说，f 是该位置的头结点，而且不为空 V oldVal = null; // 获取数组该位置的头结点的监视器锁 synchronized (f) { if (tabAt(tab, i) == f) { if (fh \u0026gt;= 0) { // 头结点的 hash 值大于 0，说明是链表 // 用于累加，记录链表的长度 binCount = 1; // 遍历链表 for (Node\u0026lt;K,V\u0026gt; e = f;; ++binCount) { K ek; // 如果发现了\u0026quot;相等\u0026quot;的 key，判断是否要进行值覆盖，然后也就可以 break 了 if (e.hash == hash \u0026amp;\u0026amp; ((ek = e.key) == key || (ek != null \u0026amp;\u0026amp; key.equals(ek)))) { oldVal = e.val; if (!onlyIfAbsent) e.val = value; break; } // 到了链表的最末端，将这个新值放到链表的最后面 Node\u0026lt;K,V\u0026gt; pred = e; if ((e = e.next) == null) { pred.next = new Node\u0026lt;K,V\u0026gt;(hash, key, value, null); break; } } } else if (f instanceof TreeBin) { // 红黑树 Node\u0026lt;K,V\u0026gt; p; binCount = 2; // 调用红黑树的插值方法插入新节点 if ((p = ((TreeBin\u0026lt;K,V\u0026gt;)f).putTreeVal(hash, key, value)) != null) { oldVal = p.val; if (!onlyIfAbsent) p.val = value; } } } } if (binCount != 0) { // 判断是否要将链表转换为红黑树，临界值和 HashMap 一样，也是 8 if (binCount \u0026gt;= TREEIFY_THRESHOLD) // 这个方法和 HashMap 中稍微有一点点不同，那就是它不是一定会进行红黑树转换， // 如果当前数组的长度小于 64，那么会选择进行数组扩容，而不是转换为红黑树 // 具体源码我们就不看了，扩容部分后面说 treeifyBin(tab, i); if (oldVal != null) return oldVal; break; } } } // addCount(1L, binCount); return null; } Get  计算 hash 值 根据 hash 值找到数组对应位置: (n - 1) \u0026amp; h 根据该位置处结点性质进行相应查找  如果该位置为 null ，那么直接返回 null 就可以了 如果该位置处的节点刚好就是我们需要的，返回该节点的值即可 如果该位置节点的 hash 值小于 0，说明正在扩容，或者是红黑树，后面我们再介绍 find 方法 如果以上 3 条都不满足，那就是链表，进行遍历比对即可    "});index.add({'id':18,'href':'/interview/docs/java/concurrent/count-down-latch/','title':"CountDownLatch",'content':"CountDownLatch CountDownLatch 是可以使一个或者多个线程等待其他线程完成某些操作的同步器。CountDownLatch 通过一个给定的数字 count 进行初始化。调用 await 方法的线程会一直阻塞到其他线程调用 countDown 将 count 变为0，这时所有的线程都将释放，并且后续的 await 方法调用都会立即返回。count 值不能重置。如果你需要重置 count 请考虑使用 CyclicBarrier。\nCountDownLatch 是一个能力很强的同步工具，可以用在多种途径。CountDownLatch 最重要的属性是其不要求 调用 countDown 的线程等待到 count 为0，只是要求所有 await 调用线程等待。\nCountDownLatch 内部使用的是 AQS，AQS 里面的 state 是一个整数值，这边用一个 int count 参数其实初始化就是设置了这个值，所有调用了 await 方法的等待线程会挂起，然后有其他一些线程会做 state = state - 1 操作，当 state 减到 0 的同时，那个将 state 减为 0 的线程会负责唤醒 所有调用了 await 方法的线程。\n countDown() 方法每次调用都会将 state 减 1，直到 state 的值为 0；而 await 是一个阻塞方法，当 state 减为 0 的时候，await 方法才会返回。await 可以被多个线程调用，读者这个时候脑子里要有个图：所有调用了 await 方法的线程阻塞在 AQS 的阻塞队列中，等待条件满足（state == 0），将线程从队列中一个个唤醒过来。 await() 方法，它代表线程阻塞，等待 state 的值减为 0。  "});index.add({'id':19,'href':'/interview/docs/architecture/distributed/dubbo/','title':"Dubbo",'content':"Dubbo 领域模型 在 Dubbo 的核心领域模型中：\n Protocol 是服务域，它是 Invoker 暴露和引用的主功能入口，它负责 Invoker 的生命周期管理。 Invoker 是实体域，它是 Dubbo 的核心模型，其它模型都向它靠扰，或转换成它，它代表一个可执行体，可向它发起 invoke 调用，它有可能是一个本地的实现，也可能是一个远程的实现，也可能一个集群实现。 Invocation 是会话域，它持有调用过程中的变量，比如方法名，参数等。  基本设计原则  采用 Microkernel + Plugin 模式，Microkernel 只负责组装 Plugin，Dubbo 自身的功能也是通过扩展点实现的，也就是 Dubbo 的所有功能点都可被用户自定义扩展所替换。 采用 URL 作为配置信息的统一格式，所有扩展点都通过传递 URL 携带配置信息。  Dubbo 服务暴露过程 官方文档\u0026ndash;服务导出\nDubbo 结构  第一层：service 层，接口层，给服务提供者和消费者来实现的 第二层：config 层，配置层，主要是对 dubbo 进行各种配置的 第三层：proxy 层，服务代理层，无论是 consumer 还是 provider，dubbo 都会给你生成代理，代理之间进行网络通信 第四层：registry 层，服务注册层，负责服务的注册与发现 第五层：cluster 层，集群层，封装多个服务提供者的路由以及负载均衡，将多个实例组合成一个服务 第六层：monitor 层，监控层，对 rpc 接口的调用次数和调用时间进行监控 第七层：protocal 层，远程调用层，封装 rpc 调用 第八层：exchange 层，信息交换层，封装请求响应模式，同步转异步 第九层：transport 层，网络传输层，抽象 mina 和 netty 为统一接口 第十层：serialize 层，数据序列化层  工作流程  第一步：provider 向注册中心去注册 第二步：consumer 从注册中心订阅服务，注册中心会通知 consumer 注册好的服务 第三步：consumer 调用 provider 第四步：consumer 和 provider 都异步通知监控中心  注册中心挂了可以继续通信吗？ 可以，因为刚开始初始化的时候，消费者会将提供者的地址等信息拉取到 本地缓存，所以注册中心挂了可以继续通信。\nDubbo 支持哪些序列化协议？说一下 Hessian 的数据结构？PB 知道吗？为什么 PB 的效率是最高的？ Dubbo 支持不同的通信协议  dubbo 协议：默认就是走 dubbo 协议，单一长连接，进行的是 NIO 异步通信，基于 hessian 作为序列化协议。使用的场景是：传输数据量小（每次请求在 100kb 以内），但是并发量很高。 rmi 协议：走 Java 二进制序列化，多个短连接，适合消费者和提供者数量差不多的情况，适用于文件的传输，一般较少用。 hessian 协议：走 hessian 序列化协议，多个短连接，适用于提供者数量比消费者数量还多的情况，适用于文件的传输，一般较少用。 http 协议：走 json 序列化 webservice：走 SOAP 文本序列化  Dubbo 支持的序列化协议 dubbo 支持 hession 、 Java 二进制序列化、json、SOAP 文本序列化多种序列化协议。但是 hessian 是其默认的序列化协议。\n为什么 PB 的效率是最高的？ 其实 PB 之所以性能如此好，主要得益于两个：\n 它使用 proto 编译器，自动进行序列化和反序列化，速度非常快，应该比 XML 和 JSON 快上了 20~100 倍； 它的数据压缩效果好，就是说它序列化后的数据量体积小。因为体积小，传输起来带宽和速度上会有优化。  dubbo 负载均衡策略和集群容错策略都有哪些？动态代理策略呢？ dubbo 负载均衡策略 random loadbalance 默认情况下，dubbo 是 random load balance ，即 随机 调用实现负载均衡，可以对 provider 不同实例 设置不同的权重，会按照权重来负载均衡，权重越大分配流量越高，一般就用这个默认的就可以了。\nroundrobin loadbalance 这个的话默认就是均匀地将流量打到各个机器上去，但是如果各个机器的性能不一样，容易导致性能差的机器负载过高。所以此时需要调整权重，让性能差的机器承载权重小一些，流量少一些。\nleastactive loadbalance 这个就是自动感知一下，如果某个机器性能越差，那么接收的请求越少，越不活跃，此时就会给 不活跃的性能差的机器更少的请求。\nconsistanthash loadbalance 一致性 Hash 算法，相同参数的请求一定分发到一个 provider 上去， provider 挂掉的时候，会基于虚拟节点均匀分配剩余的流量，抖动不会太大。如果你需要的不是随机负载均衡，是要一类请求都到一个节点，那就走这个一致性 Hash 策略。\ndubbo 集群容错策略 failover cluster 模式 失败自动切换，自动重试其他机器，默认就是这个，常见于读操作。（失败重试其它机器）\nfailfast cluster模式 一次调用失败就立即失败，常见于写操作。（调用失败就立即失败）\nfailsafe cluster 模式 出现异常时忽略掉，常用于不重要的接口调用，比如记录日志。\nfailback cluster 模式 失败了后台自动记录请求，然后定时重发，比较适合于写消息队列这种。\nforking cluster 模式 并行调用 多个 provider ，只要一个成功就立即返回。\nbroadcacst cluster 逐个调用所有的 provider。\ndubbo动态代理策略 默认使用 javassist 动态字节码生成，创建代理类。但是可以通过 spi 扩展机制配置自己的动态代理策略。\ndubbo 的 spi 思想是什么？ spi ，简单来说，就是 service provider interface，说白了是什么意思呢，比如你有个接口，现在这个接口有 3 个实现类，那么在系统运行的时候对这个接口到底选择哪个实现类呢？这就需要 spi 了，需要根据指定的配置或者是默认的配置，去找到对应的实现类加载进来，然后用这个实现类的实例对象。\ndubbo 也用了 spi 思想，不过没有用 jdk 的 spi 机制，是自己实现的一套 spi 机制。\nProtocol protocol = ExtensionLoader.getExtensionLoader(Protocol.class).getAdaptiveExtension(); Protocol 接口，在系统运行的时候， dubbo 会判断一下应该选用这个 Protocol 接口的哪个实现类来实例化对象来使用。\n它会去找一个你配置的 Protocol ，将你配置的 Protocol 实现类，加载到 jvm 中来，然后实例化对象，就用你的那个 Protocol 实现类就可以了。\n如何基于 Dubbo 进行服务治理、服务降级、失败重试以及超时重试？ 服务治理 1. 调用链路自动生成 一个大型的分布式系统，或者说是用现在流行的微服务架构来说吧，分布式系统由大量的服务组成。那么这些服务之间互相是如何调用的？调用链路是啥？说实话，几乎到后面没人搞的清楚了，因为服务实在太多了，可能几百个甚至几千个服务。\n那就需要基于 dubbo 做的分布式系统中，对各个服务之间的调用自动记录下来，然后自动将 各个服务之间的依赖关系和调用链路生成出来，做成一张图，显示出来，大家才可以看到对吧。\n2. 服务访问压力以及时长统计 需要自动统计 各个接口和服务之间的调用次数以及访问延时，而且要分成两个级别。\n 一个级别是接口粒度，就是每个服务的每个接口每天被调用多少次，TP50/TP90/TP99，三个档次的请求延时分别是多少； 第二个级别是从源头入口开始，一个完整的请求链路经过几十个服务之后，完成一次请求，每天全链路走多少次，全链路请求延时的 TP50/TP90/TP99，分别是多少。  这些东西都搞定了之后，后面才可以来看当前系统的压力主要在哪里，如何来扩容和优化啊。\n3. 其它  服务分层（避免循环依赖） 调用链路失败监控和报警 服务鉴权 每个服务的可用性的监控（接口调用成功率？几个 9？99.99%，99.9%，99%）  服务降级 比如说服务 A 调用服务 B，结果服务 B 挂掉了，服务 A 重试几次调用服务 B，还是不行，那么直接降级，走一个备用的逻辑，给用户返回响应。\n举个栗子，我们有接口 HelloService。HelloServiceImpl 有该接口的具体实现。\npublic interface HelloService { void sayHello(); } public class HelloServiceImpl implements HelloService { public void sayHello() { System.out.println(\u0026quot;hello world......\u0026quot;); } } \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;beans xmlns=\u0026#34;http://www.springframework.org/schema/beans\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xmlns:dubbo=\u0026#34;http://code.alibabatech.com/schema/dubbo\u0026#34; xsi:schemaLocation=\u0026#34;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://code.alibabatech.com/schema/dubbo http://code.alibabatech.com/schema/dubbo/dubbo.xsd\u0026#34;\u0026gt; \u0026lt;dubbo:application name=\u0026#34;dubbo-provider\u0026#34; /\u0026gt; \u0026lt;dubbo:registry address=\u0026#34;zookeeper://127.0.0.1:2181\u0026#34; /\u0026gt; \u0026lt;dubbo:protocol name=\u0026#34;dubbo\u0026#34; port=\u0026#34;20880\u0026#34; /\u0026gt; \u0026lt;dubbo:service interface=\u0026#34;com.zhss.service.HelloService\u0026#34; ref=\u0026#34;helloServiceImpl\u0026#34; timeout=\u0026#34;10000\u0026#34; /\u0026gt; \u0026lt;bean id=\u0026#34;helloServiceImpl\u0026#34; class=\u0026#34;com.zhss.service.HelloServiceImpl\u0026#34; /\u0026gt; \u0026lt;/beans\u0026gt; \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;beans xmlns=\u0026#34;http://www.springframework.org/schema/beans\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xmlns:dubbo=\u0026#34;http://code.alibabatech.com/schema/dubbo\u0026#34; xsi:schemaLocation=\u0026#34;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://code.alibabatech.com/schema/dubbo http://code.alibabatech.com/schema/dubbo/dubbo.xsd\u0026#34;\u0026gt; \u0026lt;dubbo:application name=\u0026#34;dubbo-consumer\u0026#34; /\u0026gt; \u0026lt;dubbo:registry address=\u0026#34;zookeeper://127.0.0.1:2181\u0026#34; /\u0026gt; \u0026lt;dubbo:reference id=\u0026#34;fooService\u0026#34; interface=\u0026#34;com.test.service.FooService\u0026#34; timeout=\u0026#34;10000\u0026#34; check=\u0026#34;false\u0026#34; mock=\u0026#34;return null\u0026#34;\u0026gt; \u0026lt;/dubbo:reference\u0026gt; \u0026lt;/beans\u0026gt; 我们调用接口失败的时候，可以通过 mock 统一返回 null 。\nmock 的值也可以修改为 true，然后再跟接口同一个路径下实现一个 Mock 类，命名规则是 “接口名称+Mock” 后缀。然后在 Mock 类里实现自己的降级逻辑。\npublic class HelloServiceMock implements HelloService { public void sayHello() { // 降级逻辑 } } 失败重试和超时重试 所谓失败重试，就是 consumer 调用 provider 要是失败了，比如抛异常了，此时应该是可以重试的，或者调用超时了也可以重试。配置如下：\n\u0026lt;dubbo:reference id=\u0026#34;xxxx\u0026#34; interface=\u0026#34;xx\u0026#34; check=\u0026#34;true\u0026#34; async=\u0026#34;false\u0026#34; retries=\u0026#34;3\u0026#34; timeout=\u0026#34;2000\u0026#34;/\u0026gt; 参考链接 advanced-java\n"});index.add({'id':20,'href':'/interview/docs/java/collection/HashMap/','title':"HashMap",'content':"HashMap 在get和put的过程中，计算下标时，先对hashCode进行hash操作，然后再通过hash值进一步计算下标，如下图所示：\n在对hashCode()计算hash时具体实现是这样的：\nstatic final int hash(Object key) { int h; return (key == null) ? 0 : (h = key.hashCode()) ^ (h \u0026gt;\u0026gt;\u0026gt; 16); }  可以看到这个函数大概的作用就是：高16bit不变，低16bit和高16bit做了一个异或。\n 在设计hash函数时，因为目前的table长度n为2的幂，而计算下标的时候，是这样实现的(使用\u0026amp;位操作，而非%求余)。设计者认为这方法很容易发生碰撞。为什么这么说呢？不妨思考一下，在n - 1为15(0x1111)时，其实散列真正生效的只是低4bit的有效位，当然容易碰撞了。\n因此，设计者想了一个顾全大局的方法(综合考虑了速度、作用、质量)，就是把高16bit和低16bit异或了一下。设计者还解释到因为现在大多数的hashCode的分布已经很不错了，就算是发生了碰撞也用O(logn)的tree去做了。仅仅异或一下，既减少了系统的开销，也不会造成的因为高位没有参与下标的计算(table长度比较小时)，从而引起的碰撞。\n如果还是产生了频繁的碰撞，会发生什么问题呢？作者注释说，他们使用树来处理频繁的碰撞(we use trees to handle large sets of collisions in bins)，在JEP-180中，描述了这个问题：\n Improve the performance of java.util.HashMap under high hash-collision conditions by using balanced trees rather than linked lists to store map entries. Implement the same improvement in the LinkedHashMap class.\n 之前已经提过，在获取HashMap的元素时，基本分两步：\n 首先根据 hashCode() 做 hash ，然后确定 bucket 的 index ； 如果 bucket 的节点的 key 不是我们需要的，则通过 keys.equals() 在链中找。  在Java 8之前的实现中是用链表解决冲突的，在产生碰撞的情况下，进行 get 时，两步的时间复杂度是 $$O(1)+O(n)$$。因此，当碰撞很厉害的时候n很大， $$O(n)$$ 的速度显然是影响速度的。因此在Java 8中，如果一个 bucket 中碰撞冲突的元素超过某个限制(默认是8)，则使用红黑树来替换链表，这样复杂度就变成了 $$O(1)+O(logn)$$ 了，这样在 n 很大的时候，能够比较理想的解决这个问题，在Java 8：HashMap的性能提升一文中有性能测试的结果。\nResize 当put时，如果发现目前的bucket占用程度已经超过了Load Factor所希望的比例，那么就会发生resize。在resize的过程，简单的说就是把bucket扩充为2倍，之后重新计算index，把节点再放到新的bucket中。resize的注释是这样描述的：\n Initializes or doubles table size. If null, allocates in accord with initial capacity target held in field threshold. Otherwise, because we are using power-of-two expansion, the elements from each bin must either stay at same index, or move with a power of two offset in the new table.\n 大致意思就是说，当超过限制的时候会resize，然而又因为我们使用的是2次幂的扩展(指长度扩为原来2倍)，所以，元素的位置要么是在原位置，要么是在原位置再移动2次幂的位置。例如我们从16扩展为32时，具体的变化如下所示：\n因此元素在重新计算 hash 之后，因为n变为2倍，那么 n-1 的 mask 范围在高位多1bit(红色)，因此新的index就会发生这样的变化：\n因此，我们在扩充HashMap的时候，不需要重新计算hash，只需要看看原来的hash值新增的那个bit是1还是0就好了，是0的话索引没变，是1的话索引变成“原索引+oldCap”。可以看看下图为16扩充为32的resize示意图：\n这个设计确实非常的巧妙，既省去了重新计算hash值的时间，而且同时，由于新增的1bit是0还是1可以认为是随机的，因此resize的过程，均匀的把之前的冲突的节点分散到新的bucket了。\n并发问题 疫苗：JAVA HASHMAP的死循环\n在 HashMap 并发进行 Resize 的过程中会出现环形链表，导致 get() 操作死循环。\n"});index.add({'id':21,'href':'/interview/docs/basic/net/http/','title':"HTTP",'content':"HTTP  HTTP构建于TCP/IP协议之上，默认端口号是80。 HTTP是 无连接无状态 的。  无连接的含义是 限制每次连接只处理一个请求。服务器处理完客户的请求，并收到客户的应答后，即断开连接。后来使用了Keep-Alive技术。\n无状态是指 协议对于事务处理没有记忆能力，服务器不知道客户端是什么状态。即我们给服务器发送 HTTP 请求之后，服务器根据请求，会给我们发送数据过来，但是，发送完，不会记录任何信息。\nHTTP 协议这种特性有优点也有缺点，优点在于解放了服务器，每一次请求“点到为止”不会造成不必要连接占用，缺点在于每次请求会传输大量重复的内容信息。\n为了解决HTTP无状态的缺点，两种用于保持 HTTP 连接状态的技术就应运而生了，一个是 Cookie，而另一个则是 Session。Cookie在客户端记录状态，比如登录状态。Session在服务器记录状态。\nHttp的报文结构 HTTP 请求报文头部  User-Agent：产生请求的浏览器类型。 Accept：客户端可识别的响应内容类型列表; Accept-Language：客户端可接受的自然语言; Accept-Encoding：客户端可接受的编码压缩格式; Accept-Charset：可接受的应答的字符集; Host：请求的主机名，允许多个域名同处一个IP 地址，即虚拟主机;（必选） Connection：连接方式(close 或 keep-alive); Cookie：存储于客户端扩展字段，向同一域名的服务端发送属于该域的cookie; 请求包体：在POST方法中使用。 Referer：包含一个URL，用户从该URL代表的页面出发访问当前请求的页面。 If-Modified-Since：文档的最后改动时间  HTTP 响应头  Allow\t服务器支持哪些请求方法（如GET、POST等）。 Content-Encoding\t文档的编码（Encode）方法。 Content-Length\t表示内容长度。只有当浏览器使用持久HTTP连接时才需要这个数据。 Content-Type\t表示后面的文档属于什么MIME类型。 Date\t当前的GMT时间。你可以用setDateHeader来设置这个头以避免转换时间格式的麻烦。 Expires\t应该在什么时候认为文档已经过期，从而不再缓存它。 Last-Modified\t文档的最后改动时间。 Refresh\t表示浏览器应该在多少时间之后刷新文档，以秒计。 Server\t服务器名字。 Set-Cookie\t设置和页面关联的Cookie。 ETag：被请求变量的实体值。ETag是一个可以与Web资源关联的记号（MD5值）。 Cache-Control：这个字段用于指定所有缓存机制在整个请求/响应链中必须服从的指令。   max-age：表示当访问此网页后的 x 秒内再次访问不会去服务器；no-cache，实际上Cache-Control: no-cache是会被缓存的，只不过每次在向客户端（浏览器）提供响应数据时，缓存都要向服务器评估缓存响应的有效性；no-store，这个才是响应不被缓存的意思；\n  Last-Modified与If-Modified-Since都是用来记录页面的最后修改时间。当客户端访问页面时，服务器会将页面最后修改时间通过 Last-Modified 标识由服务器发往客户端，客户端记录修改时间，再次请求本地存在的cache页面时，客户端会通过 If-Modified-Since 头将先前服务器端发过来的最后修改时间戳发送回去，服务器端通过这个时间戳判断客户端的页面是否是最新的，如果不是最新的，则返回新的内容，如果是最新的，则返回 304。\n Http的状态码含义。  1**\t信息，服务器收到请求，需要请求者继续执行操作 2**\t成功，操作被成功接收并处理 3**\t重定向，需要进一步的操作以完成请求  301 Moved Permanently。请求的资源已被永久的移动到新URI，返回信息会包括新的URI，浏览器会自动定向到新URI。今后任何新的请求都应使用新的URI代替 302 Moved Temporarily。与301类似。但资源只是临时被移动。客户端应继续使用原有URI 304 Not Modified。所请求的资源未修改，服务器返回此状态码时，不会返回任何资源。客户端通常会缓存访问过的资源，通过提供一个头信息指出客户端希望只返回在指定日期之后修改的资源。   4**\t客户端错误，请求包含语法错误或无法完成请求  400 Bad Request 由于客户端请求有语法错误，不能被服务器所理解。 401 Unauthorized 请求未经授权。这个状态代码必须和WWW-Authenticate报头域一起使用 403 Forbidden 服务器收到请求，但是拒绝提供服务。服务器通常会在响应正文中给出不提供服务的原因 404 Not Found 请求的资源不存在，例如，输入了错误的URL   5**\t服务器错误，服务器在处理请求的过程中发生了错误  500 Internal Server Error 服务器发生不可预期的错误，导致无法完成客户端的请求。 503 Service Unavailable 服务器当前不能够处理客户端的请求，在一段时间之后，服务器可能会恢复正常。    Http request的几种类型。  GET\t请求指定的页面信息，并返回实体主体。 POST\t向指定资源提交数据进行处理请求（例如提交表单或者上传文件）。数据被包含在请求体中。POST请求可能会导致新的资源的建立和/或已有资源的修改。 PUT\t从客户端向服务器传送的数据取代指定的文档的内容。 DELETE\t请求服务器删除指定的页面。   GET可提交的数据量受到URL长度的限制，HTTP协议规范没有对URL长度进行限制。这个限制是特定的浏览器及服务器对它的限制\n  理论上讲，POST是没有大小限制的，HTTP协议规范也没有进行大小限制，出于安全考虑，服务器软件在实现时会做一定限制\n 条件 GET HTTP条件GET 是 HTTP 协议为了减少不必要的带宽浪费，提出的一种方案。实际上就是利用If-Modified-Since做浏览器缓存。\n持久连接 我们知道 HTTP 协议采用请求-应答模式，当使用普通模式，即非 Keep-Alive 模式时，每个请求/应答客户和服务器都要新建一个连接，完成之后立即断开连接（HTTP协议为无连接的协议）；当使用 Keep-Alive 模式（又称持久连接、连接重用）时，Keep-Alive 功能使客户端到服务器端的连接持续有效，当出现对服务器的后继请求时，Keep-Alive 功能避免了建立或者重新建立连接。\n在 HTTP 1.0 中, 没有官方的 keep alive 的操作。通常是在现有协议上添加一个指数。如果浏览器支持 keep-alive，它会在请求的包头中添加：\nConnection: Keep-Alive 然后当服务器收到请求，作出回应的时候，它也添加一个头在响应中：\nConnection: Keep-Alive 这样做，连接就不会中断（超过 Keep-Alive 规定的时间\u0026ndash;服务器设置，意外断电等情况除外），而是保持连接。当客户端发送另一个请求时，它会使用同一个连接。这一直继续到客户端或服务器端认为会话已经结束，其中一方中断连接。\n在 HTTP 1.1 版本中，默认情况下所有连接都被保持，如果加入 \u0026ldquo;Connection: close\u0026rdquo; 才关闭。\n HTTP Keep-Alive 简单说就是保持当前的TCP连接，避免了重新建立连接。\n  HTTP 长连接不可能一直保持，例如 Keep-Alive: timeout=5, max=100，表示这个TCP通道可以保持5秒，max=100，表示这个长连接最多接收100次请求就断开。\n  HTTP是一个无状态协议，这意味着每个请求都是独立的，Keep-Alive没能改变这个结果。另外，Keep-Alive也不能保证客户端和服务器之间的连接一定是活跃的，在HTTP1.1版本中也如此。唯一能保证的就是当连接被关闭时你能得到一个通知，所以不应该让程序依赖于Keep-Alive的保持连接特性，否则会有意想不到的后果。\n  使用长连接之后，客户端、服务端怎么知道本次传输结束呢？两部分：1. 判断传输数据是否达到了Content-Length 指示的大小；2. 动态生成的文件没有 Content-Length ，它是分块传输（chunked），这时候就要根据 chunked 编码来判断，chunked 编码的数据在最后有一个空 chunked 块，表明本次传输数据结束。\n 跨站攻击 CSRF（Cross-site request forgery，跨站请求伪造）伪造请求，冒充用户在站内的正常操作，比如爬虫。\n防范的方法  关键操作只接受POST请求 验证码 检测 Referer Token  Token 要足够随机——只有这样才算不可预测 Token 是一次性的，即每次请求成功后要更新Token——这样可以增加攻击难度，增加预测难度 Token 要注意保密性——敏感操作使用 post，防止 Token 出现在 URL 中    断点续传 要实现断点续传的功能，通常都需要客户端记录下当前的下载进度，并在需要续传的时候通知服务端本次需要下载的内容片段。\nHTTP1.1协议中定义了断点续传相关的HTTP头 Range 和 Content-Range 字段，一个最简单的断点续传实现大概如下：\n 客户端下载一个1024K的文件，已经下载了其中512K 网络中断，客户端请求续传，因此需要在HTTP头中申明本次需要续传的片段：Range:bytes=512000-，这个头通知服务端从文件的512K位置开始传输文件。 服务端收到断点续传请求，从文件的512K位置开始传输，并且在HTTP头中增加：Content-Range:bytes 512000-/1024000，并且此时服务端返回的HTTP状态码应该是206，而不是200。  但是在实际场景中，会出现一种情况，即在终端发起续传请求时，URL对应的文件内容在服务端已经发生变化，此时续传的数据肯定是错误的。如何解决这个问题了？显然此时我们需要有一个标识文件唯一性的方法。在RFC2616中也有相应的定义，比如 实现Last-Modified来标识文件的最后修改时间，这样即可判断出续传文件时是否已经发生过改动。同时RFC2616中还定义有一个ETag的头，可以使用ETag头来放置文件的唯一标识，比如文件的MD5值。\n客户端在发起续传请求时应该在HTTP头中申明If-Match 或者 If-Modified-Since 字段，帮助服务端判别文件变化。\n一次HTTP请求  域名解析 1. 浏览器缓存 2. 系统缓存 3. hosts 4. ISP DNS 缓存 5. DNS 服务器搜索 浏览器发送 HTTP 请求到目标服务器 服务器永久重定向 浏览器跟踪重定向地址 服务器“处理”请求 服务器发回一个HTML响应 浏览器开始显示HTML 浏览器请求获取嵌入在 HTML 中的对象（图片\u0026amp;脚本等） 浏览器发送异步（AJAX）请求  "});index.add({'id':22,'href':'/interview/docs/basic/net/https/','title':"HTTPS",'content':"HTTPS HTTPS 是一种通过计算机网络进行安全通信的传输协议。HTTPS经由HTTP进行通信，但利用 SSL/TLS 来加密数据包。 HTTPS 开发的主要目的，是提供对网站服务器的身份认证，保护交换数据的隐私与完整性。\nHTTPS 的主要思想是在不安全的网络上创建一安全信道，并可在使用适当的加密包和服务器证书可被验证且可被信任时，对窃听和中间人攻击提供合理的防护。HTTPS的信任继承基于预先安装在浏览器中的证书颁发机构（如Symantec、Comodo、GoDaddy和GlobalSign等）（意即“我信任证书颁发机构告诉我应该信任的”）\nHTTP 为什么不安全 http 协议属于 明文传输协议 ，交互过程以及数据传输都没有进行加密，通信双方也没有进行任何认证，通信过程非常容易遭遇劫持、监听、篡改，严重情况下，会造成恶意的流量劫持等问题，甚至造成个人隐私泄露（比如银行卡卡号和密码泄露）等严重的安全问题。\n比如常见的，在 http 通信过程中，“中间人”将广告链接嵌入到服务器发给用户的 http 报文里，导致用户界面出现很多不良链接； 或者是修改用户的请求头 URL ，导致用户的请求被劫持到另外一个网站，用户的请求永远到不了真正的服务器。这些都会导致用户得不到正确的服务，甚至是损失惨重。\nHTTPS 如何保证安全 要解决 http 带来的问题，就要引入加密以及身份验证机制。\n数字证书 服务器首先生成公私钥，将公钥提供给相关机构（CA），CA 将公钥放入数字证书并将数字证书颁布给服务器，此时服务器就不是简单的把公钥给客户端，而是给客户端一个数字证书，数字证书中加入了一些数字签名的机制，保证了数字证书一定是服务器给客户端的。中间人发送的伪造证书，不能够获得 CA 的认证，此时，客户端和服务器就知道通信被劫持了。\n证书由 公钥、证书主体、数字签名 等内容组成。在客户端发起 SSL 请求后，服务端会将数字证书发给客户端，客户端会对证书进行验证（验证这张证书是否是伪造的？也就是公钥是否是伪造的），如果证书不是伪造的，客户端就获取用于对称密钥交换的非对称密钥（获取公钥）\n数字证书有三个作用：  身份授权：确保浏览器访问的网站是经过CA验证的可信任的网站。 分发公钥：每个数字证书都包含了注册者生成的公钥（验证确保是合法的，非伪造的公钥）。在 SSL 握手时会通过 certificate 消息传输给客户端。 验证证书合法性：客户端接收到数字证书后，会对证书合法性进行验证。只有验证通过后的证书，才能够进行后续通信过程。  证书的认证   信任：浏览器内置了信任的根证书，就是看看web服务器的证书是不是这些信任根发的或者信任根的二级证书机构颁发的。 \u0026gt; 对方是不是上述证书的合法持有者。简单来说证明对方是否持有证书的对应私钥。验证方法两种，一种是对方签个名，我用证书验证签名；另外一种是用证书做个信封，看对方是否能解开。\n  有效，就是看看web服务器证书是否在有效期，是否被吊销了。 \u0026gt; 验证正式是否吊销可以采用黑名单方式或者OCSP方式。黑名单就是定期从CA下载一个名单列表，里面有吊销的证书序列号，自己在本地比对一下就行。优点是效率高。缺点是不实时。OCSP是实时连接CA去验证，优点是实时，缺点是效率不高。\n  怎样避免第三方伪造这个证书？答案就是数字签名（ digital signature ）。数字签名是证书的防伪标签，目前使用最广泛的 SHA-RSA （SHA用于哈希算法，RSA用于非对称加密算法）。数字签名的制作和验证过程如下：\n1. 数字签名的签发：首先是使用哈希函数对证书内容进行安全哈希，生成消息摘要，然后使用CA自己的私钥对消息摘要进行加密。 2. 数字签名的校验：使用 CA 的公钥和证书里的解密算法解密签名，根据证书的摘要算法计算证书摘要信息，并进行比较，如果相同就认为校验成功。  需要注意的是：\n1. 数字签名签发和校验使用的非对称密钥是CA自己的公钥和私钥，跟证书申请者（提交证书申请的公司实体）提交的公钥没有任何关系。 2. 数字签名的签发过程跟公钥加密的过程刚好相反，即是用私钥加密，公钥解密。（一对公钥和私钥，公钥加密的内容只有私钥能够解密；反过来，私钥加密的内容，也就有公钥才能够解密） 3. 现在大的CA都会有证书链，证书链的好处：首先是安全，保持CA的私钥离线使用。第二个好处是方便部署和撤销。这里为啥要撤销呢？因为，如果CA数字证书出现问题（被篡改或者污染），只需要撤销相应级别的证书，根证书依然是安全的。 4. 根CA证书都是自签名，即用自己的公钥和私钥完成了签名的制作和验证。而证书链上的证书签名都是使用上一级证书的非对称密钥进行签名和验证的。 5. 怎样获取根CA和多级CA的密钥对？还有，既然是自签名和自认证，那么它们是否安全可信？这里的答案是：当然可信，因为这些厂商跟浏览器和操作系统都有合作，它们的根公钥都默认装到了浏览器或者操作系统环境里。  SSL/TLS协议 不使用SSL/TLS的HTTP通信，就是不加密的通信。所有信息明文传播，带来了三大风险。\n（1） 窃听风险（eavesdropping）：第三方可以获知通信内容。 （2） 篡改风险（tampering）：第三方可以修改通信内容。 （3） 冒充风险（pretending）：第三方可以冒充他人身份参与通信。 SSL/TLS协议是为了解决这三大风险而设计的，希望达到：\n（1） 所有信息都是加密传播，第三方无法窃听。 （2） 具有校验机制，一旦被篡改，通信双方会立刻发现。 （3） 配备身份证书，防止身份被冒充。 目前，应用最广泛的是 TLS 1.0，接下来是SSL 3.0。但是，主流浏览器都已经实现了 TLS 1.2 的支持。TLS 1.0通常被标示为SSL 3.1，TLS 1.1为SSL 3.2，TLS 1.2为SSL 3.3。\n1994年，NetScape公司设计了SSL协议（Secure Sockets Layer）的1.0版，但是未发布。 1995年，NetScape公司发布SSL 2.0版，很快发现有严重漏洞。 1996年，SSL 3.0版问世，得到大规模应用。 1999年，互联网标准化组织ISOC接替NetScape公司，发布了SSL的升级版TLS 1.0版。 2006年和2008年，TLS进行了两次升级，分别为TLS 1.1版和TLS 1.2版。最新的变动是2011年TLS 1.2的修订版。 TLS 运行过程 SSL/TLS协议的基本思路是采用 公钥加密法，也就是说，客户端先向服务器端索要公钥，然后用公钥加密信息，服务器收到密文后，用自己的私钥解密。因此，SSL/TLS协议的基本过程是这样的：\n（1） 客户端向服务器端索要并验证公钥。 （2） 双方协商生成\u0026quot;对话密钥\u0026quot;。 （3） 双方采用\u0026quot;对话密钥\u0026quot;进行加密通信。 \u0026ldquo;握手阶段\u0026quot;涉及四次通信，我们一个个来看。需要注意的是，\u0026ldquo;握手阶段\u0026quot;的所有通信都是明文的。\n客户端发出请求（ClientHello） 首先，客户端（通常是浏览器）先向服务器发出加密通信的请求，这被叫做 ClientHello 请求。\n在这一步，客户端主要向服务器提供以下信息。\n（1） 支持的协议版本，比如TLS 1.0版。 （2） 一个客户端生成的随机数，稍后用于生成\u0026quot;对话密钥\u0026quot;。 （3） 支持的加密方法，比如RSA公钥加密。 （4） 支持的压缩方法。 这里需要注意的是，客户端发送的信息之中不包括服务器的域名。也就是说，理论上服务器只能包含一个网站，否则会分不清应该向客户端提供哪一个网站的数字证书。这就是为什么通常一台服务器只能有一张数字证书的原因。\n对于虚拟主机的用户来说，这当然很不方便。2006年，TLS协议加入了一个 Server Name Indication 扩展，允许客户端向服务器提供它所请求的域名。\n服务器回应（SeverHello） 服务器收到客户端请求后，向客户端发出回应，这叫做 SeverHello 。服务器的回应包含以下内容。\n（1）确认使用的加密通信协议版本，比如TLS 1.0版本。如果浏览器与服务器支持的版本不一致，服务器关闭加密通信。 （2） 一个服务器生成的随机数，稍后用于生成 对话密钥。 （3） 确认使用的加密方法，比如 RSA 公钥加密。 （4） 服务器证书。 除了上面这些信息，如果服务器需要确认客户端的身份，就会再包含一项请求，要求客户端提供 \u0026ldquo;客户端证书\u0026rdquo;。比如，金融机构往往只允许认证客户连入自己的网络，就会向正式客户提供 USB 密钥，里面就包含了一张客户端证书。\n客户端回应 客户端收到服务器回应以后，首先验证服务器证书。如果证书不是可信机构颁布、或者证书中的域名与实际域名不一致、或者证书已经过期，就会向访问者显示一个警告，由其选择是否还要继续通信。\n如果证书没有问题，客户端就会从证书中取出服务器的公钥。然后，向服务器发送加密信息，包含下面三项信息。\n（1） 一个随机数。该随机数用服务器公钥加密，防止被窃听。 （2） 编码改变通知，表示随后的信息都将用双方商定的加密方法和密钥发送。 （3） 客户端握手结束通知，表示客户端的握手阶段已经结束。这一项同时也是前面发送的所有内容的hash值，用来供服务器校验。 上面第一项的随机数，是整个握手阶段出现的第三个随机数，又称 pre-master key 。有了它以后，客户端和服务器就同时有了三个随机数，接着双方就用事先商定的加密方法，各自生成本次会话所用的同一把\u0026quot;会话密钥\u0026rdquo;。\n至于 为什么一定要用三个随机数，来生成\u0026quot;会话密钥\u0026rdquo;：\n 不管是客户端还是服务器，都需要随机数，这样生成的密钥才不会每次都一样。由于SSL协议中证书是静态的，因此十分有必要引入一种随机因素来保证协商出来的密钥的随机性。\n对于 RSA 密钥交换算法来说，pre-master-key本身就是一个随机数，再加上 hello 消息中的随机，三个随机数通过一个密钥导出器最终导出一个对称密钥。\npre master 的存在在于 SSL 协议不信任每个主机都能产生完全随机的随机数，如果随机数不随机，那么 pre master secret（对称密钥） 就有可能被猜出来，那么仅适用 pre master secret 作为密钥就不合适了，因此必须引入新的随机因素，那么客户端和服务器三个随机数一同生成的密钥就不容易被猜出了，一个伪随机可能完全不随机，可是是三个伪随机就十分接近随机了，每增加一个自由度，随机性增加的可不是一个量级。\n 此外，如果前一步，服务器要求客户端证书，客户端会在这一步发送证书及相关信息。\n服务器的最后回应 服务器收到客户端的第三个随机数 pre-master key 之后，计算生成本次会话所用的\u0026quot;会话密钥\u0026rdquo;。然后，向客户端最后发送下面信息。\n（1）编码改变通知，表示随后的信息都将用双方商定的加密方法和密钥发送。 （2）服务器握手结束通知，表示服务器的握手阶段已经结束。这一项同时也是前面发送的所有内容的 hash 值，用来供客户端校验。 至此，整个握手阶段全部结束。接下来，客户端与服务器进入加密通信，就完全是使用普通的HTTP协议，只不过用\u0026quot;会话密钥\u0026quot;加密内容。\nTCP 流过程  Client Hello：客户端将其SSL版本号、加密设置参数、与session有关的数据以及其它一些必要信息（如加密算法和能支持的密钥大小）发送到服务器。 Server Hello：服务器将其SSL版本号、加密设置参数、与session有关的数据以及其它一些必要信息发送给客户端 Certificate（可选）：服务器发一个证书或一个证书链到客户端，证书链开始于服务器公共钥匙并结束于证明权威的根证书。该证书用于向客户端确认服务器的身份，该消息是可选的。如果配置服务器的SSL需要验证服务器的身份，会发送该消息。多数电子商务应用都需要服务器端身份验证。 Certificate Request（可选）：如果配置服务器的SSL需要验证用户身份，还要发出请求要求浏览器提供用户证书。 多数电子商务不需要客户端身份验证，不过，在支付过程中经常需要客户端身份验证。 Server Key Exchange（可选）：如果服务器发送的公共密钥对加密密钥的交换不是很合适，则发送一个服务器密钥交换消息。 ServerHelloDone：通知客户端，服务器已经完成了交流过程的初始化。 Certificate（可选）：客户端发送客户端证书给服务器。仅当服务器请求客户端身份验证的时候会发送客户端证书 Client Key Exchange：客户端产生一个会话密钥与服务器共享。在SSL握手协议完成后，客户端与服务器端通信信息的加密就会使用该会话密钥。如果使用RSA加密算法，客户端将使用服务器的公钥将会话加密后再发送给服务器。服务器使用自己的私钥对接收的消息进行解密得到共享的会话密钥。 Certificate Verify：如果服务器请求验证客户端，则这消息允许服务器完成验证过程。 Change cipher spec：客户端要求服务器在后续的通信中使用加密模式 Finished：客户端告诉服务器已经准备好安全通信了。 Change cipher spec：服务器要求客户端在后续的通信中使用加密模式 Finished：服务器告诉客户端它已经准备好安全通信了。SSL握手完成的标志 Encrypted Data：客户端和服务端在安全信道上进行加密信息的交流  HTTPS 的七个误解  HTTPS无法缓存？：许多人以为，出于安全考虑，浏览器不会在本地保存HTTPS缓存。实际上，只要在HTTP头中使用特定命令，HTTPS是可以缓存的。 SSL证书很贵？：如果你在网上搜一下，就会发现很多便宜的SSL证书，大概10美元一年，这和一个 .com 域名的年费差不多。而且事实上，还能找到免费的 SSL 证书。 HTTPS站点必须有独享的IP地址？使用子域名通配符SSL证书（wildcard SSL certificate，价格大约是每年125美元），就能在一个IP地址上部署多个HTTPS子域名。 转移服务器时要购买新证书？ HTTPS太慢？：使用HTTPS不会使你的网站变得更快（实际上有可能，请看下文），但是有一些技巧可以大大减少额外开销。 有了HTTPS，Cookie和查询字符串就安全了？：虽然无法直接从HTTPS数据中读取Cookie和查询字符串，但是你仍然需要使它们的值变得难以预测。 只有注册登录页，才需要HTTPS？：这种想法很普遍。人们觉得，HTTPS可以保护用户的密码，此外就不需要了。Firefox浏览器新插件Firesheep，证明了这种想法是错的。我们可以看到，在Twitter和Facebook上，劫持其他人的session是非常容易的。  "});index.add({'id':23,'href':'/interview/docs/basic/os/io/','title':"I/O",'content':"I/O 基本概念 文件描述符fd 文件描述符（File descriptor）是计算机科学中的一个术语，是一个用于表述指向文件的引用的抽象化概念。\n文件描述符在形式上是一个非负整数。实际上，它是一个索引值，指向内核为每一个进程所维护的该进程打开文件的记录表。当程序打开一个现有文件或者创建一个新文件时，内核向进程返回一个文件描述符。在程序设计中，一些涉及底层的程序编写往往会围绕着文件描述符展开。但是文件描述符这一概念往往只适用于 UNIX、Linux 这样的操作系统。\n缓存 I/O 缓存 I/O 又被称作标准 I/O，大多数文件系统的默认 I/O 操作都是缓存 I/O。在 Linux 的缓存 I/O 机制中，操作系统会将 I/O 的数据缓存在文件系统的页缓存（ page cache ）中，也就是说，数据会先被拷贝到操作系统内核的缓冲区中，然后才会从操作系统内核的缓冲区拷贝到应用程序的地址空间。\n缓存 I/O 的缺点：数据在传输过程中需要在应用程序地址空间和内核进行多次数据拷贝操作，这些数据拷贝操作所带来的 CPU 以及内存开销是非常大的。\nIO模式 刚才说了，对于一次IO访问（以read举例），数据会先被拷贝到操作系统内核的缓冲区中，然后才会从操作系统内核的缓冲区拷贝到应用程序的地址空间。所以说，当一个read操作发生时，它会经历两个阶段：\n 等待数据准备 将数据从内核拷贝到进程中  正式因为这两个阶段，Linux系统产生了下面五种网络模式的方案。\n 阻塞 I/O（blocking IO） 非阻塞 I/O（nonblocking IO） I/O 多路复用（ IO multiplexing） 信号驱动 I/O（ signal driven IO） 异步 I/O（asynchronous IO）   由于signal driven IO在实际中并不常用，所以这里只提及剩下的四种 IO Model。\n 阻塞IO 在 Linux 中，默认情况下所有的 socket 都是 blocking ，一个典型的读操作流程大概是这样：\n当用户进程调用了 recvfrom 这个系统调用， kernel 就开始了 IO 的第一个阶段：准备数据（对于网络IO来说，很多时候数据在一开始还没有到达。比如，还没有收到一个完整的 UDP 包。这个时候 kernel 就要等待足够的数据到来）。这个过程需要等待，也就是说数据被拷贝到操作系统内核的缓冲区中是需要一个过程的。而在用户进程这边，整个进程会被阻塞（当然，是进程自己选择的阻塞）。当 kernel 一直等到数据准备好了，它就会将数据从 kernel 中拷贝到用户内存，然后 kernel 返回结果，用户进程才解除 block 的状态，重新运行起来。\n blocking IO的特点就是在IO执行的两个阶段都被block了\n 非阻塞 I/O Linux 下，可以通过设置 socket 使其变为 non-blocking 。当对一个 non-blocking socket 执行读操作时，流程是这个样子：\n当用户进程发出 read 操作时，如果 kernel 中的数据还没有准备好，那么它并不会 block 用户进程，而是立刻返回一个 error 。从用户进程角度讲 ，它发起一个 read 操作后，并不需要等待，而是马上就得到了一个结果。用户进程判断结果是一个 error 时，它就知道数据还没有准备好，于是它可以再次发送 read 操作。一旦 kernel 中的数据准备好了，并且又再次收到了用户进程的 system call ，那么它马上就将数据拷贝到了用户内存，然后返回。\n nonblocking IO的特点是用户进程需要不断的主动询问kernel数据好了没有\n IO多路复用 IO多路复用就是我们说的 select，poll，epoll ，有些地方也称这种IO方式为 event driven IO 。select/epoll 的好处就在于单个 process 就可以同时处理多个网络连接的 IO 。它的基本原理就是 select，poll，epoll 这个 function 会不断的轮询所负责的所有 socket ，当某个 socket 有数据到达了，就通知用户进程。\n当用户进程调用了 select，那么整个进程会被 block，而同时， kernel 会监视所有 select 负责的 socket ，当任何一个 socket 中的数据准备好了， select 就会返回。这个时候用户进程再调用 read 操作，将数据从 kernel 拷贝到用户进程。\n I/O 多路复用的特点是通过一种机制一个进程能同时等待多个文件描述符，而这些文件描述符（套接字描述符）其中的任意一个进入读就绪状态，select() 函数就可以返回。\n 这个图和 blocking IO 的图其实并没有太大的不同，事实上，还更差一些。因为这里需要使用两个 system call (select 和 recvfrom)，而 blocking IO 只调用了一个 system call (recvfrom)。但是，用 select 的优势在于它可以同时处理多个 connection 。\n所以，如果处理的连接数不是很高的话，使用 select/epoll 的 web server 不一定比使用 multi-threading + blocking IO 的 web server 性能更好，可能延迟还更大。select/epoll 的优势并不是对于单个连接能处理得更快，而是在于能处理更多的连接。\n在IO多路复用实际使用中，对于每一个socket，一般都设置成为 non-blocking ，但是，如上图所示，整个用户的 process 其实是一直被block的。只不过 process 是被 select 这个函数 block ，而不是被 socket IO 给 block 。\n基本概念 在 I/O 编程过程中,当需要同时处理多个客户端接入请求时，可以利用多线程或者 I/O 多路复用 技术进行处理。I/O多路复用 技术通过把多个I/O的阻塞复用到同一个selct的阻塞上，从而使得系统在单线程的情况下可以同时处理多个客户端请求。与传统的 多线程/多进程 模型比，I/O多路复用的最大优势是系统开销小，系统不需要创建新的额外进程或者线程，也不需要维护这些进程和线程的运行，降低了系统的维护工作量，节省了系统资源，I/O多路复用的主要应用场景如下。\n 服务器需要同时处理多个处于监听状态或者多个连接状态的套接字 服务器需要同时处理多种网络协议的套接字  目前支持I/O多路复用的系统调用有 select、pselect、poll、epoll，在Linux网络编程; 过程中，很长一段时间都使用 select 做轮询和网络事件通知，然而 select 的一些固有缺陷导致了它的应用受到了很大的限制。最终 Linux 不得不在新的内核版本中寻找 select 的替代方案，最终选择了 epoll。 epoll 与 select 的原理比较类似，为了克服 select 的缺点， epoll 作了很多重大改进，现总结如下。\n支持一个进程打开的 socket 描述符（FD）不受限制（仅受限于操作系统的最大文件句柄数）。 select 最大的缺陷就是单个进程所打开的 FD 是有一定限制的，它由 FD_SETSIZE 设置，默认值是 1024 。对于那些需要支持上万个 TCP 连接的大型服务器来说显然太少了。可以选择修改这个宏然后重新编译内核，不过这会带来网络效率的下降。我们也可以通过选择多进程的方案（传统的 Apache 方案）解决这个问题，不过虽然在 Linux上创建进程的代价比较小，但仍旧是不可忽视的，另外，进程间的数据交换非常麻烦，对于 Java 由于没有共享内存，需要通过 Socket 通信或者其他方式进行数据同步，这带来了额外的性能损耗，增加了程序复杂度，所以也不是一种完美的解决方案。值得庆幸的是， epoll 并没有这个限制，它所支持的 FD 上限是操作系统的 最大文件句柄数，这个数字远远大于 1024 。例如，在 1 GB 内存的机器上大约是 10万个句柄左右，具体的值可以通过cat /proc/sys/fs/file- max 察看，通常情况下这个值跟系统的内存关系比较大。\nI/O效率不会随着FD数目的增加而线性下降。 传统的 select/poll 另-个致命弱点就是当你拥有一个很大的 socket 集合，由于网络延时或者链路空闲，任一时刻只有少部分的 socket 是“活跃”的，但是 select/poll 每次调用都会线性扫描全部的集合，导致效率呈现线性下降。 epoll 不存在这个问题，它只会对“活跃”的 socket 进行操作，这是因为在内核实现中 epoll 是根据每个 fd 上面的 callback 函数实现的，那么，只有“活跃”的 socket 才会主动的去调用 callback 函数，其他 idle 状态 socket 则不会。在这点上， epoll 实现了一个伪 AIO。针对 epoll 和 select 性能对比的 benchmark 测试表明：如果所有的 socket 都处于活跃态，例如一个高速 LAN 环境， epoll 并不比 select/poll 效率高太多；相反，如果过多使用 epoll_ ctl , 效率相比还有稍微的下降。但是一旦使用 idleconnections 模拟 WAN 环境，epoll 的效率就远在 select/poll 之上了。\n使用 mmap 加速内核与用户空间的消息传递。 无论是 select，poll 还是 epoll 都需要内核把 FD 消息通知给用户空间，如何避免不必要的内存复制就显得非常重要， epoll 是通过内核和用户空间 mmap 同一块内存实现。\nEpoll 的 API 更加简单。 包括创建一个 epoll 描述符、添加监听事件、阻塞等待所监听的事件发生，关闭 epoll 描述符等。\n值得说明的是，用来克服 select/poll 缺点的方法不只有 epoll , epoll 只是一种 Linux 的 实现方案。在 freeBSD 下有 kqueue\nEpoll 边缘触发\u0026amp;水平触发 epoll 对文件描述符的操作有两种模式：LT（level trigger）和ET（edge trigger）。LT模式是 默认模式 ，LT模式与ET模式的区别如下：\n LT模式：当 epoll_wait 检测到描述符事件发生并将此事件通知应用程序，应用程序可以不立即处理该事件。下次调用 epoll_wait 时，会再次响应应用程序并通知此事件。 ET模式：当 epoll_wait 检测到描述符事件发生并将此事件通知应用程序，应用程序必须立即处理该事件。如果不处理，下次调用 epoll_wait 时，不会再次响应应用程序并通知此事件。   ET模式 在很大程度上减少了 epoll 事件被重复触发的次数，因此 效率要比LT模式高。epoll 工作在ET模式的时候，必须使用非阻塞套接口，以避免由于一个文件句柄的阻塞读/阻塞写操作把处理多个文件描述符的任务饿死。\n 异步 I/O 用户进程发起 read 操作之后，立刻就可以开始去做其它的事。而另一方面，从 kernel 的角度，当它受到一个 asynchronous read 之后，首先它会立刻返回，所以不会对用户进程产生任何 block 。然后，kernel 会等待数据准备完成，然后将数据拷贝到用户内存，当这一切都完成之后，kernel 会给用户进程发送一个 signal ，告诉它 read 操作完成了。\nblocking vs non-blocking 调用 blocking IO 会一直 block 住对应的进程直到操作完成，而 non-blocking IO 在 kernel 还准备数据的情况下会立刻返回。\nsynchronous IO vs asynchronous IO 在说明synchronous IO和asynchronous IO的区别之前，需要先给出两者的定义。 POSIX 的定义是这样子的：\n A synchronous I/O operation causes the requesting process to be blocked until that I/O operation completes; An asynchronous I/O operation does not cause the requesting process to be blocked;  两者的区别就在于 synchronous IO 做 IO operation 的时候会将 process 阻塞。按照这个定义，之前所述的 blocking IO，non-blocking IO，IO multiplexing 都属于 synchronous IO。\n有人会说，non-blocking IO 并没有被 block 啊。这里有个非常 狡猾 的地方，定义中所指的 IO operation 是指真实的 IO 操作，就是例子中的 recvfrom 这个 system call 。non-blocking IO 在执行 recvfrom 这个 system call 的时候，如果 kernel 的数据没有准备好，这时候不会 block 进程。但是，当 kernel 中数据准备好的时候，recvfrom 会将数据从 kernel 拷贝到用户内存中，这个时候进程是被 block 了，在这段时间内，进程是被 block 的。\n而 asynchronous IO 则不一样，当进程发起 IO 操作之后，就直接返回再也不理睬了，直到 kernel 发送一个信号，告诉进程说IO完成。在这整个过程中，进程完全没有被 block 。\n参考  Linux IO模式及 select、poll、epoll详解  "});index.add({'id':24,'href':'/interview/docs/basic/database/innodb/','title':"InnoDB",'content':"InnoDB 数据存储  MySQL 存储格式可通过 SQL：SHOW TABLE STATUS IN {dbName} 查看\n 与现有的大多数存储引擎一样，InnoDB 使用页作为磁盘管理的最小单位；数据在 InnoDB 存储引擎中都是按行存储的，每个 16KB 大小的页中可以存放 2-200 行的记录。\n当 InnoDB 存储数据时，它可以使用不同的行格式进行存储；MySQL 5.7 版本支持以下格式的行存储方式：\n Antelope 是 InnoDB 最开始支持的文件格式，它包含两种行格式 Compact 和 Redundant ，它最开始并没有名字； Antelope 的名字是在新的文件格式 Barracuda 出现后才起的， Barracuda 的出现引入了两种新的行格式 Compressed 和 Dynamic ；InnoDB 对于文件格式都会向前兼容，而官方文档中也对之后会出现的新文件格式预先定义好了名字：Cheetah、Dragon、Elk 等等。\n 两种行记录格式 Compact 和 Redundant 在磁盘上按照以下方式存储：\nCompact 和 Redundant 格式最大的不同就是记录格式的第一个部分；在 Compact 中，行记录的第一部分倒序存放了一行数据中列的长度（Length），而 Redundant 中存的是每一列的偏移量（Offset），从总体上上看， Compact 行记录格式相比 Redundant 格式能够减少 20% 的存储空间。\n行溢出数据 当 InnoDB 使用 Compact 或者 Redundant 格式存储极长的 VARCHAR 或者 BLOB 这类大对象时，我们并不会直接将所有的内容都存放在数据页节点中，而是将数据中的前 768 个字节存储在数据页中，后面会通过偏移量指向溢出页（off-page），最大768字节的作用是便于创建 前缀索引。溢出页（off-page）不存储在 B+tree 中，使用的是uncompress BLOB page，并且每个字段的溢出都是存储独享。\n但是当我们使用新的行记录格式 Compressed 或者 Dynamic 时都只会在行记录中保存 20 个字节的指针，实际的数据都会存放在溢出页面中。\n当然在实际存储中，可能会对不同长度的 TEXT 和 BLOB 列进行优化。\n 想要了解更多与 InnoDB 存储引擎中记录的数据格式的相关信息，可以阅读 InnoDB Record Structure\n 数据页结构 页是 InnoDB 存储引擎管理数据的最小磁盘单位，而 B-Tree 节点就是实际存放表中数据的页面，我们在这里将要介绍页是如何组织和存储记录的；首先，一个 InnoDB 页有以下七个部分：\n每一个页中包含了两对 header/trailer：内部的 Page Header/Page Directory 关心的是页的状态信息，而 Fil Header/Fil Trailer 关心的是记录页的头信息。\n在页的头部和尾部之间就是用户记录和空闲空间了，每一个数据页中都包含 Infimum 和 Supremum 这两个虚拟的记录（可以理解为占位符）， Infimum 记录是比该页中任何主键值都要小的值， Supremum 是该页中的最大值：\nUser Records 就是整个页面中真正用于存放行记录的部分，而 Free Space 就是空余空间了，它是一个链表的数据结构，为了保证插入和删除的效率，整个页面并不会按照主键顺序对所有记录进行排序，它会自动从左侧向右寻找空白节点进行插入，行记录在物理存储上并不是按照顺序的，它们之间的顺序是由 next_record 这一指针控制的。\nB+ 树在查找对应的记录时，并不会直接从树中找出对应的行记录，它只能获取记录所在的页，将整个页加载到内存中，再通过 Page Directory 中存储的稀疏索引和 n_owned、next_record 属性取出对应的记录，不过因为这一操作是在内存中进行的，所以通常会忽略这部分查找的耗时。这样就存在一个命中率的问题，如果一个page中能够相对的存放足够多的行，那么命中率就会相对高一些，性能就会有提升。\nInnodb的 page 大小默认为 16kb，Innodb存储引擎表为索引组织表，树底层的叶子节点为一双向链表，因此每个页中至少应该有两行记录，这就决定了 Innodb 在存储一行数据的时候不能够超过 8kb，但事实上应该更小，因为还有一些 InnoDB 内部数据结构要存储。\n通常我们认为 blob 这类的大对象的存储会把数据存放在 off-page，其实不然，关键点还是要看一个 page 中到底能否存放两行数据，blob 可以完全存放在数据页中(单行长度没有超过 8kb)，而 varchar 类型的也有可能存放在溢出页中(单行长度超过 8kb，前 768byte 存放在数据页中)。\n索引 索引是数据库中非常非常重要的概念，它是存储引擎能够快速定位记录的秘密武器，对于提升数据库的性能、减轻数据库服务器的负担有着非常重要的作用；索引优化是对查询性能优化的最有效手段，它能够轻松地将查询的性能提高几个数量级。\nInnoDB 存储引擎在绝大多数情况下使用 B+ 树建立索引，这是关系型数据库中查找最为常用和有效的索引，但是 B+ 树索引并不能找到一个给定键对应的具体值，它只能找到数据行对应的页，然后正如上一节所提到的，数据库把整个页读入到内存中，并在内存中查找具体的数据行。\nB+ 树是平衡树，它查找任意节点所耗费的时间都是完全相同的，比较的次数就是 B+ 树的高度；\n B+ 树的叶子节点存放所有指向关键字的指针，节点内部关键字记录和节点之间都根据关键字的大小排列。当顺序递增插入的时候，只有最后一个节点会在满掉的时候引起索引分裂，此时无需移动记录，只需创建一个新的节点即可。而当非递增插入的时候，会使得旧的节点分裂，还可能伴随移动记录，以便使得新数据能够插入其中。一般建议使用一列顺序递增的 ID 来作为主键，但不必是数据库的 autoincrement 字段，只要满足顺序增加即可，如 twitter 的 snowflake 即为顺序递增的 ID 生成器。\n 聚集索引和辅助索引 数据库中的 B+ 树索引可以分为聚集索引（clustered index）和辅助索引（secondary index），它们之间的最大区别就是，聚集索引中存放着一条行记录的全部信息，而辅助索引中只包含索引列和一个用于查找对应行记录的『书签』。\n聚集索引 InnoDB 存储引擎中的表都是使用索引组织的，也就是按照键的顺序存放；聚集索引就是按照表中主键的顺序构建一颗 B+ 树，并在叶节点中存放表中的行记录数据。\n 如果没有定义主键，则会使用非空的 UNIQUE键 做主键 ; 如果没有非空的 UNIQUE键 ，则系统生成一个6字节的 rowid 做主键;\n CREATE TABLE users( id INT NOT NULL, first_name VARCHAR(20) NOT NULL, last_name VARCHAR(20) NOT NULL, age INT NOT NULL, PRIMARY KEY(id), KEY(last_name, first_name, age) KEY(first_name) ); 如果使用上面的 SQL 在数据库中创建一张表，B+ 树就会使用 id 作为索引的键，并在叶子节点中存储一条记录中的所有信息。\n 图中对 B+ 树的描述与真实情况下 B+ 树中的数据结构有一些差别，不过这里想要表达的主要意思是：聚集索引叶节点中保存的是整条行记录，而不是其中的一部分。\n 聚集索引与表的物理存储方式有着非常密切的关系，所有正常的表应该 有且仅有一个 聚集索引（绝大多数情况下都是主键），表中的所有行记录数据都是按照 聚集索引 的顺序存放的。\n当我们使用聚集索引对表中的数据进行检索时，可以直接获得聚集索引所对应的整条行记录数据所在的页，不需要进行第二次操作。\n辅助索引 数据库将 所有的非聚集索引都划分为辅助索引，但是这个概念对我们理解辅助索引并没有什么帮助；辅助索引也是通过 B+ 树实现的，但是它的叶节点并不包含行记录的全部数据，仅包含索引中的所有键和一个用于查找对应行记录的『书签』，在 InnoDB 中这个书签就是当前记录的主键。\n辅助索引的存在并不会影响聚集索引，因为聚集索引构成的 B+ 树是数据实际存储的形式，而辅助索引只用于加速数据的查找，所以一张表上往往有多个辅助索引以此来提升数据库的性能。\n 一张表一定包含一个聚集索引构成的 B+ 树以及若干辅助索引的构成的 B+ 树。\n 如果在表 users 中存在一个辅助索引 (first_name, age)，那么它构成的 B+ 树大致就是上图这样，按照 (first_name, age) 的字母顺序对表中的数据进行排序，当查找到主键时，再通过聚集索引获取到整条行记录。\n上图展示了一个使用辅助索引查找一条表记录的过程：通过辅助索引查找到对应的主键，最后在聚集索引中使用主键获取对应的行记录，这也是通常情况下行记录的查找方式。\nInnoDB 锁机制 InnoDB默认使用行锁，实现了两种标准的行锁——共享锁与排他锁；\n|行锁类型| 锁功能|锁兼容性| 加锁|释放锁| | :\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;- | :\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;- | |共享锁（读锁、S锁）| 允许获取共享锁的亊务读数据|与共享锁兼容，与排它锁不兼容| 只有 SerializaWe 隔离级别会默认为：读加共享锁；其他隔离级别下，可显示使用 select...lock in share model 为读加共享锁| 在事务提交或回滚后会自动同时释放锁；除了使用 start transaction 的方式显式开启事务，InnoDB 也会自动为增删改査语句开启事务，并自动提交或回滚；(autocommit=1)| |排它锁（写锁、X锁）|允许获取排它锁的事务更新或删除数据|与共享锁不兼容，与排它锁不兼容|在默认的 Reapeatable Read 隔离级别下，InnoDB 会自动为增删改操作的行加排它锁；也可显式使用 select...for update 为读加排它锁|\u0026hellip;|\n  除了显式加锁的情况，其他情况下的加锁与解锁都无需人工干预 InnoDB 所有的行锁算法都是基于索引实现的，锁定的也都是索引或索引区间   当前读 \u0026amp; 快照读 当前读：即加锁读，读取记录的最新版本，会加锁保证其他并发事务不能修改当前记录，直至获取锁的事务释放锁；使用当前读的操作主要包括：显式加锁的读操作与插入/更新/删除等写操作，如下所示：\nselect * from table where ? lock in share mode; select * from table where ? for update; insert into table values (…); update table set ? where ?; delete from table where ?;  注：当 Update SQL 被发给 MySQL 后， MySQL Server 会根据where条件，读取第一条满足条件的记录，然后 InnoDB 引擎会将第一条记录返回，并加锁，待 MySQL Server 收到这条加锁的记录之后，会再发起一个 Update 请求，更新这条记录。一条记录操作完成，再读取下一条记录，直至没有满足条件的记录为止。因此， Update 操作内部，就包含了当前读。同理， Delete 操作也一样。 Insert 操作会稍微有些不同，简单来说，就是 Insert 操作可能会触发 Unique Key 的冲突检查，也会进行一个当前读。\n 快照读：即不加锁读，读取记录的快照版本而非最新版本，通过MVCC实现；\nInnoDB 默认的 RR 事务隔离级别下，不显式加lock in share mode与for update的 select 操作都属于快照读，保证事务执行过程中只有第一次读之前提交的修改和自己的修改可见，其他的均不可见；\n共享锁与独占锁 意向锁 InnoDB 支持多粒度的锁，允许表级锁和行级锁共存。一个类似于 LOCK TABLES ... WRITE 的语句会获得这个表的 x 锁。为了实现多粒度锁，InnoDB 使用了意向锁（简称 I 锁）。I 锁是表明一个事务稍后要获得针对一行记录的某种锁（s or x）的对应表的表级锁，有两种：\n 意向排它锁（简称 IX 锁）表明一个事务意图在某个表中设置某些行的 x 锁 意向共享锁（简称 IS 锁）表明一个事务意图在某个表中设置某些行的 s 锁  SELECT ... LOCK IN SHARE MODE 设置一个 IS 锁, SELECT ... FOR UPDATE 设置一个 IX 锁。意向锁的原则如下：\n 一个事务必须先持有该表上的 IS 或者更强的锁才能持有该表中某行的 S 锁 一个事务必须先持有该表上的 IX 锁才能持有该表中某行的 X 锁  新请求的锁只有兼容已有锁才能被允许，否则必须等待不兼容的已有锁被释放。一个不兼容的锁请求不被允许是因为它会引起死锁，错误会发生。意向锁只会阻塞全表请求（比如 LOCK TABLES ... WRITE ）。意向锁的主要目的是展示某人正在锁定表中一行，或者将要锁定一行。\n更多信息参见：并发控制\nRecord Lock 记录锁（Record Lock）是加到索引记录上的锁，假设我们存在下面的一张表 users：\n CREATE TABLE users( id INT NOT NULL AUTO_INCREMENT, last_name VARCHAR(255) NOT NULL, first_name VARCHAR(255), age INT, PRIMARY KEY(id), KEY(last_name), KEY(age) ); 如果我们使用 id 或者 last_name 作为 SQL 中 WHERE 语句的过滤条件，那么 InnoDB 就可以通过索引建立的 B+ 树找到行记录并添加索引，但是如果使用 first_name 作为过滤条件时，由于 InnoDB 不知道待修改的记录具体存放的位置，也无法对将要修改哪条记录提前做出判断就会锁定整个表。\nGap Lock 记录锁是在存储引擎中最为常见的锁，除了记录锁之外，InnoDB 中还存在间隙锁（Gap Lock），间隙锁是对索引记录中的一段连续区域的锁；当使用类似 SELECT * FROM users WHERE id BETWEEN 10 AND 20 FOR UPDATE; 的 SQL 语句时，就会阻止其他事务向表中插入 id = 15 的记录，因为整个范围都被间隙锁锁定了。\n 间隙锁是存储引擎对于性能和并发做出的权衡，并且只用于某些事务隔离级别。\n 虽然间隙锁中也分为共享锁和互斥锁，不过它们之间并不是互斥的，也就是不同的事务可以同时持有一段相同范围的共享锁和互斥锁，它唯一阻止的就是其他事务向这个范围中添加新的记录。\n间隙锁的缺点  间隙锁有一个比较致命的弱点，就是当锁定一个范围键值之后，即使某些不存在的键值也会被无辜的锁定，而造成在锁定的时候无法插入锁定键值范围内的任何数据。在某些场景下这可能会对性能造成很大的危害 当Query无法利用索引的时候， Innodb会放弃使用行级别锁定而改用表级别的锁定，造成并发性能的降低； 当Quuery使用的索引并不包含所有过滤条件的时候，数据检索使用到的索引键所指向的数据可能有部分并不属于该Query的结果集的行列，但是也会被锁定，因为间隙锁锁定的是一个范围，而不是具体的索引键； 当Query在使用索引定位数据的时候，如果使用的索引键一样但访问的数据行不同的时候（索引只是过滤条件的一部分），一样会被锁定  Next-Key Lock Next-Key 锁相比前两者就稍微有一些复杂，它是记录锁和记录前的间隙锁的结合，在 users 表中有以下记录：\n +------|-------------|--------------|-------+ | id | last_name | first_name | age | |------|-------------|--------------|-------| | 4 | stark | tony | 21 | | 1 | tom | hiddleston | 30 | | 3 | morgan | freeman | 40 | | 5 | jeff | dean | 50 | | 2 | donald | trump | 80 | +------|-------------|--------------|-------+ 如果使用 Next-Key 锁，那么 Next-Key 锁就可以在需要的时候锁定以下的范围：\n (-∞, 21] (21, 30] (30, 40] (40, 50] (50, 80] (80, ∞)  既然叫 Next-Key 锁，锁定的应该是当前值和后面的范围，但是实际上却不是，Next-Key 锁锁定的是当前值和前面的范围。\n 当我们更新一条记录，比如 SELECT * FROM users WHERE age = 30 FOR UPDATE;，InnoDB 不仅会在范围 (21, 30] 上加 Next-Key 锁，还会在这条该记录索引增长方向的范围 (30, 40] 加间隙锁，所以插入 (21, 40] 范围内的记录都会被锁定。\n Next-Key 锁的作用其实是为了解决幻读的问题。\n 插入意向锁 插入意向锁是在插入一行记录操作之前设置的一种间隙锁，这个锁释放了一种插入方式的信号，亦即多个事务在相同的索引间隙插入时如果不是插入间隙中相同的位置就不需要互相等待。假设有索引值4、7，几个不同的事务准备插入5、6，每个锁都在获得插入行的独占锁之前用插入意向锁各自锁住了4、7之间的间隙，但是不阻塞对方因为插入行不冲突。\n自增锁 自增锁是一个特殊的表级锁，事务插入自增列的时候需要获取，最简单情况下如果一个事务插入一个值到表中，任何其他事务都要等待，这样第一个事物才能获得连续的主键值。\n锁选择 +——-+————-+ | id | name | +——-+————-+ | 1 | title1 | +——-+————-+ | 2 | title2 | +——-+————-+ | 3 | title3 | +——-+————-+ | 9 | title9 | +——-+————-+ | 10 | title10 | +——-+————-+ 按照原理来说，id\u0026gt;5 and id\u0026lt;7这个查询条件，在表中找不到满足条件的项，因此会对第一个不满足条件的项(id = 9)上加GAP锁，防止后续其他事务插入满足条件的记录。\n而 GAP 锁与GAP 锁是不冲突的，那么为什么两个同时执行id\u0026gt;5 and id\u0026lt;7查询的事务会冲突呢？\n原因在于，MySQL Server并没有将id\u0026lt;7这个查询条件下降到InnoDB引擎层，因此InnoDB看到的查询，是id\u0026gt;5，正向扫描。读出的记录id=9，先加上next key锁(Lock X + GAP lock)，然后返回给 MySQL Server 进行判断。 MySQL Server 此时才会判断返回的记录是否满足id\u0026lt;7的查询条件。此处不满足，查询结束。\n因此，id=9记录上，真正持有的锁是next key锁，而next key锁之间是相互冲突的，这也说明了为什么两个id\u0026gt;5 and id\u0026lt;7查询的事务会冲突的原因。\nInnoDB 事务隔离 几种隔离级别 事务的隔离性是数据库处理数据的几大基础之一，而隔离级别其实就是提供给用户用于在性能和可靠性做出选择和权衡的配置项。\nISO 和 ANIS SQL 标准制定了四种事务隔离级别，而 InnoDB 遵循了 SQL:1992 标准中的四种隔离级别：READ UNCOMMITED、READ COMMITED、REPEATABLE READ 和 SERIALIZABLE；每个事务的隔离级别其实都比上一级多解决了一个问题：\n  RAED UNCOMMITED：使用查询语句不会加锁，可能会读到未提交的行（Dirty Read）；\n 可以读取未提交记录。此隔离级别，不会使用，忽略。\n   READ COMMITED：只对记录加记录锁，而不会在记录之间加间隙锁，所以允许新的记录插入到被锁定记录的附近，所以再多次使用查询语句时，可能得到不同的结果（Non-Repeatable Read）；\n 快照读忽略，本文不考虑。针对当前读，RC隔离级别保证对读取到的记录加锁 (记录锁)，存在幻读现象。\n   REPEATABLE READ：快照读忽略，本文不考虑。针对当前读，RR隔离级别保证对读取到的记录加锁 (记录锁)，同时保证对读取的范围加锁，新的满足查询条件的记录不能够插入 (间隙锁)，不存在幻读现象。\n  SERIALIZABLE：从MVCC并发控制退化为基于锁的并发控制。不区别快照读与当前读，所有的读操作均为当前读，读加读锁 (S锁)，写加写锁 (X锁)。\n Serializable隔离级别下，读写冲突，因此并发度急剧下降，在MySQL/InnoDB下不建议使用。\n   MySQL 中默认的事务隔离级别就是 REPEATABLE READ，但是它通过 Next-Key 锁也能够在某种程度上解决幻读的问题。\n接下来，我们将数据库中创建如下的表并通过个例子来展示在不同的事务隔离级别之下，会发生什么样的问题：\n CREATE TABLE test( id INT NOT NULL, UNIQUE(id) ); 脏读  在一个事务中，读取了其他事务未提交的数据。\n 当事务的隔离级别为 READ UNCOMMITED 时，我们在 SESSION 2 中插入的未提交数据在 SESSION 1 中是可以访问的。\n不可重复读  在一个事务中，同一行记录被访问了两次却得到了不同的结果。\n 当事务的隔离级别为 READ COMMITED 时，虽然解决了脏读的问题，但是如果在 SESSION 1 先查询了一行数据，在这之后 SESSION 2 中修改了同一行数据并且提交了修改，在这时，如果 SESSION 1 中再次使用相同的查询语句，就会发现两次查询的结果不一样。\n不可重复读的原因就是，在 READ COMMITED 的隔离级别下，存储引擎不会在查询记录时添加行锁，锁定 id = 3 这条记录。\n幻读  在一个事务中，同一个范围内的记录被读取时，其他事务向这个范围添加了新的记录。\n 重新开启了两个会话 SESSION 1 和 SESSION 2，在 SESSION 1 中我们查询全表的信息，没有得到任何记录；在 SESSION 2 中向表中插入一条数据并提交；由于 REPEATABLE READ 的原因，再次查询全表的数据时，我们获得到的仍然是空集，但是在向表中插入同样的数据却出现了错误。\n这种现象在数据库中就被称作幻读，虽然我们使用查询语句得到了一个空的集合，但是插入数据时却得到了错误，好像之前的查询是幻觉一样。\n在标准的事务隔离级别中，幻读是由更高的隔离级别 SERIALIZABLE 解决的，但是它也可以通过 MySQL 提供的 Next-Key 锁解决：\nREPEATABLE READ 和 READ UNCOMMITED 其实是矛盾的，如果保证了前者就看不到已经提交的事务，如果保证了后者，就会导致两次查询的结果不同，MySQL 为我们提供了一种折中的方式，能够在 REPEATABLE READ 模式下加锁访问已经提交的数据，其本身并不能解决幻读的问题，而是通过文章前面提到的 Next-Key 锁来解决。\n加锁规则 参考连接  『浅入浅出』MySQL 和 InnoDB Locks Set by Different SQL Statements in InnoDB  "});index.add({'id':25,'href':'/interview/','title':"Introduction",'content':"Summary 本文档收集整理 计算机、Java 相关基础知识，有问题欢迎提 issue 👏\n页面生成工具：Hugo\n主题：hugo-book\n预览地址：Interview\n"});index.add({'id':26,'href':'/interview/docs/fromwork/spring/ioc/','title':"IOC",'content':"IOC Ioc—Inversion of Control，即“控制反转”，不是什么技术，而是一种设计思想。在Java 开发中，Ioc意味着将你设计好的对象交给容器控制，而不是传统的在你的对象内部直接控制。如何理解好Ioc呢？理解好Ioc的关键是要明确“谁控制谁，控制什么，为何是反转（有反转就应该有正转了），哪些方面反转了”，那我们来深入分析一下：\n　- 谁控制谁，控制什么：传统Java SE程序设计，我们直接在对象内部通过new进行创建对象，是程序主动去创建依赖对象；而IoC是有专门一个容器来创建这些对象，即由Ioc容器来控制对 象的创建；谁控制谁？当然是IoC 容器控制了对象；控制什么？那就是主要控制了外部资源获取（不只是对象包括比如文件等）。 - 为何是反转，哪些方面反转了：有反转就有正转，传统应用程序是由我们自己在对象中主动控制去直接获取依赖对象，也就是正转；而反转则是由容器来帮忙创建及注入依赖对象；为何是反转？因为由容器帮我们查找及注入依赖对象，对象只是被动的接受依赖对象，所以是反转；哪些方面反转了？依赖对象的获取被反转了。\nIoC能做什么 　IoC 不是一种技术，只是一种思想，一个重要的面向对象编程的法则，它能指导我们如何设计出松耦合、更优良的程序。传统应用程序都是由我们在类内部主动创建依赖对象，从而导致类与类之间高耦合，难于测试；有了IoC容器后，把创建和查找依赖对象的控制权交给了容器，由容器进行注入组合对象，所以对象与对象之间是 松散耦合，这样也方便测试，利于功能复用，更重要的是使得程序的整个体系结构变得非常灵活。\nIoC和DI DI—Dependency Injection，即“依赖注入”：组件之间依赖关系由容器在运行期决定，形象的说，即由容器动态的将某个依赖关系注入到组件之中。依赖注入的目的并非为软件系统带来更多功能，而是为了提升组件重用的频率，并为系统搭建一个灵活、可扩展的平台。通过依赖注入机制，我们只需要通过简单的配置，而无需任何代码就可指定目标需要的资源，完成自身的业务逻辑，而不需要关心具体的资源来自何处，由谁实现。\n理解DI的关键是：“谁依赖谁，为什么需要依赖，谁注入谁，注入了什么”，那我们来深入分析一下：\n 谁依赖于谁：当然是应用程序依赖于IoC容器； 为什么需要依赖：应用程序需要IoC容器来提供对象需要的外部资源； 谁注入谁：很明显是IoC容器注入应用程序某个对象，应用程序依赖的对象； 注入了什么：就是注入某个对象所需要的外部资源（包括对象、资源、常量数据）。  IoC和DI由什么关系呢？其实它们是同一个概念的不同角度描述，由于控制反转概念比较含糊（可能只是理解为容器控制对象这一个层面，很难让人想到谁来维护对象关系），所以2004年大师级人物Martin Fowler又给出了一个新的名字：“依赖注入”，相对IoC 而言，“依赖注入”明确描述了“被注入对象依赖IoC容器配置依赖对象”。\nIOC vs Factory 简单来说，IOC 与 工厂模式 分别代表了 push 与 pull 的机制：\n Pull 机制：类间接依赖于 Factory Method ，而 Factory Method 又依赖于具体类。 Push 机制：容器可以在一个位置配置所有相关组件，从而促进高维护和松耦合。  使用 工厂模式 的责任仍然在于类（尽管间接地）来创建新对象，而 依赖注入 将责任外包。\n循环依赖 Spring 为了解决单例的循环依赖问题，使用了 三级缓存 ，递归调用时发现 Bean 还在创建中即为循环依赖\n/** 一级缓存：用于存放完全初始化好的 bean **/ private final Map\u0026lt;String, Object\u0026gt; singletonObjects = new ConcurrentHashMap\u0026lt;String, Object\u0026gt;(256); /** 二级缓存：存放原始的 bean 对象（尚未填充属性），用于解决循环依赖 */ private final Map\u0026lt;String, Object\u0026gt; earlySingletonObjects = new HashMap\u0026lt;String, Object\u0026gt;(16); /** 三级级缓存：存放 bean 工厂对象，用于解决循环依赖 */ private final Map\u0026lt;String, ObjectFactory\u0026lt;?\u0026gt;\u0026gt; singletonFactories = new HashMap\u0026lt;String, ObjectFactory\u0026lt;?\u0026gt;\u0026gt;(16); /** bean 的获取过程：先从一级获取，失败再从二级、三级里面获取 创建中状态：是指对象已经 new 出来了但是所有的属性均为 null 等待被 init */  A 创建过程中需要 B，于是 A 将自己放到三级缓里面 ，去实例化 B B 实例化的时候发现需要 A，于是 B 先查一级缓存，没有，再查二级缓存，还是没有，再查三级缓存，找到了！  然后把三级缓存里面的这个 A 放到二级缓存里面，并删除三级缓存里面的 A B 顺利初始化完毕，将自己放到一级缓存里面（此时B里面的A依然是创建中状态）   然后回来接着创建 A，此时 B 已经创建结束，直接从一级缓存里面拿到 B ，然后完成创建，并将自己放到一级缓存里面 如此一来便解决了循环依赖的问题  "});index.add({'id':27,'href':'/interview/docs/basic/net/ip/','title':"IP",'content':"IP 地址分类  A类：8位网络号，0_ _ _ _ _ _ _，1.0.0.0 ~ 126.0.0.0 B类：16位网络号，10 _ _ ...，128.0.0.0 ~ 191.255.255.255 C类：24位网络号，110_ _ _...，192.0.0.0 ~ 223.255.255.255 D类：多播地址，1110_ _ _... E类：保留地址，1111_ _ _ ...  私有地址  A类:10.0.0.0 ~ 10.255.255.255(长度相当于1个A类IP地址) B类:172.16.0.0 ~ 172.31.255.255(长度相当于16个连续的B类IP地址) C类:192.168.0.0 ~ 192.168.255.255(长度相当于256个连续的C类IP地址)  特殊的IP地址   0.0.0.0：已经不是一个真正意义上的IP地址。它表示的是这样一个集合：所有不清楚的主机和目的网络。这里的“不清楚”是指在本机的路由表里没有特定条目指明如何到达。如果在网络设置中设置了缺省网关,那么系统会自动产生一个目的地址为0.0.0.0的缺省路由.对本机来说,它就是一个“收容所”,所有不认识的“三无”人员,一 律送进去。\n  255.255.255.255： 限制广播地址，对本机来说,这个地址指本网段内(同一广播域)的所有主机。这个地址不能被路由器转发。\n  127.0.0.1：本机地址主要用于测试。这样一个地址,是不能把它发到网络接口的。\n  "});index.add({'id':28,'href':'/interview/docs/java/proxy/','title':"Java 代理",'content':"代理 Java动态代理与CGLIB\n我们常说的代理分为静态代理和动态代理。\n 静态代理：代码中显式指定代理 动态代理：类比静态代理，可以发现，代理类不需要实现原接口了，而是实现InvocationHandler。  静态代理 因为需要对一些函数进行二次处理，或是某些函数不让外界知道时，可以使用代理模式，通过访问第三方，间接访问原函数的方式，达到以上目的。\n弊端 如果要想为多个类进行代理，则需要建立多个代理类，维护难度加大。\n仔细想想，为什么静态代理会有这些问题，是因为代理在编译期就已经决定，如果代理发生在运行期，这些问题解决起来就比较简单，所以动态代理的存在就很有必要了。\n动态代理 当动态生成的代理类调用方法时，会触发 invoke 方法，在 invoke 方法中可以对被代理类的方法进行增强。\n// 1. 首先实现一个InvocationHandler，方法调用会被转发到该类的invoke()方法。 class LogInvocationHandler implements InvocationHandler{ ... private Hello hello; public LogInvocationHandler(Hello hello) { this.hello = hello; } @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable { if(\u0026quot;sayHello\u0026quot;.equals(method.getName())) { logger.info(\u0026quot;You said: \u0026quot; + Arrays.toString(args)); } return method.invoke(hello, args); } } // 2. 然后在需要使用Hello的时候，通过JDK动态代理获取Hello的代理对象。 Hello hello = (Hello)Proxy.newProxyInstance( getClass().getClassLoader(), // 1. 类加载器 new Class\u0026lt;?\u0026gt;[] {Hello.class}, // 2. 代理需要实现的接口，可以有多个 new LogInvocationHandler(new HelloImp()));// 3. 方法调用的实际处理者 System.out.println(hello.sayHello(\u0026quot;I love you!\u0026quot;)); 通过动态代理可以很明显的看到它的好处，在使用静态代理时，如果不同接口的某些类想使用代理模式来实现相同的功能，将要实现多个代理类，但在动态代理中，只需要一个代理类就好了。\n除了省去了编写代理类的工作量，动态代理实现了可以在原始类和接口还未知的时候，就确定代理类的代理行为，当代理类与原始类脱离直接联系后，就可以很灵活地重用于不同的应用场景中。\n 继承了Proxy类，实现了代理的接口，由于java不能多继承，这里已经继承了Proxy类了，不能再继承其他的类，所以JDK的动态代理不支持对实现类的代理，只支持接口的代理。 提供了一个使用InvocationHandler作为参数的构造方法。 生成静态代码块来初始化接口中方法的Method对象，以及Object类的equals、hashCode、toString方法  弊端 代理类和委托类需要都实现同一个接口。也就是说只有实现了某个接口的类可以使用Java动态代理机制。但是，事实上使用中并不是遇到的所有类都会给你实现一个接口。因此，对于没有实现接口的类，就不能使用该机制。\n动态代理与静态代理的区别  Proxy类的代码被固定下来，不会因为业务的逐渐庞大而庞大； 代理对象是在程序运行时产生的，而不是编译期； 可以实现AOP编程，这是静态代理无法实现的； 解耦，如果用在web业务下，可以实现数据层和业务层的分离。 动态代理的优势就是实现无侵入式的代码扩展。 静态代理这个模式本身有个大问题，如果类方法数量越来越多的时候，代理类的代码量是十分庞大的。所以引入动态代理来解决此类问题  CGLib cglib 是针对类来实现代理的，他的 原理是对指定的目标类生成一个子类，并覆盖其中方法实现增强，但因为采用的是继承，所以不能对 final 修饰的类进行代理。同样的，final 方法是不能重载的，所以也不能通过CGLIB代理，遇到这种情况不会抛异常，而是会跳过 final 方法只代理其他方法。\nCGLIB 代理主要通过对字节码的操作，为对象引入间接级别，以控制对象的访问。 CGLIB 底层使用了ASM（一个短小精悍的字节码操作框架）来操作字节码生成新的类。\nCGLIB和Java动态代理的区别   Java 动态代理只能够对接口进行代理，不能对普通的类进行代理（因为所有生成的代理类的父类为 Proxy，Java 类继承机制不允许多重继承）；CGLIB能够代理普通类；\n  Java 动态代理使用 Java 原生的反射 API 进行操作，在生成类上比较高效；CGLIB 使用 ASM 框架直接对字节码进行操作，在类的执行过程中比较高效\n  "});index.add({'id':29,'href':'/interview/docs/java/jvm/dispatcher/','title':"Java 分派机制",'content':"Java分派机制 在Java中，符合“编译时可知，运行时不可变”这个要求的方法主要是静态方法和私有方法。这两种方法都不能通过继承或别的方法重写，因此它们适合在类加载时进行解析。\nJava虚拟机中有四种方法调用指令：\n invokestatic：调用静态方法。 invokespecial：调用实例构造器方法，私有方法和super。 invokeinterface：调用接口方法。 invokevirtual：调用以上指令不能调用的方法（虚方法）。  只要能被invokestatic和invokespecial指令调用的方法，都可以在解析阶段确定唯一的调用版本，符合这个条件的有：静态方法、私有方法、实例构造器、父类方法，他们在类加载的时候就会把符号引用解析为改方法的直接引用。这些方法被称为非虚方法，反之其他方法称为虚方法（final方法除外）。\n 虽然final方法是使用invokevirtual 指令来调用的，但是由于它无法被覆盖，多态的选择是唯一的，所以是一种非虚方法。\n 静态分派  对于类字段的访问也是采用静态分派\n People man = new Man()\n静态分派主要针对重载，方法调用时如何选择。在上面的代码中，People被称为变量的引用类型，Man被称为变量的实际类型。静态类型是在编译时可知的，而动态类型是在运行时可知的，编译器不能知道一个变量的实际类型是什么。\n编译器在重载时候通过参数的静态类型而不是实际类型作为判断依据。并且静态类型在编译时是可知的，所以编译器根据重载的参数的静态类型进行方法选择。\n 在某些情况下有多个重载，那编译器如何选择呢？ 编译器会选择\u0026quot;最合适\u0026quot;的函数版本，那么怎么判断\u0026quot;最合适“呢？越接近传入参数的类型，越容易被调用。\n 动态分派 动态分派主要针对重写，使用invokevirtual指令调用。invokevirtual指令多态查找过程：\n 找到操作数栈顶的第一个元素所指向的对象的实际类型，记为C。 如果在类型C中找到与常量中的描述符合简单名称都相符的方法，则进行访问权限校验，如果通过则返回这个方法的直接引用，查找过程结束；如果权限校验不通过，返回java.lang.IllegalAccessError异常。 否则，按照继承关系从下往上一次对C的各个父类进行第2步的搜索和验证过程。 如果始终没有找到合适的方法，则抛出 java.lang.AbstractMethodError异常。  虚拟机动态分派的实现 由于动态分派是非常繁琐的动作，而且动态分派的方法版本选择需要考虑运行时在类的方法元数据中搜索合适的目标方法，因此在虚拟机的实现中基于性能的考虑，在方法区中建立一个虚方法表（invokeinterface 有接口方法表），来提高性能。\n虚方法表中存放各个方法的实际入口地址。如果某个方法在子类没有重写，那么子类的虚方法表里的入口和父类入口一致，如果子类重写了这个方法，那么子类方法表中的地址会被替换为子类实现版本的入口地址。\n"});index.add({'id':30,'href':'/interview/docs/java/exception/','title':"Java 异常",'content':"Java异常 Java中有Error和Exception，它们都是继承自Throwable类。\n二者的不同之处 Exception：\n  可以是可被控制(checked) 或不可控制的(unchecked)。\n  表示一个由程序员导致的错误。\n  应该在应用程序级被处理。\n  Error：\n  总是不可控制的(unchecked)。\n  经常用来用于表示系统错误或低层资源的错误。\n  如何可能的话，应该在系统级被捕捉。\n  异常的分类   Checked exception: 这类异常都是Exception的子类。异常的向上抛出机制进行处理，假如子类可能产生A异常，那么在父类中也必须throws A异常。可能导致的问题：代码效率低，耦合度过高。\n  Unchecked exception: 这类异常都是RuntimeException的子类，虽然RuntimeException同样也是Exception的子类，但是它们是非凡的，它们不能通过client code来试图解决，所以称为Unchecked exception 。\n  "});index.add({'id':31,'href':'/interview/docs/java/generics/','title':"Java 泛型",'content':"Java泛型 开发人员在使用泛型的时候，很容易根据自己的直觉而犯一些错误。比如一个方法如果接收List\u0026lt;Object\u0026gt;作为形式参数，那么如果尝试将一个List\u0026lt;String\u0026gt;的对象作为实际参数传进去，却发现无法通过编译。虽然从直觉上来说，Object是String的父类，这种类型转换应该是合理的。但是实际上这会产生隐含的类型转换问题，因此编译器直接就禁止这样的行为。\n类型擦除 Java中的泛型基本上都是在编译器这个层次来实现的，在生成的Java字节代码中是不包含泛型中的类型信息的。使用泛型的时候加上的类型参数，会被编译器在编译的时候去掉，这个过程就称为类型擦除。如在代码中定义的List\u0026lt;Object\u0026gt;和List\u0026lt;String\u0026gt;等类型，在编译之后都会变成List。JVM看到的只是List，而由泛型附加的类型信息对JVM来说是不可见的。Java编译器会在编译时尽可能的发现可能出错的地方，但是仍然无法避免在运行时刻出现类型转换异常的情况。\n很多泛型的奇怪特性都与这个类型擦除的存在有关，包括：\n  泛型类并没有自己独有的Class类对象。比如并不存在List\u0026lt;String\u0026gt;.class或是List\u0026lt;Integer\u0026gt;.class，而只有List.class。\n  静态变量是被泛型类的所有实例所共享的。对于声明为MyClass\u0026lt;T\u0026gt;的类，访问其中的静态变量的方法仍然是 MyClass.myStaticVar。不管是通过new MyClass\u0026lt;String\u0026gt;还是new MyClass\u0026lt;Integer\u0026gt;创建的对象，都是共享一个静态变量。\n  泛型的类型参数不能用在Java异常处理的catch语句中。因为异常处理是由JVM在运行时刻来进行的。由于类型信息被擦除，JVM是无法区分两个异常类型MyException\u0026lt;String\u0026gt;和MyException\u0026lt;Integer\u0026gt;的。对于JVM来说，它们都是 MyException类型的。也就无法执行与异常对应的catch语句。\n  类型擦除的基本过程也比较简单，首先是找到用来替换类型参数的具体类。这个具体类一般是Object。如果指定了类型参数的上界的话，则使用这个上界。把代码中的类型参数都替换成具体的类。同时去掉出现的类型声明，即去掉\u0026lt;\u0026gt;的内容。比如T get()方法声明就变成了Object get()；List\u0026lt;String\u0026gt;就变成了List。接下来就可能需要生成一些桥接方法（bridge method）。这是由于擦除了类型之后的类可能缺少某些必须的方法。比如考虑下面的代码：\nclass MyString implements Comparable\u0026lt;String\u0026gt; { public int compareTo(String str) { return 0; } } 当类型信息被擦除之后，上述类的声明变成了class MyString implements Comparable。但是这样的话，类MyString就会有编译错误，因为没有实现接口Comparable声明的int compareTo(Object)方法。这个时候就由编译器来动态生成这个方法。\n通配符 在使用泛型类的时候，既可以指定一个具体的类型，如List\u0026lt;String\u0026gt;就声明了具体的类型是String；也可以用通配符?来表示未知类型，如List\u0026lt;?\u0026gt;就声明了List中包含的元素类型是未知的。 通配符所代表的其实是一组类型，但具体的类型是未知的。List\u0026lt;?\u0026gt;所声明的就是所有类型都是可以的。但是List\u0026lt;?\u0026gt;并不等同于List\u0026lt;Object\u0026gt;。List\u0026lt;Object\u0026gt;实际上确定了List中包含的是Object及其子类，在使用的时候都可以通过Object来进行引用。而List\u0026lt;?\u0026gt;则其中所包含的元素类型是不确定。其中可能包含的是String，也可能是 Integer。如果它包含了String的话，往里面添加Integer类型的元素就是错误的。正因为类型未知，就不能通过new ArrayList中的元素确总是可以用Object来引用的，因为虽然类型未知，但肯定是Object及其子类。考虑下面的代码：\npublic void wildcard(List\u0026lt;?\u0026gt; list) { list.add(1);//编译错误 }  如上所示，试图对一个带通配符的泛型类进行操作的时候，总是会出现编译错误。其原因在于通配符所表示的类型是未知的。\n 因为对于List\u0026lt;?\u0026gt;中的元素只能用Object来引用，在有些情况下不是很方便。在这些情况下，可以使用上下界来限制未知类型的范围。 如 List\u0026lt;? extends Number\u0026gt;说明List中可能包含的元素类型是Number及其子类。而List\u0026lt;? super Number\u0026gt;则说明List中包含的是Number及其父类。当引入了上界之后，在使用类型的时候就可以使用上界类中定义的方法。\n类型系统 在Java中，大家比较熟悉的是通过继承机制而产生的类型体系结构。比如String继承自Object。根据Liskov替换原则，子类是可以替换父类的。当需要Object类的引用的时候，如果传入一个String对象是没有任何问题的。但是反过来的话，即用父类的引用替换子类引用的时候，就需要进行强制类型转换。编译器并不能保证运行时刻这种转换一定是合法的。这种自动的子类替换父类的类型转换机制，对于数组也是适用的。 String[]可以替换Object[]。但是泛型的引入，对于这个类型系统产生了一定的影响。正如前面提到的List是不能替换掉List的。\n引入泛型之后的类型系统增加了两个维度：一个是类型参数自身的继承体系结构，另外一个是泛型类或接口自身的继承体系结构。第一个指的是对于 List\u0026lt;String\u0026gt;和List\u0026lt;Object\u0026gt;这样的情况，类型参数String是继承自Object的。而第二种指的是 List接口继承自Collection接口。对于这个类型系统，有如下的一些规则：\n  相同类型参数的泛型类的关系取决于泛型类自身的继承体系结构。即List\u0026lt;String\u0026gt;是Collection\u0026lt;String\u0026gt; 的子类型，List\u0026lt;String\u0026gt;可以替换Collection\u0026lt;String\u0026gt;。这种情况也适用于带有上下界的类型声明。\n  当泛型类的类型声明中使用了通配符的时候，其子类型可以在两个维度上分别展开。如对Collection\u0026lt;? extends Number\u0026gt;来说，其子类型可以在Collection这个维度上展开，即List\u0026lt;? extends Number\u0026gt;和Set\u0026lt;? extends Number\u0026gt;等；也可以在Number这个层次上展开，即Collection\u0026lt;Double\u0026gt;和Collection\u0026lt;Integer\u0026gt;等。如此循环下去，ArrayList\u0026lt;Long\u0026gt;和 HashSet\u0026lt;Double\u0026gt;等也都算是Collection\u0026lt;? extends Number\u0026gt;的子类型。\n  如果泛型类中包含多个类型参数，则对于每个类型参数分别应用上面的规则。\n  "});index.add({'id':32,'href':'/interview/docs/java/concurrent/thread/','title':"Java线程",'content':"Java线程 线程定义 线程（英语：thread）是操作系统能够进行运算调度的最小单位。它被包含在进程之中，是进程中的实际运作单位。一条线程指的是进程中一个单一顺序的控制流，一个进程中可以并发多个线程，每条线程并行执行不同的任务。在Unix System V及SunOS中也被称为轻量进程（lightweight processes），但轻量进程更多指内核线程（kernel thread），而把用户线程（user thread）称为线程。\n线程是独立调度和分派的基本单位。线程可以操作系统内核调度的内核线程，如Win32线程；由用户进程自行调度的用户线程，如Linux平台的POSIX Thread；或者由内核与用户进程，如Windows 7的线程，进行混合调度。\n同一进程中的多条线程将共享该进程中的全部系统资源，如虚拟地址空间，文件描述符和信号处理等等。但同一进程中的多个线程有各自的调用栈（call stack），自己的寄存器环境（register context），自己的线程本地存储（thread-local storage）。\n线程实现 Java中的线程都是调用的原生系统的本地函数，Java线程模型是基于操作系统原生线程模型实现的，实现线程有三种方式：内核线程实现、用户线程实现、混合线程实现。\n内核线程实现 直接由操作系统内核支持的线程，通过内核来完成进程切换。每个内核线程就是一个内核的分身，这样操作系统就可以同时处理多件事情，支持多线程的内核被称为多线程内核。\n程序一般不直接使用内核线程，而是使用一种高级接口——轻量级进程，轻量级进程就是我们通常意义上的线程，可以获得内核线程的支持，与内核线程构成1:1的线程模型。\n由于得到内核线程的支持，每个轻量级进程都成为一个独立的调度单元，即时有一个轻量级进程在系统调用中阻塞，也不会影响整个进程，但也有其局限性：由于是基于内核线程实现的，各种操作，如创建、销毁及同步，都需要进行系统调用。而系统调用代价较高，需要在内核态和用户态来回切换。\n用户线程实现 从广义上说，一个线程不是内核线程，就是用户线程，所以轻量级进程也属于用户线程。狭义的用户线程是指完全建立在用户空间上的，系统内核不能感知到其存在。\n用户线程的创建、同步、销毁和调度都是在用户空间实现的，因此相对较快，代价相对较低。这种用户线程和进程是N:1的线程模型。\n由于用户线程没有内核的支持，线程的创建、切换和调度是需要自己实现的，而且由于操作系统只把CPU资源分配到进程，那诸如“阻塞如何处理”、“多处理器系统中如何将线程映射到其他处理器”这类问题解决起来异常复杂。\n混合实现 这种实现模式将内核线程与用户线程一起使用，在这种方式下既存在用户线程，也存在轻量级进程。用户线程还是完全建立在用户空间，因此用户线程的创建、切换等操作依旧低廉。而操作系统提供的轻量级进程则作为用户线程和内核线程的桥梁，这样就可以使用内核提供的线程调度及处理器映射。这种实现下，用户线程和轻量级进程是M:N的模式。\nJava线程调度 线程调度分为协同式和抢占式。\n 协同式调度：线程的执行时间由线程自己控制，这种的实现很简单，但是很可能造成很严重的后果。 抢占式调度：由操作系统分配线程执行的时间，线程切换的决定权在操作系统。  有时候我们需要为某些线程多分配时间，这时我们就需要用到线程优先级的方法，Java提供了10种优先级。Java优先级是在操作系统的原生线程优先级上实现的，所以对于同一个优先级，不同的操作系统可能有不同的表现，也就是说 Java线程优先级不是可靠的。\nJava线程状态切换 Java线程模型定义了 6 种状态，在任意一个时间点，一个线程有且只有其中一个状态：\n 新建（New）：新建的Thread，尚未开始。 运行（Runable）：包含操作系统线程状态中的Running、Ready，也就是处于正在执行或正在等待CPU分配时间的状态。 无限期等待（Waiting）：处于这种状态的线程不会被分配CPU时间，等待其他线程唤醒。 限期等待（Timed Waiting）：处于这种状态的线程不会被分配CPU时间，在一定时间后会由系统自动唤醒。 阻塞（Blocked）：在等待获得排他锁。 结束（Terminated）：已终止的线程。  线程安全 多线程访问同一代码，不会产生不确定的结果。\nJava 线程池 "});index.add({'id':33,'href':'/interview/docs/java/jvm/memory-model/','title':"JVM 内存模型",'content':"JVM 内存模型 HotSpot 内存模型 \u0026ndash; JDK8\n Heap: Java 堆是可供各线程共享的运行时内存区域，是 Java 虚拟机所管理的内存区域中最大的一块。此区域非常重要，几乎所有的对象实例和数组实例都要在 Java 堆上分配，但随着 JIT 编译器及逃逸分析技术的发展，也可能会被优化为栈上分配 Internd String: 字符串字面量常量池 Calss Meta Data: 每一个类的结构信息，比如 字段 和 方法数据、构造函数和普通方法的字节码内容，还包括一些初始化的时候用到的特殊方法。 Runtime Constant Pool: 运行时常量池是 class 文件中每一个类或接口的 常量池表(Constant Pool)的运行时表示形式，是方法区的一部分。它包括了若干种不同的常量。常量池表存放编译器生成的 各种字面量和符号引用，这部分内容将在类加载后进入方法区的运行时常量池中存放。运行时常量池具有动态性，运行期间也可以将新的量放到运行时常量池中，典型的应用是 String 类的 intern 方法 Code Cache: JIT 编译缓存的本地代码 PC Register: CPU内部的寄存器中就包含一个程序计数器，存放程序执行的下一条指令地址。 JVM Stacks: Java 虚拟机栈也是线程私有的，每一条线程都拥有自己私有的Java 虚拟机栈，它与线程同时创建。它描述了 Java 方法执行的内存模型：每个方法在执行的同时都会创建一个栈帧，用于存储局部变量表、操作数栈、动态链接、方法出口等信息。 Native Stacks: 本地方法栈和 Java 虚拟机栈的作用相似，Java 虚拟机栈执行的是字节码，而本地方法栈执行的是 native 方法。本地方法栈使用传统的栈（C Stack）来支持 native 方法。   JDK 1.7开始，字符串常量和符号引用等就被移出永久代：\n 符号引用迁移至系统堆内存(Native Heap) 字符串字面量迁移至Java堆(Java Heap)   在 JVM 规范中，并不存在这么多分区，只包含 5 大分区：\n Metahod Area Heap JVM Stack PC Register Native Stack  在 HotSpot 中，方法区涵盖了除 Heap,JVM Stack,PC Register,Native Stack 之外的其他区域。\n参考链接  JEP 122: Remove the Permanent Generation Java JVM Run-time Data Areas - Javapapers JVM Internals 深入探究 JVM | Java 的内存区域解析 The Java Virtual Machine The Java HotSpot Performance Engine Architecture  "});index.add({'id':34,'href':'/interview/docs/java/gc/jvm-gc/','title':"JVM 垃圾回收",'content':"JVM垃圾回收  本片文章均指 HotSpot 的GC\n Java堆中存放着大量的Java对象实例，在垃圾收集器回收内存前，第一件事情就是确定哪些对象是“活着的”，哪些是可以回收的。\n引用计数算法 引用计数算法是判断对象是否存活的基本算法：给每个对象添加一个引用计数器，没当一个地方引用它的时候，计数器值加1；当引用失效后，计数器值减1。但是这种方法有一个致命的缺陷，当两个对象相互引用时会导致这两个都无法被回收。\n根搜索算法 在主流的商用语言中（Java、C#\u0026hellip;）都是使用根搜索算法来判断对象是否存活。对于程序来说，根对象总是可以访问的。从这些根对象开始，任何可以被触及的对象都被认为是\u0026quot;活着的\u0026quot;的对象。无法触及的对象被认为是垃圾，需要被回收。\nJava虚拟机的根对象集合根据实现不同而不同，但是总会包含以下几个方面：\n 虚拟机栈（栈帧中的本地变量表）中引用的对象。 方法区中的类静态属性引用的变量。 方法区中的常量引用的变量。 本地方法JNI的引用对象。  区分活动对象和垃圾的两个基本方法是引用计数和根搜索。 引用计数是通过为堆中每个对象保存一个计数来区分活动对象和垃圾。根搜索算法实际上是追踪从根结点开始的引用图。\n引用对象 引用对象封装了指向其他对象的连接：被指向的对象称为引用目标。Reference有三个直接子类SoftReference、WeakReference、PhantomReference分别代表：软引用、弱引用、虚引用。强引用在Java中是普遍存在的，类似Object o = new Object();这类引用就是强引用，强引用和以上引用的区别在于：强引用禁止引用目标被垃圾收集器收集，而其他引用不禁止。\n 当使用软引用、弱引用、虚引用时，并且对可触及性状态的改变有兴趣，可以把引用对象和引用队列关联起来。\n 对象有六种可触及状态变化：\n  强可触及：对象可以从根节点不通过任何引用对象搜索到。垃圾收集器不会回收这个对象的内存空间。\n  软可触及：对象可以从根节点通过一个或多个(未被清除的)软引用对象触及，垃圾收集器在要发生内存溢出前将这些对象列入回收范围中进行回收，如果该软引用对象和引用队列相关联，它会把该软引用对象加入队列。\n   SoftReference可以用来创建内存中缓存，JVM的实现需要在抛出OutOfMemoryError之前清除软引用，但在其他的情况下可以选择清理的时间或者是否清除它们。\n   弱可触及：对象可以从根节点开始通过一个或多个(未被清除的)弱引用对象触及，垃圾收集器在一次GC的时候会回收所有的弱引用对象，如果该弱引用对象和引用队列相关联，它会把该弱引用对象加入队列。\n  可复活的：对象既不是强可触及、软可触及、也不是弱可触及，但仍然可能通过执行某些终结方法复活到这几个状态之一。\n   Java类可以通过重写finalize方法复活准备回收的对象，但finalize方法只是在对象第一次回收时会调用。\n   虚可触及：垃圾收集器不会清除一个虚引用，所有的虚引用都必须由程序明确的清除。 同时也不能通过虚引用来取得一个对象的实例。\n  不可触及：不可触及对象已经准备好回收了。\n   若一个对象的引用类型有多个，那到底如何判断它的可达性呢？其实规则如下：\n 单条引用链的可达性以最弱的一个引用类型来决定； 多条引用链的可达性以最强的一个引用类型来决定；   垃圾回收算法 标记\u0026ndash;清除算法 首先标记出所有需要回收的对象，在标记完成后统一回收所有被标记的对象，标记的方法使用根搜索算法。主要有两个缺点：\n  效率问题，标记和清除的效率都不高。\n  空间问题，标记清除后会产生大量不连续的内存碎片。\n  复制回收算法 将可用内存分为大小相等的两份，在同一时刻只使用其中的一份。当这一份内存使用完了，就将还存活的对象复制到另一份上，然后将这一份上的内存清空。复制算法能有效避免内存碎片，但是算法需要将内存一分为二，导致内存使用率大大降低。\n标记\u0026ndash;整理算法 复制算法在对象存活率较高的情况下会复制很多的对象，效率会很低。标记\u0026ndash;整理算法就解决了这样的问题，标记过程和标记\u0026ndash;清除算法一样，但后续是将所有存活的对象都移动到内存的一端，然后清理掉端外界的对象。\n分代回收(HotSpot) 在JVM中不同的对象拥有不同的生命周期，因此对于不同生命周期的对象也可以采用不同的垃圾回收方法，以提高效率，这就是分代回收算法的核心思想。\n在不进行对象存活时间区分的情况下，每次垃圾回收都是对整个堆空间进行回收，花费的时间相对会长。同时，因为每次回收都需要遍历所有存活对象，但实际上，对于生命周期长的对象而言，这种遍历是没有效果的，因为可能进行了很多次遍历，但是他们依旧存在。因此，分代垃圾回收采用分治的思想，进行代的划分，把不同生命周期的对象放在不同代上，不同代上采用最适合它的垃圾回收方式进行回收。\nJVM中的共划分为三个代：新生代（Young Generation）、老年代（Old Generation）和永久代（Permanent Generation）。其中永久代主要存放的是Java类的类信息，与垃圾收集要收集的Java对象关系不大。\n  新生代：所有新生成的对象首先都是放在新生代的，新生代采用复制回收算法。新生代的目标就是尽可能快速的收集掉那些生命周期短的对象。新生代分三个区。一个Eden区，两个Survivor区(一般而言)。大部分对象在Eden区中生成。当Eden区满时，还存活的对象将被复制到Survivor区（两个中的一个），当这个Survivor区满时，此区的存活对象将被复制到另外一个Survivor区，当一个对象经过多次的 GC 后还没有被回收，那么它将被移动到“年老区(Tenured)”。需要注意，Survivor 的两个区是对称的，没先后关系，所以同一个区中可能同时存在从 Eden 复制过来 对象，和从前一个 Survivor 复制过来的对象，而复制到年老区的只有从第一个 Survivor 去过来的对象。而且，Survivor 区总有一个是空的。\n 在HotSpot虚拟机内部默认Eden和Survivor的大小比例是8:1， 也就是每次新生代中可用内存为整个新生代的90%，这大大提高了复制回收算法的效率。\n   老年代：在新生代中经历了N次垃圾回收后仍然存活的对象，就会被放到老年代中，老年代采用标记整理回收算法。因此，可以认为老年代中存放的都是一些生命周期较长的对象。\n  永久代：HotSpot 的方法区实现，用于存储类信息、常量池、静态变量、JIT编译后的代码等数据\n  HotSpot 各版本永久代变化  在Java 6中，方法区中包含的数据，除了JIT编译生成的代码存放在native memory的CodeCache区域，其他都存放在永久代； 在Java 7中，Symbol 的存储从 PermGen 移动到了 native memory ，并且把静态变量从instanceKlass末尾（位于PermGen内）移动到了java.lang.Class对象的末尾（位于普通Java heap内）； 在Java 8中，永久代被彻底移除，取而代之的是另一块与堆不相连的本地内存——元空间（Metaspace）,‑XX:MaxPermSize 参数失去了意义，取而代之的是-XX:MaxMetaspaceSize。  移除永久代 Java 8 彻底将永久代 (PermGen) 移除出了 HotSpot JVM，将其原有的数据迁移至 Java Heap 或 Metaspace。\n在 HotSpot JVM 中，永久代中用于存放类和方法的元数据以及常量池，比如Class和Method。每当一个类初次被加载的时候，它的元数据都会放到永久代中。\n永久代是有大小限制的，因此如果加载的类太多，很有可能导致永久代内存溢出，即万恶的 java.lang.OutOfMemoryError: PermGen ，为此我们不得不对虚拟机做调优。\n那么，Java 8 中 PermGen 为什么被移出 HotSpot JVM 了？\n 由于 · 内存经常会溢出，引发恼人的 java.lang.OutOfMemoryError: PermGen，因此 JVM 的开发者希望这一块内存可以更灵活地被管理，不要再经常出现这样的 OOM 移除 PermGen 可以促进 HotSpot JVM 与 JRockit VM 的融合，因为 JRockit 没有永久代。  根据上面的各种原因，PermGen 最终被移除，方法区移至 Metaspace，字符串常量移至 Java Heap。\n元空间 首先，Metaspace（元空间）是哪一块区域？官方的解释是：\n In JDK 8, classes metadata is now stored in the native heap and this space is called Metaspace.\n 也就是说，JDK 8 开始把类的元数据放到本地堆内存(native heap)中，这一块区域就叫 Metaspace，中文名叫元空间。\n垃圾回收触发条件 由于对象进行了分代处理，因此垃圾回收区域、时间也不一样。GC有两种类型：Scavenge GC和Full GC。对于一个拥有终结方法的对象，在垃圾收集器释放对象前必须执行终结方法。但是当垃圾收集器第二次收集这个对象时便不会再次调用终结方法。\nScavenge GC 一般情况下，当新对象生成，并且在 Eden 申请空间失败时，就会触发 Scavenge GC，对 Eden 区域进行 GC ，清除非存活对象，并且把尚且存活的对象移动到 Survivor 区，然后整理 Survivor 的两个区。这种方式的 GC是对新生代的 Eden 区进行，不会影响到老年代。因为大部分对象都是从 Eden 区开始的，同时 Eden 区不会分配的很大，所以 Eden 区的 GC 会频繁进行。\nFull GC 对整个堆进行整理，包括 Young 、 Tenured 和 Perm 。Full GC因为需要对整个对进行回收，所以比 Scavenge GC 要慢，因此应该尽可能减少 Full GC 的次数。在对 JVM 调优的过程中，很大一部分工作就是对于 FullGC 的调节。有如下原因可能导致Full GC：\n 老年代（Tenured）被写满 永久代（Perm）被写满 System.gc()被显示调用  堆外内存 GC Native Memory Tracking\nDirectBuffer 的引用是直接分配在堆得 Old 区的，因此其回收时机是在 FullGC 时。因此，需要避免频繁的分配 DirectBuffer ，这样很容易导致 Native Memory 溢出。\nDirectByteBuffer 申请的直接内存，不再GC范围之内，无法自动回收。JDK提供了一种机制，可以为堆内存对象注册一个钩子函数(其实就是实现 Runnable 接口的子类)，当堆内存对象被GC回收的时候，会回调run方法，我们可以在这个方法中执行释放 DirectByteBuffer 引用的直接内存，即在run方法中调用 Unsafe 的 freeMemory 方法。注册是通过sun.misc.Cleaner类来实现的。\n垃圾收集器 垃圾收集器是内存回收的具体实现，下图展示了7种用于不同分代的收集器，两个收集器之间有连线表示可以搭配使用。下面的这些收集器没有“最好的”这一说，每种收集器都有最适合的使用场景。\nSerial收集器 Serial收集器是最基本的收集器，这是一个单线程收集器，它“单线程”的意义不仅仅是说明它只用一个线程去完成垃圾收集工作，更重要的是在它进行垃圾收集工作时，必须暂停其他工作线程，直到它收集完成。Sun将这件事称之为”Stop the world“。\n 没有一个收集器能完全不停顿，只是停顿的时间长短。\n 虽然Serial收集器的缺点很明显，但是它仍然是JVM在Client模式下的默认新生代收集器。它有着优于其他收集器的地方：简单而高效（与其他收集器的单线程比较），Serial收集器由于没有线程交互的开销，专心只做垃圾收集自然也获得最高的效率。在用户桌面场景下，分配给JVM的内存不会太多，停顿时间完全可以在几十到一百多毫秒之间，只要收集不频繁，这是完全可以接受的。\nParNew收集器 ParNew是Serial的多线程版本，在回收算法、对象分配原则上都是一致的。ParNew收集器是许多运行在Server模式下的默认新生代垃圾收集器，其主要在于除了Serial收集器，目前只有ParNew收集器能够与CMS收集器配合工作。\nParallel Scavenge收集器（1.8默认新生代） Parallel Scavenge收集器是一个新生代垃圾收集器，其使用的算法是复制算法，也是并行的多线程收集器。\nParallel Scavenge 收集器更关注可控制的吞吐量，吞吐量等于运行用户代码的时间/(运行用户代码的时间+垃圾收集时间)。直观上，只要最大的垃圾收集停顿时间越小，吞吐量是越高的，但是GC停顿时间的缩短是以牺牲吞吐量和新生代空间作为代价的。比如原来10秒收集一次，每次停顿100毫秒，现在变成5秒收集一次，每次停顿70毫秒。停顿时间下降的同时，吞吐量也下降了。\n停顿时间越短就越适合需要与用户交互的程序；而高吞吐量则可以最高效的利用CPU的时间，尽快的完成计算任务，主要适用于后台运算。\nSerial Old收集器 Serial Old收集器是Serial收集器的老年代版本，也是一个单线程收集器，采用“标记-整理算法”进行回收。其运行过程与Serial收集器一样。\nParallel Old收集器（1.8默认老年代） Parallel Old收集器是Parallel Scavenge收集器的老年代版本，使用多线程和标记-整理算法进行垃圾回收。其通常与Parallel Scavenge收集器配合使用，“吞吐量优先”收集器是这个组合的特点，在注重吞吐量和CPU资源敏感的场合，都可以使用这个组合。\nCMS 收集器 CMS（Concurrent Mark Sweep）收集器是一种以获取最短停顿时间为目标的收集器，CMS收集器采用标记--清除算法，运行在老年代。主要包含以下几个步骤：\n 初始标记 并发标记 重新标记 并发清除  其中初始标记和重新标记仍然需要“Stop the world”。初始标记仅仅标记GC Root能直接关联的对象，并发标记就是进行GC Root Tracing过程，而重新标记则是为了修正并发标记期间，因用户程序继续运行而导致标记变动的那部分对象的标记记录。\n由于整个过程中最耗时的并发标记和并发清除，收集线程和用户线程一起工作，所以总体上来说，CMS收集器回收过程是与用户线程并发执行的。虽然CMS优点是并发收集、低停顿，很大程度上已经是一个不错的垃圾收集器，但是还是有三个显著的缺点：\n  CMS收集器对CPU资源很敏感。在并发阶段，虽然它不会导致用户线程停顿，但是会因为占用一部分线程（CPU资源）而导致应用程序变慢。\n  CMS收集器不能处理浮动垃圾。所谓的“浮动垃圾”，就是在并发标记阶段，由于用户程序在运行，那么自然就会有新的垃圾产生，这部分垃圾被标记过后，CMS无法在当次集中处理它们，只好在下一次GC的时候处理，这部分未处理的垃圾就称为“浮动垃圾”。也是由于在垃圾收集阶段程序还需要运行，即还需要预留足够的内存空间供用户使用，因此CMS收集器不能像其他收集器那样等到老年代几乎填满才进行收集，需要预留一部分空间提供并发收集时程序运作使用。要是CMS预留的内存空间不能满足程序的要求，这是JVM就会启动预备方案：临时启动Serial Old收集器来收集老年代，这样停顿的时间就会很长。\n  由于CMS使用标记\u0026ndash;清除算法，所以在收集之后会产生大量内存碎片。当内存碎片过多时，将会给分配大对象带来困难，这是就会进行Full GC。\n  G1收集器 G1收集器与CMS相比有很大的改进：\n G1收集器采用标记\u0026ndash;整理算法实现。 可以非常精确地控制停顿。  G1收集器可以实现在基本不牺牲吞吐量的情况下完成低停顿的内存回收，这是由于它极力的避免全区域的回收，G1收集器将Java堆（包括新生代和老年代）划分为多个区域（Region），并在后台维护一个优先列表，每次根据允许的时间，优先回收垃圾最多的区域 。\n"});index.add({'id':35,'href':'/interview/docs/java/jvm/jvm-architecture/','title':"JVM 架构",'content':"JVM架构 Java虚拟机简介 “Java虚拟机”可能指如下三个不同的东西\n 抽象规范 一个具体的实现 一个运行中的虚拟机实例  每个Java程序都运行在某个具体的Java虚拟机实现的实例上。一个Java虚拟机的实例负责运行一个Java程序。当启动一个Java程序的时候，一个虚拟机的实例也就诞生了。当该程序关闭退出时，这个虚拟机实例也就随之消亡。\n线程介绍 在Java虚拟机内部有两种线程：\n  守护线程：通常是由虚拟机自己使用，比如GC线程。但是，Java程序也可以把它自己创建的任何线程标记为守护线程（public final void setDaemon(boolean on)来设置，但必须在start()方法之前调用）。\n  非守护线程：main方法执行的线程，我们通常也称为用户线程。\n   只要有任何的非守护线程在运行，Java程序也会继续运行。当该程序中所有的非守护线程都终止时，虚拟机实例将自动退出（守护线程随JVM一同结束工作）。\n 守护线程中不适合进行IO、计算等操作，因为守护线程是在所有的非守护线程退出后结束，这样并不能判断守护线程是否完成了相应的操作，如果非守护线程退出后，还有大量的数据没来得及读写，这将造成很严重的后果。\n web服务器中的Servlet，容器启动时后台初始化一个服务线程，即调度线程，负责处理http请求，然后每个请求过来调度线程从线程池中取出一个工作者线程来处理该请求，从而实现并发控制的目的。\n Java虚拟机体系结构 每个Java虚拟机都有一个类装载器子系统，他根据给定的全限定名来装在类型。同样，每个Java虚拟机都有一个执行引擎，它负责执行那些包含在被装载类的方法中的指令。当Java虚拟机运行一个程序时，它需要内存来存储很多东西，例如：字节码，从已装载的class文件中得到的其他信息，程序创建的对象，传递给方法的参数，返回值，局部变量，以及运算的中间结果等等，Java虚拟机把这些东西都组织到几个“运行时数据区”中，以便管理。\n每个Java虚拟机实例都有一个方法区以及一个堆， 他们是由 该虚拟机实例中所有线程共享的。 当虚拟机装载一个class文件时，它会从这个class文件包含的二进制数据中解析类型信息。然后把这些类型信息放到方法区中。 当程序运行的时候，虚拟机会把所有该程序在运行时创建的对象都放到堆中。\n每个新线程都会得到它自己的PC寄存器(程序计数器)以及一个Java栈。\n  如果线程正在执行的是一个Java方法(非Native方法)。那么PC寄存器的值将总指向下一条将被执行的指令，而 它的Java栈则总是存储该线程中Java方法调用的转台\u0026ndash;包括它的局部变量、被调用时传进来的参数、返回值以及运算的中间结果等等。\n  Native方法调用的状态，则是以某种依赖于具体实现的方式存储在本地方法栈中，也可能是在寄存器或者其他某些与特定实现相关的内存区中。\n  Java栈是由很多的栈帧(stack frame)或者说帧(frame)组成的，一个栈帧包含一个Java方法调用状态。 当现场调用一个Java方法的时候，虚拟机压入一个新的栈帧到该线程的Java栈中：当该方法返回时，这个栈帧被从Java栈中弹出并抛弃\nJava虚拟机没有指令寄存器，其指令集使用Java栈来存储中间数据。这样设计的原因是为了保持Java虚拟机的指令集尽量紧凑、同时也便于Java虚拟机在那些只有很少通用寄存器的平台上实现。另外，Java虚拟机这种基于栈的体系结构，也有助于运行时某些虚拟机实现的动态编译器和即时编译器的代码优化。\n  这些内存区域是私有的，任何线程都不能访问另外一个线程的PC寄存器或Java栈。 图中是一个虚拟机实例的快照，它有三个线程正在执行。线程1和线程2都正在执行Java方法，而线程3在执行Native方法。   数据类型 数据类型分为两种：\n 基本类型：基本类型的变量持有原始值。 引用类型：引用类型的变量持有引用值。引用值是指对某个对象的引用，而不是该对象本身。    基本类型:\n Java语言中的所有基本类型都是Java虚拟机中的基本类型。但boolean有点特别，虽然Java虚拟机也把boolean看做基本类型，但指令集对boolean只有很有限的支持。 当编译器把Java源码编译成字节码时，它会用int或byte来表示boolean。在Java虚拟机中false是由整数'0'表示，所有的非零整数都表示true，涉及boolean值的操作则会用int。另外boolean数组是当做byte数组来访问的， 但是在“堆”区，它也可以被表示为位域。 Java虚拟机的基本类型的值域在任何地方都是一致的， 比如：不管底层主机平台是什么，一个long在任何虚拟机中总是一个64位二进制补码表示的又复活整数。 Java虚拟机中有一个值在内部使用的基本类型returnAddress，Java程序员不能使用这个类型。这个基本类型是用来实现Java程序中的finally子句。    引用类型：\nJava虚拟机中有三种引用类型，它们的值都是对动态创建对象的引用：\n 类类型：类实例(对象)的引用。 接口类型：是对实现了该接口的某个类实例的引用。 数组类型：数组对象的引用，在Java虚拟机中数组是个真正的对象。   还有一个特殊的引用值\u0026ndash;NULL，它表示引用变量没有引用任何对象。\n   Java虚拟机规范定义了每一种数据类型的取值范围，但没有定义它们的位宽。存储这些类型的值所需的占位宽度，是由具体的虚拟机实现的设计者决定的。\n类装载器 Java类加载器是Java运行时环境（Java Runtime Environment）的一部分，负责动态加载Java类到Java虚拟机的内存空间中。类通常是按需加载，即第一次使用该类时才加载。 由于有了类加载器，Java运行时系统不需要知道文件与文件系统。每个Java类必须由某个类加载器装入到内存。\n类装载器子系统涉及Java虚拟机的其他几个组成部分，以及几个来自java.lang库的类。比如，用户自定义的类装载器只是普通的Java对象，它的类必须派生自java.lang.ClassLoader。ClassLoader中定义的方法为程序提供了访问类装载器机制的接口。此外，对于每个被装载的类型，Java虚拟机都会为他创建一个java.lang.Class类的实例来代表该类型。和所有其他对象一样，用户自定义的类装载器以及Class类的实例都放在内存中的堆区，而装载的类型信息都位于方法区。\n类装载器子系统除了要定位和导入二进制class文件外，还必须负责验证被导入类的正确性，为变量分配初始化内存，以及帮助解析符号引用。这些动作必须严格按一下顺序完成：\n 装载\u0026ndash;查找并装载类型的二进制数据。 链接\u0026ndash;执行验证、准备以及解析(可选) - 验证 确保被导入类型的正确性 - 准备 为类变量分配内存，并将其初始化为默认值。 - 解析 把类型中的符号引用转换为直接引用。 初始化\u0026ndash;把类变量初始化为正确的初始值。  方法法区 方法区（method area）只是JVM规范中定义的一个概念，用于存储类信息、常量池、静态变量、JIT编译后的代码等数据，具体放在哪里，不同的实现可以放在不同的地方。\n在Java虚拟机中，关于被装载类型的信息存储在逻辑上一个被称为方法区的内存中。 当虚拟机装载某个类型市，它使用类装载器定位相应的class文件，然后读入这个class文件\u0026ndash;一个线性二进制数据流，然后把他传输到虚拟机中。紧接着虚拟机提取其中的类型信息，并将这些信息存储到方法区。该类型的类(静态)变量同样也是存储在方法区内。\n 当虚拟机运行Java程序时，它会查找使用存储在方法区中的类型信息。\n 由于所有线程都共享方法区，因此他们对方法区数据的访问必须设计为线程安全的。 方法区的大小不必是固定的，虚拟机可以根据应用需要动态调整。同样，方法区也不必是连续的，方法区可以在一个堆中自由分配。方法区也可以被垃圾收集\u0026ndash;这里涉及到类的卸载。\n方法区中包含的信息：\n 类型信息：对每个装载的类型，虚拟机都会在方法区存储一下信息 - 这个类型的全限定名 - 这个类型的直接父类的全限定名 - 这个类型是类类型还是接口类型 - 这个类型的访问修饰符 - 任何直接父接口的全限定名的有序列表 常量池：虚拟机必须为每个被装载的类型维护一个常量池。常量池就是该类型所用常量的一个有序集合，包直接常量(string,integer\u0026hellip;)和对其他类型、字段和方法的符号引用。池中的数据项通过索引访问。 字段信息。 方法信息。 类变量。 编译时常量。 指向ClassLoader类的引用。 指向Class类的引用。 方法表：为了提高访问效率，虚拟机对每个装载的非抽象类都生成一个方法表，把它作为类信息的一部分，它主要存储了所有它的实例可能被调用的实例方法的直接引用，包括从父类继承的 实例 方法。  "});index.add({'id':36,'href':'/interview/docs/java/jvm/jvm-class-load-init/','title':"JVM 类加载",'content':"JVM类加载三步走 简介 Java虚拟机通过装载、连接和初始化一个类型，使该类型可以被正在运行的Java程序使用。\n 装载：把二进制形式的Java类型读入Java虚拟机中。 连接：把装载的二进制形式的类型数据合并到虚拟机的运行时状态中去。 1. 验证：确保Java类型数据格式正确并且适合于Java虚拟机使用。 2. 准备：负责为该类型分配它所需内存。 3. 解析：把常量池中的符号引用转换为直接引用。(可推迟到运行中的程序真正使用某个符号引用时再解析) 初始化：为类变量赋适当的初始值  所有Java虚拟机实现必须在每个类或接口首次主动使用时初始化。以下六种情况符合主动使用的要求：\n 当创建某个类的新实例时(new、反射、克隆、序列化) 调用某个类的静态方法 使用某个类或接口的静态字段，或对该字段赋值(用final修饰的静态字段除外，它被初始化为一个编译时常量表达式) 当调用Java API的某些反射方法时。 初始化某个类的子类时。 当虚拟机启动时被标明为启动类的类。  除以上六种情况，所有其他使用Java类型的方式都是被动的，它们不会导致Java类型的初始化。\n 对于接口来说，只有在某个接口声明的非常量字段被使用时，该接口才会初始化，而不会因为事先这个接口的子接口或类要初始化而被初始化。\n 父类需要在子类初始化之前被初始化，所以这些类应该被装载了。当实现了接口的类被初始化的时候，不需要初始化父接口。然而，当实现了父接口的子类(或者是扩展了父接口的子接口)被装载时，父接口也要被装载。(只是被装载，没有初始化)\n装载  通过该类型的全限定名，产生一个代表该类型的二进制数据流。 解析这个二进制数据流为方法去内的内部数据结构。 创建一个表示该类型的java.lang.Class类的实例。  Java虚拟机在识别Java class文件，产生了类型的二进制数据后，Java虚拟机必须把这些二进制数据解析为与实现相关的内部数据结构。装载的最终产品就是Class实例，它称为Java程序与内部数据结构之间的接口。要访问关于该类型的信息(存储在内部数据结构中)，程序就要调用该类型对应的Class实例的方法。这样一个过程，就是把一个类型的二进制数据解析为方法区中的内部数据结构，并在堆上建立一个Class对象的过程，这被称为\u0026quot;创建\u0026quot;类型。\n验证 确认装载后的类型符合Java语言的语义，并且不会危及虚拟机的完整性。\n 装载时验证：检查二进制数据以确保数据全部是预期格式、确保除Object之外的每个类都有父类、确保该类的所有父类都已经被装载。 正式验证阶段：检查final类不能有子类、确保final方法不被覆盖、确保在类型和超类型之间没有不兼容的方法声明(比如拥有两个名字相同的方法，参数在数量、顺序、类型上都相同，但返回类型不同)。 符号引用的验证：当虚拟机搜寻一个被符号引用的元素(类型、字段或方法)时，必须首先确认该元素存在。如果虚拟机发现元素存在，则必须进一步检查引用类型有访问该元素的权限。  准备 当Java虚拟机装载一个类，并执行了一些验证之后，类就可以进入准备阶段。在准备阶段，Java虚拟机为类变量分配内存，设置默认初始值。但在到到初始化阶段之前，类变量都没有被初始化为真正的初始值。\n boolean在内部常常被实现为一个int，会被默认初始化为0。\n 解析 类型经过连接的前两个阶段\u0026ndash;验证和准备\u0026ndash;之后，就可以进入第三个阶段\u0026ndash;解析。解析的过程就是在类型的常量池总寻找类、接口、字段和方法的符号引用，把这些符号引用替换为直接引用的过程。\n  类或接口的解析：判断所要转化成的直接引用是数组类型，还是普通的对象类型的引用，从而进行不同的解析。\n  字段解析：对字段进行解析时，会先在本类中查找是否包含有简单名称和字段描述符都与目标相匹配的字段，如果有，则查找结束；如果没有，则会按照继承关系从上往下递归搜索该类所实现的各个接口和它们的父接口，还没有，则按照继承关系从上往下递归搜索其父类，直至查找结束，\n  初始化 为类变量赋予“正确”的初始值。这里的“正确”的初始值是指程序员希望这个类变量所具备的初始值。所有的类变量(即静态量)初始化语句和类型的静态初始化器都被Java编译器收集在一起，放到一个特殊的方法中。 对于类来说，这个方法被称作类初始化方法；对于接口来说，它被称为接口初始化方法。在类和接口的class文件中，这个方法被称为\u0026lt;clinit\u0026gt;。\n初始化类的步骤：  如果存在直接父类，且直接父类没有被初始化，先初始化直接父类。 如果类存在一个类初始化方法，执行此方法。  这个步骤是递归执行的，即第一个初始化的类一定是Object。初始化接口并不需要初始化它的父接口。\nJava虚拟机必须确保初始化过程被正确地同步。 如果多个线程需要初始化一个类，仅仅允许一个线程来进行初始化，其他线程需等待。\n 这个特性可以用来写单例模式。\n CInit 方法  对于静态变量和静态初始化语句来说：执行的顺序和它们在类或接口中出现的顺序有关。 并非所有的类都需要在它们的class文件中拥有\u0026lt;clinit\u0026gt;()方法， 如果类没有声明任何类变量，也没有静态初始化语句，那么它就不会有\u0026lt;clinit\u0026gt;()方法。如果类声明了类变量，但没有明确的使用类变量初始化语句或者静态代码块来初始化它们，也不会有\u0026lt;clinit\u0026gt;()方法。如果类仅包含静态final常量的类变量初始化语句，而且这些类变量初始化语句采用编译时常量表达式，类也不会有\u0026lt;clinit\u0026gt;()方法。只有那些需要执行Java代码来赋值的类才会有\u0026lt;clinit\u0026gt;() final常量：Java虚拟机在使用它们的任何类的常量池或字节码中直接存放的是它们表示的常量值。  主动使用和被动使用 主动使用有六种情况，在前面已经写过。\n使用一个非常量的静态字段只有当类或接口的确声明了这个字段时才是主动使用。 比如：类中声明的字段可能被子类引用；接口中声明的字段可能被子接口或实现了这个接口的类引用。对于子类、子接口或实现了接口的类来说，这是被动使用\u0026ndash;不会触发它们的初始化。只有当字段的确是被类或接口声明的时候才是主动使用。\n"});index.add({'id':37,'href':'/interview/docs/java/jvm/jvm-class-loader/','title':"JVM 类加载器",'content':"类加载器 简介 Java类加载器是Java运行时环境（Java Runtime Environment）的一部分，负责动态加载Java类到Java虚拟机的内存空间中。类通常是按需加载，即第一次使用该类时才加载。 由于有了类加载器，Java运行时系统不需要知道文件与文件系统。每个Java类必须由某个类加载器装入到内存。\n类装载器子系统涉及Java虚拟机的其他几个组成部分，以及几个来自java.lang库的类。比如，用户自定义的类装载器只是普通的Java对象，它的类必须派生自java.lang.ClassLoader。ClassLoader中定义的方法为程序提供了访问类装载器机制的接口。此外，对于每个被装载的类型，Java虚拟机都会为他创建一个java.lang.Class类的实例来代表该类型。和所有其他对象一样，用户自定义的类装载器以及Class类的实例都放在内存中的堆区，而装载的类型信息都位于方法区。\n类装载器子系统除了要定位和导入二进制class文件外，还必须负责验证被导入类的正确性，为变量分配初始化内存，以及帮助解析符号引用。这些动作必须严格按一下顺序完成：\n 装载\u0026ndash;查找并装载类型的二进制数据。 链接\u0026ndash;执行验证、准备以及解析(可选) - 验证：确保被导入类型的正确性 - 准备：为类变量分配内存，并将其初始化为默认值。 - 解析：把类型中的符号引用转换为直接引用。 初始化\u0026ndash;把类变量初始化为正确的初始值。 使用 卸载：类加载器加载的每个类和类加载器本身都被没有引用  分类 在Java虚拟机中存在多个类装载器，Java应用程序可以使用两种类装载器：\n  启动(bootstrap)类装载器：此装载器是Java虚拟机实现的一部分。由原生代码（如C语言）编写，不继承自java.lang.ClassLoader。负责加载核心Java库，存储在\u0026lt;JAVA_HOME\u0026gt;/jre/lib目录中。（如果Java虚拟机在已有操作系统中实现为C程序，那么启动类加载器就是此C程序的一部分） 启动类装载器通常使用某种默认的方式从本地磁盘中加载类，包括Java API。\n  用户自定义类装载器：（包含但不止，扩展类加载器以及系统类加载器） ，继承自Java中的java.lang.ClassLoader类，Java应用程序能在运行时安装用户自定义类装载器，这种累装载器使用自定义的方式来装载类。用户定义的类装载器能用Java编写，能够被编译为Class文件，能被虚拟机装载，还能像其他对象一样实例化。它们实际上只是运行中的Java程序可执行代码的一部分。一般JVM都会提供一些基本实现。应用程序的开发人员也可以根据需要编写自己的类加载器。JVM中最常使用的是系统类加载器（system），它用来启动Java应用程序的加载。 通过java.lang.ClassLoader.getSystemClassLoader() 可以获取到该类加载器对象。该类由sun.misc.Launcher$AppClassLoader实现。\n  全盘负责双亲委托机制 全盘负责是指当一个ClassLoader装载一个类的时，除非显式地使用另一个ClassLoader，该类所依赖及引用的类也由这个ClassLoader载入；“双亲委托机制”是指先委托父装载器寻找目标类，只有在找不到的情况下才从自己的类路径中查找并装载目标类。这一点是从安全角度考虑的，试想如果有人编写了一个恶意的基础类（如java.lang.String）并装载到JVM中将会引起多么可怕的后果。但是由于有了“全盘负责委托机制”，java.lang.String永远是由根装载器来装载的，这样就避免了上述事件的发生。\n类加载器需要完成的最终功能是定义一个Java类，即把Java字节代码转换成JVM中的java.lang.Class类的对象。但是类加载的过程并不是这么简单。Java类加载器有两个比较重要的特征：\n  层次组织结构指的是每个类加载器都有一个父类加载器，通过getParent()方法可以获取到。类加载器通过这种父亲-后代的方式组织在一起，形成树状层次结构。\n  代理模式则指的是一个类加载器既可以自己完成Java类的定义工作，也可以代理给其它的类加载器来完成。由于代理模式的存在，启动一个类的加载过程的类加载器和最终定义这个类的类加载器可能并不是一个。前者称为初始类加载器，而后者称为定义类加载器。\n  两者的关联在于：在每个类被装载的时候，Java虚拟机都会监视这个类，看它到底是被启动类装载器还是被用户自定义类装载器装载。当被装载的类引用了另外一个类的时候，虚拟机就会使用装载第一个类的类装载器装载被引用的类。\n 注意：JVM加载类A，并使用A的ClassLoader去加载B，但B的类加载器并不一定和A的类加载器一致，这是因为有双亲委托机制的存在。\n 一般的类加载器在尝试自己去加载某个Java类之前，会 首先代理给其父类加载器。当父类加载器找不到的时候，才会尝试自己加载。这个逻辑是封装在java.lang.ClassLoader类的loadClass()方法中的。一般来说，父类优先的策略就足够好了。在某些情况下，可能需要采取相反的策略，即先尝试自己加载，找不到的时候再代理给父类加载器。这种做法在Java的Web容器中比较常见，也是Servlet规范推荐的做法。 比如，Apache Tomcat为每个Web应用都提供一个独立的类加载器，使用的就是自己优先加载的策略。IBM WebSphere Application Server则允许Web应用选择类加载器使用的策略。\n 假设 类加载器B2被要求装载类MyClass，在parent delegation模型下，类加载器B2首先请求类加载器B代为装载，类加载器B再请求系统类装载器去装载MyClass，系统类装载器也会继续请求它的Parent扩展类加载器去装载MyClass，以此类推直到引导类装载器。若引导类装载器能成功装载，则将MyClass所对应的Class对象的reference逐层返回到类加载器B2，若引导类装载器不能成功装载，下层的扩展类装载器将尝试装载，并以此类推直到类装载器B2如果也不能成功装载，则装载失败。\n  需要指出的是，Class Loader是对象，它的父子关系和类的父子关系没有任何关系。一对父子loader可能实例化自同一个 Class，也可能不是，甚至父loader实例化自子类，子loader实例化自父类。\n defineClass vs findClass vs loadClass  loadclass：判断是否已加载，使用双亲委派模型，请求父加载器，都为空，使用 findclass findclass：根据名称或位置加载 .class 字节码,然后使用 defineClass defineclass：解析定义 .class 字节流，返回 class 对象  运行时包 类加载器的一个重要用途是 在JVM中为相同名称的Java类创建隔离空间。在JVM中，判断两个类是否相同，不仅是根据该类的二进制名称，还需要根据两个类的定义类加载器。 只有两者完全一样，才认为两个类的是相同的。\n在允许两个类型之间对包内可见的成员进行访问前，虚拟机不但要确定这个两个类型属于同一个包，还必须确认它们属于同一个运行时包－它们必须有同一个类装载器装载的。 这样，java.lang.Virus和来自核心的java.lang的类不属于同一个运行时包，java.lang.Virus就不能访问JAVA API的java.lang包中的包内可见的成员。\nClass.getClassLoader vs Thread.getContextClassLoader   每个 Class 会使用自己的 ClassLoader 去加载其他的 Class 。如果 ClassA.class 引用了 ClassB.class ，那么 ClassB 需要能被 ClassA 的 ClassLoader 或者其 父ClassLoader 找到。\n  Thread.getContextClassLoader 是当前线程使用的 ClassLoader。对象可以通过 ClassLoaderC 加载，并且传递到 Classload 是 ClassLoaderD 的线程里。在某些情况下，对象需要使用 Thread.currentThread().getContextClassLoader() 来加载 ClassLoaderC 不能获取的资源\n  Tomcat \u0026amp; ClassLoader 事实上，tomcat之所以造了一堆自己的classloader，大致是出于下面三类目的：\n - 对于各个 webapp 中的 class 和 lib ，需要相互隔离，不能出现一个应用中加载的类库会影响另一个应用的情况；而对于许多应用，需要有共享的lib以便不浪费资源，举个例子，如果 webapp1 和 webapp2 都用到了 log4j ，可以将 log4j 提到 tomcat/lib 中，表示所有应用共享此类库，试想如果 log4j 很大，并且 20 个应用都分别加载，那实在是没有必要的。 - 与 jvm 一样的安全性问题。使用单独的 classloader 去装载 tomcat 自身的类库，以免其他恶意或无意的破坏； - 热部署，相信大家一定为 tomcat 修改文件不用重启就自动重新装载类库而惊叹吧。\n"});index.add({'id':38,'href':'/interview/docs/architecture/distributed/kafka/','title':"Kafka",'content':"Kafka 术语  Broker：Kafka 集群包含一个或多个服务器，这种服务器被称为 broker 。 Topic：每条发布到 Kafka 集群的消息都有一个类别，这个类别被称为 Topic。（物理上不同 Topic 的消息分开存储，逻辑上一个 Topic 的消息虽然保存于一个或多个 broker 上，但用户只需指定消息的 Topic 即可生产或消费数据而不必关心数据存于何处）。 Partition： Partition 是物理上的概念，每个 Topic 包含一个或多个 Partition 。 Producer：负责发布消息到 Kafka broker。 Consumer：消息消费者，向 Kafka broker 读取消息的客户端。 Consumer Group：每个 Consumer 属于一个特定的 Consumer Group（可为每个 Consumer 指定 group name，若不指定 group name 则属于默认的 group）。  拓扑结构 如上图所示，一个典型的 Kafka 集群中包含若干 Producer （可以是web前端产生的Page View，或者是服务器日志，系统CPU、Memory等），若干 broker （Kafka支持水平扩展，一般broker数量越多，集群吞吐率越高），若干 Consumer Group ，以及一个 Zookeeper 集群。 Kafka 通过 Zookeeper 管理集群配置，选举 leader ，以及在 Consumer Group 发生变化时进行 rebalance。 Producer 使用 push 模式将消息发布到broker，Consumer使用pull模式从broker订阅并消费消息。\nTopic \u0026amp; Partition Topic 在逻辑上可以被认为是一个 queue ，每条消费都必须指定它的 Topic ，可以简单理解为必须指明把这条消息放进哪个 queue 里。为了使得 Kafka 的吞吐率可以线性提高，物理上把 Topic 分成一个或多个 Partition ，每个 Partition 在物理上对应一个文件夹，该文件夹下存储这个 Partition 的所有消息和索引文件。若创建 topic1 和 topic2 两个 topic ，且分别有 13 个和 19 个分区，则整个集群上会相应会生成共 32 个文件夹（本文所用集群共8个节点，此处 topic1 和 topic2 replication-factor 均为1）。\n Partition 都是通过 顺序读写，所以效率很高\n  replication-factor 配置 partition 副本数。配置副本之后,每个 partition 都有一个唯一的 leader ，有 0 个或多个 follower 。所有的读写操作都在 leader 上完成，followers 从 leader 消费消息来复制 message，就跟普通的 consumer 消费消息一样。一般情况下 partition 的数量大于等于 broker 的数量，并且所有 partition 的 leader 均匀分布在 broker 上。\n 对于传统的 MQ 而言，一般会删除已经被消费的消息，而 Kafka 集群会保留所有的消息，无论其被消费与否。当然，因为磁盘限制，不可能永久保留所有数据（实际上也没必要），因此 Kafka 提供两种策略删除旧数据。一是基于时间，二是基于 Partition 文件大小。\nProducer 消息路由 Producer 发送消息到 broker 时，会根据 Paritition 机制选择将其存储到哪一个 Partition 。如果 Partition 机制设置合理，所有消息可以均匀分布到不同的 Partition 里，这样就实现了负载均衡。如果一个 Topic 对应一个文件，那这个文件所在的机器I/O将会成为这个 Topic 的性能瓶颈，而有了 Partition 后，不同的消息可以并行写入不同 broker 的不同 Partition 里，极大的提高了吞吐率。\n可以在 $KAFKA_HOME/config/server.properties 中通过配置项 num.partitions 来指定新建 Topic 的默认 Partition 数量，也可在创建 Topic 时通过参数指定，同时也可以在 Topic 创建之后通过 Kafka 提供的工具修改。\n 指定了 patition，则直接使用 未指定 patition 但指定 key，通过对 key 进行 hash 选出一个 patition patition 和 key 都未指定，使用轮询选出一个 patition  Consumer Group 这是 Kafka 用来实现一个 Topic 消息的广播（发给所有的 Consumer ）和单播（发给某一个 Consumer ）的手段。一个 Topic 可以对应多个 Consumer Group 。如果需要实现广播，只要每个 Consumer 有一个独立的 Group 就可以了。要实现单播只要所有的 Consumer 在同一个 Group 里。用 Consumer Group 还可以将 Consumer 进行自由的分组而不需要多次发送消息到不同的 Topic 。\nConsumer 个数与 Parition 数有什么关系？ topic 下的一个分区只能被同一个 consumer group 下的一个 consumer 线程来消费，但反之并不成立，即一个 consumer 线程可以消费多个分区的数据。比如 Kafka 提供的 ConsoleConsumer ，默认就只是一个线程来消费所有分区的数据。\n 即分区数决定了同组消费者个数的上限\n 所以，如果你的分区数是 N ，那么最好线程数也保持为 N ，这样通常能够达到最大的吞吐量。超过 N 的配置只是浪费系统资源，因为多出的线程不会被分配到任何分区。\n 如果消费线程大于 patition 数量，则有些线程将收不到消息 如果 patition 数量大于消费线程数，则有些线程多收到多个 patition 的消息 如果一个线程消费多个 patition，则无法保证你收到的消息的顺序，而一个 patition 内的消息是有序的  Push vs. Pull　 作为一个消息系统，Kafka 遵循了传统的方式，选择由 Producer 向 broker push 消息并由 Consumer 从 broker pull 消息。事实上，push 模式和 pull 模式各有优劣。\n Push模式 很难适应消费速率不同的消费者，因为消息发送速率是由broker决定的。push模式的目标是尽可能以最快速度传递消息，但是这样很容易造成 Consumer 来不及处理消息，典型的表现就是拒绝服务以及网络拥塞。 Pull模式 可以根据Consumer的消费能力以适当的速率消费消息。  对于 Kafka 而言，Pull模式 更合适。Pull模式 可简化 broker 的设计，Consumer 可自主控制消费消息的速率，同时 Consumer 可以自己控制消费方式——即可批量消费也可逐条消费，同时还能选择不同的提交方式从而实现不同的传输语义。\n高可用性 Kafka 0.8 以前，是没有 HA 机制的，就是任何一个 broker 宕机了，那个 broker 上的 partition 就废了，没法写也没法读，没有什么高可用性可言。\n比如说，我们假设创建了一个 topic ，指定其 partition 数量是 3 个，分别在三台机器上。但是，如果第二台机器宕机了，会导致这个 topic 的 1/3 的数据就丢了，因此这个是做不到高可用的。\nKafka 0.8 以后，提供了 HA 机制，就是 replica 副本机制。每个 partition 的数据都会同步到其它机器上，形成自己的多个 replica 副本。所有 replica 会选举一个 leader 出来，那么生产和消费都跟这个 leader 打交道，然后其他 replica 就是 follower 。写的时候， leader 会负责把数据同步到所有 follower 上去，读的时候就直接读 leader 上的数据即可。 Kafka 会均匀地将一个 partition 的所有 replica 分布在不同的机器上，这样才可以提高容错性。\n这么搞，就有所谓的高可用性了，因为如果某个 broker 宕机了，没事儿，那个 broker 上面的 partition 在其他机器上都有副本的，如果这上面有某个 partition 的 leader ，那么此时会从 follower 中 重新选举 一个新的 leader 出来，大家继续读写那个新的 leader 即可。这就有所谓的高可用性了。\n 写数据 的时候，生产者就写 leader ，然后 leader 将数据落地写本地磁盘，接着其他 follower 自己主动从 leader 来 pull 数据。一旦所有 follower 同步好数据了，就会发送 ack 给 leader ， leader 收到所有 follower 的 ack 之后，就会返回写成功的消息给生产者。（当然，这只是其中一种模式，还可以适当调整这个行为） 消费 的时候，只会从 leader 去读，但是 只有当一个消息已经被所有 follower 都同步成功返回 ack 的时候，这个消息才会被消费者读到。  消息幂等性 Kafka 实际上有个 offset 的概念，就是每个消息写进去，都有一个 offset ，代表消息的序号，然后 consumer 消费了数据之后，每隔一段时间（定时定期），会把自己消费过的消息的 offset 提交一下，表示“我已经消费过了，下次我要是重启啥的，你就让我继续从上次消费到的 offset 来继续消费吧”。\n但是凡事总有意外，比如我们之前生产经常遇到的，就是你有时候重启系统，看你怎么重启了，如果碰到点着急的，直接 kill 进程了，再重启。这会导致 consumer 有些消息处理了，但是没来得及提交 offset ，尴尬了。重启之后，少数消息会再次消费一次。\n幂等性，通俗点说，一个数据，或者一个请求，给你重复来多次，你得确保对应的数据是不会改变的，不能出错。其实重复消费不可怕，可怕的是你没考虑到重复消费之后，怎么保证幂等性？其实还是得 结合业务来思考，这里给几个思路：\n 比如你拿个数据要写库，你先根据主键查一下，如果这数据都有了，你就别插入了，update 一下好吧。 比如你是写 Redis ，那没问题了，反正每次都是 set，天然幂等性。 比如你不是上面两个场景，那做的稍微复杂一点，你需要让生产者发送每条数据的时候，里面加一个全局唯一的 id，类似订单 id 之类的东西，然后你这里消费到了之后，先根据这个 id 去比如 Redis 里查一下，之前消费过吗？如果没有消费过，你就处理，然后这个 id 写 Redis。如果消费过了，那你就别处理了，保证别重复处理相同的消息即可。 比如基于数据库的唯一键来保证重复数据不会重复插入多条。因为有唯一键约束了，重复数据插入只会报错，不会导致数据库中出现脏数据。  消息丢失 消费端弄丢了数据 唯一可能导致消费者弄丢数据的情况，就是说，你消费到了这个消息，然后消费者那边 自动提交了 offset ，让 Kafka 以为你已经消费好了这个消息，但其实你才刚准备处理这个消息，你还没处理，你自己就挂了，此时这条消息就丢咯。\nKafka 会自动提交 offset ，那么只要关闭自动提交 offset ，在处理完之后自己手动提交 offset ，就可以保证数据不会丢。但是此时确实还是可能会有重复消费，比如你刚处理完，还没提交 offset ，结果自己挂了，此时肯定会重复消费一次，自己 保证幂等性 就好了。\nKafka 弄丢了数据 这块比较常见的一个场景，就是 Kafka 某个 broker 宕机，然后重新选举 partition 的 leader 。大家想想，要是此时其他的 follower 刚好还有些数据没有同步，结果此时 leader 挂了，然后选举某个 follower 成 leader 之后，不就少了一些数据？这就丢了一些数据啊。\n此时一般是要求起码设置如下 4 个参数：\n 给 topic 设置 replication.factor 参数：这个值必须大于 1，要求每个 partition 必须有 至少 2 个副本。 在 Kafka 服务端设置 min.insync.replicas 参数：这个值必须大于 1，这个是 要求一个 leader 至少感知到有至少一个 follower 还跟自己保持联系，没掉队，这样才能确保 leader 挂了还有一个 follower 吧。 在 producer 端设置 acks=all：这个是要求每条数据，必须是写入所有 replica 之后，才能认为是写成功了。 在 producer 端设置 retries=MAX（很大很大很大的一个值，无限次重试的意思）：这个是要求 一旦写入失败，就无限重试，卡在这里了。  这样配置之后，至少在 Kafka broker 端就可以保证在 leader 所在 broker 发生故障，进行 leader 切换时，数据不会丢失。\n生产者会不会弄丢数据？ 如果按照上述的思路设置了 acks=all，一定不会丢，要求是，你的 leader 接收到消息，所有的 follower 都同步到了消息之后，才认为本次写成功了。如果没满足这个条件，生产者可以自动不断的重试，重试无限次。\n消息的顺序性 比如说我们建了一个 topic ，有三个 partition 。生产者在写的时候，其实可以指定一个 key ，比如说我们指定了某个订单 id 作为 key ，那么这个订单相关的数据，一定会被分发到同一个 partition 中去，而且这个 partition 中的数据一定是有顺序的。\n消费者从 partition 中取出来数据的时候，也一定是有顺序的。到这里，顺序还是 ok 的，没有错乱。接着，我们在消费者里可能会搞 多个线程来并发处理消息。而多个线程并发跑的话，顺序可能就乱掉了。\n解决方案：\n 一个 topic ，一个 partition ，一个 consumer ，内部单线程消费，单线程吞吐量太低，一般不会用这个。 写 N 个内存 queue ，具有相同 key 的数据都到同一个内存 queue ；然后对于 N 个线程，每个线程分别消费一个内存 queue 即可，这样就能保证顺序性。  Kafka 如何进行扩容的？ 假如集群有 3 个 broker，一共有 4 个 TP，每个 3 副本，均匀分布。现在要扩容一台机器，新 broker 加入集群后需要通过工具进行 TP 的迁移。一共迁移 3 个 TP 的副本到新 broker 上。等迁移结束之后，会重新进行 Leader balance。\n从微观的角度看，TP 从一台 broker 迁移到另一个 broker 的流程是怎么样的呢？咱们来看下 TP3 第三个副本，从 broker1 迁移到 broker4 的过程，如下图所示，broker4 作为 TP3 的 follower，从 broker1 上最早的 offset 进行获取数据，直到赶上最新的 offset 为止，新副本被放入 ISR 中，并移除 broker1 上的副本，迁移过程完毕。\n但在现有的扩容流程中存有如下问题：数据迁移从 TP3 的最初的 offset 开始拷贝数据，这会导致大量读磁盘，消耗大量的 I/O 资源，导致磁盘繁忙，从而造成 produce 操作延迟增长，产生抖动。所以整体迁移流程不够平滑。我们看下实际的监控到的数据。从中可以看到数据迁移中， broker1 上磁盘读量增大，磁盘 util 持续打满，produce 极其不稳定。\n针对这个问题，我们回到 Kafka 迁移的流程上看，理论上 Kafka 是一个缓存系统，不需要永久存储数据，很有可能费了很多工作迁移过来的数据根本就不会被使用，甚至马上就会被删除了。从这个角度上看，那么迁移数据时，为什么一定要从 partition 最初 offset 开始迁移数据呢？细想想，好像不需要这样。\n所以，解决这个问题的思路就比较简单了，在迁移 TP 时，直接从 partition 最新的 offset 开始数据迁移，但是要同步保持一段时间，主要是确保所有 consumer 都已经跟得上了。\nLeader 选举过程 控制器的选举 在Kafka集群中会有一个或多个broker，其中有一个broker会被选举为控制器（Kafka Controller），它负责管理整个集群中所有分区和副本的状态等工作。比如当某个分区的 Leader 副本出现故障时，由控制器负责为该分区选举新的 Leader 副本。再比如当检测到某个分区的 ISR(In-Sync Replicas) 集合发生变化时，由控制器负责通知所有 broker 更新其元数据信息。\nKafka Controller 的选举是依赖 Zookeeper 来实现的，在 Kafka 集群中哪个 broker 能够成功创建 /controller 这个临时（EPHEMERAL）节点他就可以成为 Kafka Controller。Kafka Controller 的出现是处于性能考虑，当 Kafka 集群规模很大，partition 达到成千上万时，当 broker 宕机时，造成集群内大量的调整，会造成大量 Watch 事件被触发，Zookeeper负载会过重。\n分区 Leader 的选举 分区 Leader 副本的选举由 Kafka Controller 负责具体实施。当创建分区（创建主题或增加分区都有创建分区的动作）或分区上线（比如分区中原先的 Leader 副本下线，此时分区需要选举一个新的 Leader 上线来对外提供服务）的时候都需要执行 Leader 的选举动作。\n消费者相关的选举 组协调器 GroupCoordinator 需要为消费组内的消费者选举出一个消费组的 Leader，这个选举的算法也很简单，分两种情况分析。如果消费组内还没有 Leader，那么第一个加入消费组的消费者即为消费组的 Leader。如果某一时刻 Leader 消费者由于某些原因退出了消费组，那么会重新选举一个新的 Leader。\n负载均衡 Producers 负载均衡 对于同一个 topic 的不同 partition，Kafka 会尽力将这些 partition 分布到不同的 broker 服务器上，这种均衡策略实际上是基于 ZooKeeper 实现的。在一个 broker 启动时，会首先完成 broker 的注册过程，并注册一些诸如 “有哪些可订阅的 topic” 之类的元数据信息。producers 启动后也要到 ZooKeeper 下注册，创建一个临时节点来监听 broker 服务器列表的变化。由于在 ZooKeeper 下 broker 创建的也是临时节点，当 brokers 发生变化时，producers 可以得到相关的通知，从改变自己的 broker list。其它的诸如 topic 的变化以及 broker 和 topic 的关系变化，也是通过 ZooKeeper 的这种 Watcher 监听实现的。\n在生产中，必须指定 topic；但是对于 partition，有两种指定方式：\n 明确指定 partition(0-N)，则数据被发送到指定 partition； 设置为 RD_KAFKA_PARTITION_UA ，则 Kafka 会回调 partitioner 进行均衡选取， partitioner 方法需要自己实现。可以轮询或者传入 key 进行 hash。未实现则采用默认的随机方法 rd_kafka_msg_partitioner_random 随机选择。  Consumer 负载均衡 Kafka 保证同一 consumer group 中只有一个 consumer 可消费某条消息，实际上，Kafka 保证的是稳定状态下每一个 consumer 实例只会消费某一个或多个特定的数据，而某个 partition 的数据只会被某一个特定的 consumer 实例所消费。这样设计的劣势是无法让同一个 consumer group 里的 consumer 均匀消费数据，优势是每个 consumer 不用都跟大量的 broker 通信，减少通信开销，同时也降低了分配难度，实现也更简单。另外，因为同一个 partition 里的数据是有序的，这种设计可以保证每个 partition 里的数据也是有序被消费。\nconsumer 数量不等于 partition 数量 如果某 consumer group 中 consumer 数量少于 partition 数量，则至少有一个 consumer 会消费多个 partition 的数据；如果 consumer 的数量与 partition 数量相同，则正好一个 consumer 消费一个 partition 的数据，而如果 consumer 的数量多于 partition 的数量时，会有部分 consumer 无法消费该 topic 下任何一条消息。\n借助 ZooKeeper 实现负载均衡 关于负载均衡，对于某些低级别的 API，consumer 消费时必须指定 topic 和 partition，这显然不是一种友好的均衡策略。基于高级别的 API，consumer 消费时只需制定 topic，借助 ZooKeeper 可以根据 partition 的数量和 consumer 的数量做到均衡的动态配置。\nconsumers 在启动时会到 ZooKeeper 下以自己的 conusmer-id 创建临时节点 /consumer/[group-id]/ids/[conusmer-id]，并对 /consumer/[group-id]/ids 注册监听事件，当消费者发生变化时，同一 group 的其余消费者会得到通知。当然，消费者还要监听 broker 列表的变化。kafka 通常会将 partition 进行排序后，根据消费者列表，进行轮流的分配。\n参考  Kafka设计解析 Kafka的高可用 Kafka幂等性 Kafka消息丢失 快手万亿级别 Kafka 集群应用实践与技术演进之路 kafka的leader选举过程  "});index.add({'id':39,'href':'/interview/docs/basic/algo/kmp/','title':"KMP",'content':"KMP算法 KMP算法解决的问题是字符匹配，这个算法把字符匹配的时间复杂度缩小到O(m+n),而空间复杂度也只有O(m),n是target的长度，m是pattern的长度。\n  部分匹配表（Next数组）：表的作用是 让算法无需多次匹配S中的任何字符。能够实现线性时间搜索的关键是 在不错过任何潜在匹配的情况下，我们\u0026quot;预搜索\u0026quot;这个模式串本身并将其译成一个包含所有可能失配的位置对应可以绕过最多无效字符的列表。\n  Next数组（前缀和前缀的比较）：t为模式串，j为下标\n Next[0] = -1 Next[j] = MAX{ k | 0 \u0026lt; k \u0026lt; j | \u0026quot; t0 t1 ... tk \u0026quot; = \u0026quot;t ( j-k ) t ( j-k+1 ) ... t( j-1 )\u0026quot; }    |i|\t0|\t1|\t2|\t3|\t4|\t5\t|6| |\u0026ndash;| | t[i]|\tA|\tB|\tC|\tD|\tA|\tB|\tD| |next[i]|\t-1|\t0\t|0\t|0\t|0\t|1\t|2|\n NextVal数组：是一种优化后的Next数组，是为了解决类似aaaab这种模式串的匹配，减少重复的比较。 如果t[next[j]]=t[j]：nextval[j]=nextval[next[j]]，否则nextval[j]=next[j]。  |i|\t0|\t1|\t2|\t3|\t4|\t5\t|6| |\u0026ndash;| | t |\ta|\tb| c|\ta| b| a |a| |next[j] |\t-1|\t0\t|0\t|0\t|1\t|2\t|1| |nextval[j] |\t-1|\t0\t|0\t|-1\t|0\t|2\t|1|\n在上面的表格中，t[next[4]]=t[4]=b，所以nextval[4]=nextval[next[4]]=0\n"});index.add({'id':40,'href':'/interview/docs/basic/os/linux/','title':"Linux",'content':"Linux系统 sed sed是非交互式的编辑器。它不会修改文件，除非使用shell重定向来保存结果。默认情况下，所有的输出行都被打印到屏幕上。sed编辑器逐行处理文件（或输入），并将结果发送到屏幕。\nsed命令行格式为： sed [-nefri] ‘command’ 输入文本 常用选项： -n∶使用安静(silent)模式。在一般 sed 的用法中，所有来自 STDIN的资料一般都会被列出到萤幕上。但如果加上 -n 参数后，则只有经过sed 特殊处理的那一行(或者动作)才会被列出来。 -e∶直接在指令列模式上进行 sed 的动作编辑； -f∶直接将 sed 的动作写在一个档案内， -f filename 则可以执行 filename 内的sed 动作； -r∶sed 的动作支援的是延伸型正规表示法的语法。(预设是基础正规表示法语法) -i∶直接修改读取的档案内容，而不是由萤幕输出。 常用命令： a ∶新增， a 的后面可以接字串，而这些字串会在新的一行出现(目前的下一行)～ c ∶取代， c 的后面可以接字串，这些字串可以取代 n1,n2 之间的行！ d ∶删除，因为是删除啊，所以 d 后面通常不接任何咚咚； i ∶插入， i 的后面可以接字串，而这些字串会在新的一行出现(目前的上一行)； p ∶列印，亦即将某个选择的资料印出。通常 p 会与参数 sed -n 一起运作～ s ∶取代，可以直接进行取代的工作哩！通常这个 s 的动作可以搭配正规表示法！例如 1,20s/old/new/g！ g 是行内进行全局替换 ##　umask\n当我们登录系统之后创建一个文件总是有一个默认权限的，那么这个权限是怎么来的呢？这就是umask干的事情。umask设置了用户创建文件的默认权限，它与chmod的效果刚好相反，umask设置的是权限“补码”，而chmod设置的是文件权限码。\n计算方法如下：\n 例如，对于umask值0 0 2，相应的文件和目录缺省创建权限是什么呢？ // 664 775 第一步，我们首先写下目录具有全部权限的模式，即777 (所有用户都具有读、写和执行权限)，文件默认是666。 第二步，在下面一行按照umask值写下相应的位，在本例中是0 0 2。 第三步，在接下来的一行中记下上面两行中没有匹配的位。这就是目录的缺省创建权限。 稍加练习就能够记住这种方法。 第四步，对于文件来说，在创建时不能具有执行权限，只要拿掉相应的执行权限比特即可。 useradd 格式：useradd [选项] 用户名\n-p 设定帐号的密码 -d 指定用户的主目录 -m 自动建立用户的主目录 -M 不要自动建立用户的主目录 mount \u0026amp;\u0026amp; umount  mount [选项] \u0026lt;-t 类型\u0026gt; [-o 挂载选项] \u0026lt;设备\u0026gt; \u0026lt;挂载点\u0026gt; umount \u0026lt;挂载点|设备\u0026gt; ##find\n find [-H] [-L] [-P] [-Olevel] [-D help|tree|search|stat|rates|opt|exec] [path...] [expression] ##grep\n在文件中搜索指定字符所在行\n格式： grep [选项] 指定字符 文件\n-i 忽略大小写 -r 递归 -v 排除指定字符串 -n 显示列数\n eg: grep -i ab /etc/inittab ##tar\n常用的打包压缩和解压命令之一格式: tar 选项 [压缩后文件名] [目录]\n注意：打包和压缩是两个不同概念，打包只是把所有文件放在一具类似包中，并不改变其大小，而压缩才会改变其大小\n 压缩时常用 -c 打包(create) -v显示详细信息(view) -f指定文件名(filename) -z 打包同时压缩 eg: tar -zvf word.tar word 解压缩时常用 -x 解包 -v显示详细信息(view) -f指定解压文件名(filename) -z 解压缩 eg: tar -zxf word.tar "});index.add({'id':41,'href':'/interview/docs/architecture/distributed/mq/','title':"MQ",'content':"MQ 消息队列技术(Message Queue) 是分布式应用间交换信息的一种技术。消息队列可驻留在内存或磁盘上, 队列存储消息直到它们被应用程序读走。通过消息队列，应用程序可独立地执行 ———— 它们不需要知道彼此的位置、或在继续执行前不需要等待接收程序接收此消息。在分布式计算环境中，为了集成分布式应用，开发者需要对异构网络环境下的分布式应用提供有效的通信手段。\nMQ使用场景   异步通信：有些业务不想也不需要立即处理消息。消息队列提供了异步处理机制，允许用户把一个消息放入队列，但并不立即处理它。想向队列中放入多少消息就放多少，然后在需要的时候再去处理它们。\n  解耦：降低工程间的强依赖程度，针对异构系统进行适配。在项目启动之初来预测将来项目会碰到什么需求，是极其困难的。通过消息系统在处理过程中间插入了一个隐含的、基于数据的接口层，两边的处理过程都要实现这一接口，当应用发生变化时，可以独立的扩展或修改两边的处理过程，只要确保它们遵守同样的接口约束\n  冗余：有些情况下，处理数据的过程会失败。除非数据被持久化，否则将造成丢失。消息队列把数据进行持久化直到它们已经被完全处理，通过这一方式规避了数据丢失风险。许多消息队列所采用的\u0026quot;插入-获取-删除\u0026quot;范式中，在把一个消息从队列中删除之前，需要你的处理系统明确的指出该消息已经被处理完毕，从而确保你的数据被安全的保存直到你使用完毕。\n  扩展性：因为消息队列解耦了你的处理过程，所以增大消息入队和处理的频率是很容易的，只要另外增加处理过程即可。不需要改变代码、不需要调节参数。便于分布式扩容\n  过载保护：在访问量剧增的情况下，应用仍然需要继续发挥作用，但是这样的突发流量无法提取预知；如果以为了能处理这类瞬间峰值访问为标准来投入资源随时待命无疑是巨大的浪费。使用消息队列能够使关键组件顶住突发的访问压力，而不会因为突发的超负荷的请求而完全崩溃\n  可恢复性：系统的一部分组件失效时，不会影响到整个系统。消息队列降低了进程间的耦合度，所以即使一个处理消息的进程挂掉，加入队列中的消息仍然可以在系统恢复后被处理。   顺序保证：在大多使用场景下，数据处理的顺序都很重要。大部分消息队列本来就是排序的，并且能保证数据会按照特定的顺序来处理。   缓冲：在任何重要的系统中，都会有需要不同的处理时间的元素。消息队列通过一个缓冲层来帮助任务最高效率的执行，该缓冲有助于控制和优化数据流经过系统的速度。以调节系统响应时间。\n  数据流处理：分布式系统产生的海量数据流，如：业务日志、监控数据、用户行为等，针对这些数据流进行实时或批量采集汇总，然后进行大数据分析是当前互联网的必备技术，通过消息队列完成此类数据收集是最好的选择\n  MQ缺点   系统可用性降低：系统引入的外部依赖越多，越容易挂掉。本来你就是 A 系统调用 BCD 三个系统的接口就好了， ABCD 四个系统好好的，没啥问题，你偏加个 MQ 进来，万一 MQ 挂了咋整，MQ 一挂，整套系统崩溃的，你不就完了？如何保证消息队列的高可用。\n  系统复杂度提高：硬生生加个 MQ 进来，你怎么保证消息没有重复消费？怎么处理消息丢失的情况？怎么保证消息传递的顺序性？头大头大，问题一大堆，痛苦不已。\n  一致性问题： A 系统处理完了直接返回成功了，人都以为你这个请求就成功了；但是问题是，要是 BCD 三个系统那里， BD 两个系统写库成功了，结果 C 系统写库失败了，咋整？你这数据就不一致了。\n  MQ常用协议   AMQP协议 AMQP即Advanced Message Queuing Protocol,一个提供统一消息服务的应用层标准高级消息队列协议,是应用层协议的一个开放标准,为面向消息的中间件设计。基于此协议的客户端与消息中间件可传递消息，并不受客户端/中间件不同产品，不同开发语言等条件的限制。\n 优点：可靠、通用\n   MQTT协议 MQTT（Message Queuing Telemetry Transport，消息队列遥测传输）是IBM开发的一个即时通讯协议，有可能成为物联网的重要组成部分。该协议支持所有平台，几乎可以把所有联网物品和外部连接起来，被用来当做传感器和致动器（比如通过Twitter让房屋联网）的通信协议。  优点：格式简洁、占用带宽小、移动端通信、PUSH、嵌入式系统\n   STOMP协议 STOMP（Streaming Text Orientated Message Protocol）是流文本定向消息协议，是一种为MOM(Message Oriented Middleware，面向消息的中间件)设计的简单文本协议。STOMP提供一个可互操作的连接格式，允许客户端与任意STOMP消息代理（Broker）进行交互。  优点：命令模式（非topic/queue模式）\n   XMPP协议 XMPP（可扩展消息处理现场协议，Extensible Messaging and Presence Protocol）是基于可扩展标记语言（XML）的协议，多用于即时消息（IM）以及在线现场探测。适用于服务器之间的准即时操作。核心是基于XML流传输，这个协议可能最终允许因特网用户向因特网上的其他任何人发送即时消息，即使其操作系统和浏览器不同。\n 优点：通用公开、兼容性强、可扩展、安全性高，但XML编码格式占用带宽大\n   其他基于TCP/IP自定义的协议：有些特殊框架（如：redis、kafka、zeroMq等）根据自身需要未严格遵循MQ规范，而是基于TCP\\IP自行封装了一套协议，通过网络socket接口进行传输，实现了MQ的功能。\n  MQ的通讯模式  1. 点对点通讯：点对点方式是最为传统和常见的通讯方式，它支持一对一、一对多、多对多、多对一等多种配置方式，支持树状、网状等多种拓扑结构。\n 2. 多点广播：MQ适用于不同类型的应用。其中重要的，也是正在发展中的是\u0026quot;多点广播\u0026quot;应用，即能够将消息发送到多个目标站点(Destination List)。可以使用一条MQ指令将单一消息发送到多个目标站点，并确保为每一站点可靠地提供信息。MQ不仅提供了多点广播的功能，而且还拥有智能消息分发功能，在将一条消息发送到同一系统上的多个用户时，MQ将消息的一个复制版本和该系统上接收者的名单发送到目标MQ系统。目标MQ系统在本地复制这些消息，并将它们发送到名单上的队列，从而尽可能减少网络的传输量。\n 3. 发布/订阅(Publish/Subscribe)模式：发布/订阅功能使消息的分发可以突破目的队列地理指向的限制，使消息按照特定的主题甚至内容进行分发，用户或应用程序可以根据主题或内容接收到所需要的消息。发布/订阅功能使得发送者和接收者之间的耦合关系变得更为松散，发送者不必关心接收者的目的地址，而接收者也不必关心消息的发送地址，而只是根据消息的主题进行消息的收发。在MQ家族产品中，MQ Event Broker是专门用于使用发布/订阅技术进行数据通讯的产品，它支持基于队列和直接基于TCP/IP两种方式的发布和订阅。\n 4. 集群(Cluster)：为了简化点对点通讯模式中的系统配置，MQ提供 Cluster 的解决方案。集群类似于一个 域(Domain) ，集群内部的队列管理器之间通讯时，不需要两两之间建立消息通道，而是采用 Cluster 通道与其它成员通讯，从而大大简化了系统配置。此外，集群中的队列管理器之间能够自动进行负载均衡，当某一队列管理器出现故障时，其它队列管理器可以接管它的工作，从而大大提高系统的高可靠性\n消息投递保证  At most once：消息可能会丢，但绝不会重复投递 At least one：消息绝不会丢，但可能会重复投递 Exactly once：每条消息肯定会被投递一次且仅投递一次，很多时候这是用户所想要的。  参考链接 消息队列面试场景\n"});index.add({'id':42,'href':'/interview/docs/fromwork/mybatis/proxy/','title':"Mybatis 动态代理",'content':"Mybatis 动态代理 获取代理类流程 获取Mapper代理类的时序图如下：\n重点说下MapperProxy类，声明如下：\npublic class MapperProxy\u0026lt;T\u0026gt; implements InvocationHandler, Serializable 获取到 MapperProxy 之后，根据调用不同的方法，会将最终的参数传递给 SqlSession。\n"});index.add({'id':43,'href':'/interview/docs/fromwork/mybatis/cache/','title':"Mybatis 缓存机制",'content':"Mybatis 缓存机制 Mybatis 的缓存均缓存查询操作结果。按照作用域范围，可以分为：\n- **一级缓存**： `SqlSession` 级别的缓存 - **二级缓存**： `namespace` 级别的缓存  一级缓存 Mybatis 默认开启了一级缓存， 一级缓存有两个级别可以设置：分别是 SESSION 或者 STATEMENT 默认是 SESSION 级别，即在一个 MyBatis会话中执行的所有语句，都会共享这一个缓存。一种是 STATEMENT 级别，可以理解为缓存只对当前执行的这一个 Statement 有效。\n STATEMENT 级别相当于关闭一级缓存\n \u0026lt;setting name=\u0026quot;localCacheScope\u0026quot; value=\u0026quot;SESSION\u0026quot;/\u0026gt; 基本原理 在一级缓存中，当 sqlSession 执行写操作（执行插入、更新、删除），清空 SqlSession 中的一级缓存。\n总结  MyBatis 一级缓存的生命周期和SqlSession一致。 MyBatis 一级缓存内部设计简单，只是一个没有容量限定的HashMap，在缓存的功能性上有所欠缺。 MyBatis 的一级缓存最大范围是SqlSession内部，有多个SqlSession或者分布式的环境下，数据库写操作会引起脏数据，建议设定缓存级别为Statement。  二级缓存 如果多个 SqlSession 之间需要共享缓存，则需要使用到二级缓存。开启二级缓存后，会使用 CachingExecutor 装饰 Executor ，进入一级缓存的查询流程前，先在C achingExecutor 进行二级缓存的查询，具体的工作流程如下所示。\n二级缓存开启后，同一个namespace下的所有操作语句，都影响着同一个Cache，即二级缓存被多个SqlSession共享，是一个全局的变量。当开启缓存后，数据的查询执行的流程就是 二级缓存 -\u0026gt; 一级缓存 -\u0026gt; 数据库。\n\u0026lt;setting name=\u0026quot;cacheEnabled\u0026quot; value=\u0026quot;true\u0026quot;/\u0026gt; 总结  MyBatis 的二级缓存相对于一级缓存来说，实现了 SqlSession 之间缓存数据的共享，同时粒度更加的细，能够到 namespace 级别，通过 Cache 接口实现类不同的组合，对Cache的可控性也更强。 MyBatis 在多表查询时，极大可能会出现脏数据，有设计上的缺陷，安全使用二级缓存的条件比较苛刻。 在分布式环境下，由于默认的 MyBatis Cache 实现都是基于本地的，分布式环境下必然会出现读取到脏数据，需要使用集中式缓存将 MyBatis 的 Cache 接口实现，有一定的开发成本，直接使用 Redis、Memcached 等分布式缓存可能成本更低，安全性也更高。  "});index.add({'id':44,'href':'/interview/docs/basic/database/mysql/','title':"MySql",'content':"MySql 引擎 MVCC InnoDB 支持 MVCC 来提高系统读写并发性能。InnoDB MVCC 的实现基于 Undo log，通过回滚段来构建需要的版本记录。通过 ReadView 来判断哪些版本的数据可见。同时 Purge 线程是通过 ReadView 来清理旧版本数据。\nMVCC最大的优势：读不加锁，读写不冲突。在读多写少的OLTP应用中，读写不冲突是非常重要的，极大的增加了系统的并发性能\nMYSQL 事务日志 事务日志可以帮助提高事务的效率。使用事务日志，存储引擎在修改表的数据时只需要修改其内存拷贝，再把该修改行为记录到持久在硬盘上的事务日志中，而不用每次都将修改的数据本身持久到磁盘。\n事务日志采用的是追加的方式，因此写日志的操作是磁盘上一小块区域内的顺序I/O，而不像随机I/O需要在磁盘的多个地方移动磁头，所以采用事务日志的方式相对来说要快得多。事务日志持久以后，内存中被修改的数据在后台可以慢慢地刷回到磁盘。目前大多数存储引擎都是这样实现的，我们通常称之为预写式日志（Write-Ahead Logging），修改数据需要写两次磁盘。\n如果数据的修改已经记录到事务日志并持久化，但数据本身还没有写回磁盘，此时系统崩溃，存储引擎在重启时能够自动恢复这部分修改的数据。\nMySQL Innodb中跟数据持久性、原子性有关的日志，有以下几种：Redo Log、Undo Log。\n回滚日志 \u0026ndash; Undo Log 想要保证事务的 原子性，就需要在异常发生时，对已经执行的操作进行回滚，而在 MySQL 中，恢复机制是通过回滚日志（undo log）实现的，所有事务进行的修改都会先记录到这个回滚日志中，然后在对数据库中的对应行进行写入。\n这个过程其实非常好理解，为了能够在发生错误时撤销之前的全部操作，肯定是需要将之前的操作都记录下来的，这样在发生错误时才可以回滚。\n回滚日志除了能够在发生错误或者用户执行 ROLLBACK 时提供回滚相关的信息，它还能够在整个系统发生崩溃、数据库进程直接被杀死后，当用户再次启动数据库进程时，还能够立刻通过查询回滚日志将之前未完成的事务进行回滚，这也就需要回滚日志必须先于数据持久化到磁盘上，是我们需要先写日志后写数据库的主要原因。\n回滚日志并不能将数据库物理地恢复到执行语句或者事务之前的样子；它是逻辑日志，当回滚日志被使用时，它只会按照日志逻辑地将数据库中的修改撤销掉，可以理解为，我们在事务中使用的每一条 INSERT 都对应了一条 DELETE ，每一条 UPDATE 也都对应一条相反的 UPDATE 语句。\n重做日志 \u0026ndash; Redo Log 与原子性一样，事务的持久性也是通过日志来实现的，MySQL 使用重做日志（redo log）实现事务的持久性，重做日志由两部分组成，一是 内存 中的重做日志缓冲区，因为重做日志缓冲区在内存中，所以它是易失的，另一个就是在 磁盘 上的重做日志文件，它是持久的。\n当我们在一个事务中尝试对数据进行修改时，它会先将数据从磁盘读入内存，并更新内存中缓存的数据，然后生成一条重做日志并写入重做日志缓存，当事务真正提交时，MySQL 会将重做日志缓存中的内容刷新到重做日志文件，再将内存中的数据更新到磁盘上，图中的第 4、5 步就是在事务提交时执行的。\n在 InnoDB 中，重做日志都是以 512 字节的块的形式进行存储的，同时因为块的大小与磁盘扇区大小相同，所以重做日志的写入可以保证原子性，不会由于机器断电导致重做日志仅写入一半并留下脏数据。\n除了所有对数据库的修改会产生重做日志，因为回滚日志也是需要持久存储的，它们也会创建对应的重做日志，在发生错误后，数据库重启时会从重做日志中找出未被更新到数据库磁盘中的日志重新执行以满足事务的持久性。\n回滚日志和重做日志 在数据库系统中，事务的原子性和持久性是由事务日志（transaction log）保证的，在实现时也就是上面提到的两种日志，前者用于对事务的影响进行撤销，后者在错误处理时对已经提交的事务进行重做，它们能保证两点：\n 发生错误或者需要回滚的事务能够成功回滚（原子性）； 在事务提交后，数据没来得及写会磁盘就宕机时，在下次重新启动后能够成功恢复数据（持久性）；  在数据库中，这两种日志经常都是一起工作的，我们可以将它们整体看做一条事务日志，其中包含了事务的 ID、修改的行元素以及修改前后的值。\n一条事务日志同时包含了修改前后的值，能够非常简单的进行回滚和重做两种操作，在这里我们也不会对重做和回滚日志展开进行介绍，可能会在之后的文章谈一谈数据库系统的恢复机制时提到两种日志的使用。\nMySQL Server 日志 binlog 是 Mysql sever 层维护的一种二进制日志，与 innodb 引擎中的 redo/undo log 是完全不同的日志；其主要是用来记录对 mysql 数据更新或潜在发生更新的 SQL 语句，并以\u0026quot;事务\u0026quot;的形式保存在磁盘中；作用主要有：\n 复制：MySQL Replication 在 Master 端开启 binlog ，Master 把它的二进制日志传递给 slaves 并回放来达到 master-slave 数据一致的目的 数据恢复：通过 mysqlbinlog 工具恢复数据 增量备份  Buffer Pool 如果 MySQL 不使用内存缓冲池，每次读取数据时，都需要访问磁盘，会大大的增加磁盘的IO请求，导致效率低下；在 Innodb 引擎在读取数据的时候，把相应的数据和索引载入到内存的缓冲池（buffer pool）中，一定程度的提高了数据的读写速度。\n缓存包括：索引页，数据页，undo页，插入缓冲，自适应哈希索引，innodb存储的锁信息，数据字典等。工作方式是将数据库文件按照页（每页16k）读取到缓冲池，然后按照最近最少使用算法（LRU）来保留缓冲池中的缓冲数据。如果数据库文件需要修改，总是首先修改在缓冲池中的页（发生修改后即成为脏页），然后在按照一定的频率将缓冲池中的脏页刷新到文件\nMySQL 中的原则是日志先行。为了满足事务的持久性，防止 buffer pool 数据丢失以及事务持久性， InnoDB 引入了 redo log。为了满足事务的原子性，innodb 引入了 undo log。\nMVCC实现 MVCC是通过在每行记录后面保存两个隐藏的列来实现的。这两个列，一个保存了行的创建时间，一个保存行的过期时间（或删除时间）。当然存储的并不是实际的时间值，而是系统版本号（system version number)。每开始一个新的事务，系统版本号都会自动递增。事务开始时刻的系统版本号会作为事务的版本号，用来和查询到的每行记录的版本号进行比较。\n下面看一下在 REPEATABLE READ 隔离级别下，MVCC 具体是如何操作的：\n  SELECT：InnoDB 会根据以下两个条件检查每行记录：\n InnoDB 只查找版本早于当前事务版本的数据行（也就是，行的系统版本号小于或等于事务的系统版本号），这样可以确保事务读取的行，要么是在事务开始前已经存在的，要么是事务自身插入或者修改过的。 行的删除版本要么未定义，要么大于当前事务版本号。这可以确保事务读取到的行，在事务开始之前未被删除。只有符合上述两个条件的记录，才能返回作为查询结果    INSERT：InnoDB 为新插入的每一行保存当前系统版本号作为行版本号。\n  DELETE：InnoDB 为删除的每一行保存当前系统版本号作为行删除标识。\n  UPDATE：InnoDB 插入一行新记录，保存当前系统版本号作为行版本号，同时保存当前系统版本号到原来的行作为行删除标识。保存这两个额外系统版本号，使大多数读操作都可以不用加锁。这样设计使得读数据操作很简单，性能很好，并且也能保证只会读取到符合标准的行，不足之处是每行记录都需要额外的存储空间，需要做更多的行检查工作，以及一些额外的维护工作\n  主从同步 简单来说，就是保证主SQL（Master）和从SQL（Slave）的数据是一致性的，向 Master 插入数据后，Slave 会自动从 Master 把修改的数据同步过来（有一定的延迟），通过这种方式来保证数据的一致性，就是主从复制。\nMySQL主从能解决什么问题 高可用 因为数据都是相同的，所以当Master挂掉后，可以指定一台Slave充当Master继续保证服务运行，因为数据是一致性的（如果当插入Master就挂掉，可能不一致，因为同步也需要时间），当然这种配置不是简单的把一台Slave充当Master，毕竟还要考虑后续的Salve同步Master，当然本文并不是将高可用的配置，所以这里就不多讲了。\n负载均衡 因为读写分离也算是负载均衡的一种，所以就不单独写了，因为一般都是有多台Slave的，所以可以将读操作指定到Slave服务器上（需要代码控制），然后再用负载均衡来选择那台Slave来提供服务，同时也可以吧一些大量计算的查询指定到某台Slave，这样就不会影响Master的写入以及其他查询\n数据备份 一般我们都会做数据备份，可能是写定时任务，一些特殊行业可能还需要手动备份，有些行业要求备份和原数据不能在同一个地方，所以主从就能很好的解决这个问题，不仅备份及时，而且还可以多地备份，保证数据的安全\n业务模块化 可以一个业务模块读取一个Slave，再针对不同的业务场景进行数据库的索引创建和根据业务选择MySQL存储引擎\n高扩展（硬件扩展） 主从复制支持2种扩展方式：\n scale-up：向上扩展或者纵向扩展，主要是提供比现在服务器更好性能的服务器，比如增加CPU和内存以及磁盘阵列等，因为有多台服务器，所以可扩展性比单台更大 scale-out：向外扩展或者横向扩展，是指增加服务器数量的扩展，这样主要能分散各个服务器的压力  主从复制的缺点 成本增加 无可厚非的是搭建主从肯定会增加成本，毕竟一台服务器和两台服务器的成本完全不同，另外由于主从必须要开启二进制日志，所以也会造成额外的性能消耗\n数据延迟 Slave从Master复制过来肯定是会有一定的数据延迟的，所以当刚插入就出现查询的情况，可能查询不出来，当然如果是插入者自己查询，那么可以直接从Master中查询出来，当然这个也是需要用代码来控制的\n写入更慢 主从复制主要是针对读远大于写或者对数据备份实时性要求较高的系统中，因为 Master 在写中需要更多操作，而且只有一台写入的 Master，写入的压力并不能被分散\n复制方式 MySQL5.6开始主从复制有两种方式：基于日志（binlog）、基于GTID（全局事务标示符）。\n主从延时如何解决？ MySQL 实际上在有两个同步机制，一个是半同步复制，用来 解决主库数据丢失问题；一个是并行复制，用来 解决主从同步延时问题。\n 半同步复制，也叫 semi-sync 复制，指的就是主库写入 binlog 日志之后，就会将强制此时立即将数据同步到从库，从库将日志写入自己本地的 relay log 之后，接着会返回一个 ack 给主库，主库接收到至少一个从库的 ack 之后才会认为写操作完成了。 并行复制，指的是从库开启多个线程，并行读取 relay log 中不同库的日志，然后并行重放不同库的日志，这是库级别的并行。  以前线上确实处理过因为主从同步延时问题而导致的线上的 bug，属于小型的生产事故。有个同学是这样写代码逻辑的。先插入一条数据，再把它查出来，然后更新这条数据。在生产环境高峰期，写并发达到了 2000/s，这个时候，主从复制延时大概是在小几十毫秒。线上会发现，每天总有那么一些数据，我们期望更新一些重要的数据状态，但在高峰期时候却没更新。用户跟客服反馈，而客服就会反馈给我们。\n我们通过 MySQL 命令：\n show status 查看 Seconds_Behind_Master ，可以看到从库复制主库的数据落后了几 ms。一般来说，如果主从延迟较为严重，有以下解决方案：\n 分库，将一个主库拆分为多个主库，每个主库的写并发就减少了几倍，此时主从延迟可以忽略不计。 打开 MySQL 支持的并行复制，多个库并行复制。如果说某个库的写入并发就是特别高，单库写并发达到了 2000/s，并行复制还是没意义。 重写代码，写代码的同学，要慎重，插入数据时立马查询可能查不到。 如果确实是存在必须先插入，立马要求就查询到，然后立马就要反过来执行一些操作，对这个查询设置 直连主库。不推荐这种方法，你要是这么搞，读写分离的意义就丧失了。  复制原理  Master 将数据改变记录到二进制日志(binary log)中，也就是配置文件log-bin指定的文件，这些记录叫做二进制日志事件(binary log events) Slave 通过I/O线程读取 Master 中的binary log events并写入到它的中继日志(relay log) Slave 重做中继日志中的事件，把中继日志中的事件信息一条一条的在本地执行一次，完成数据在本地的存储，从而实现将改变反映到它自己的数据(数据重放)  要求  主从服务器操作系统版本和位数一致 Master和Slave数据库的版本要一致 Master和Slave数据库中的数据要一致 Master开启二进制日志，Master和Slave的server_id在局域网内必须唯一  分库、扩容的时候的数据迁移 分库分表 目前绝大多数应用采取的两种分库分表规则\n mod方式 dayofweek系列日期方式（所有星期1的数据在一个库/表,或所有?月份的数据在一个库表）  这两种方式有个本质的特点，就是 离散性加周期性。例如以一个表的主键对 3 取余数的方式分库或分表：\n那么随着数据量的增大，每个表或库的数据量都是各自增长。当一个表或库的数据量增长到了一个极限，要加库或加表的时候， 介于这种分库分表算法的离散性，必需要做数据迁移才能完成。例如从3个扩展到5个的时候：\n需要将原先以 mod3 分类的数据，重新以 mod5 分类，不可避免的带来数据迁移。每个表的数据都要被重新分配到多个新的表 相似的例子比如从 dayofweek 分的 7 个库/表,要扩张为以 dayofmonth 分的 31 张库/表，同样需要进行数据迁移。\n数据迁移带来的问题是\n 业务至少要两次发布 要专门写工具来导数据。由于各业务之间的差别，很难做出统一的工具。目前几乎都是每个业务写一套 要解决增量、全量、时间点，数据不一致等问题  如何在数据量扩张到现有库表极限，加库加表时避免数据迁移呢？\n通常的数据增长往往是随着时间的推移增长的。随着业务的开展，时间的推移，数据量不断增加。\n考虑到数据增长的特点，如果我们以代表时间增长的字段，按递增的范围分库，则可以避免数据迁移。这样的方式下，在数据量再增加达到前几个库/表的上限时，则继续水平增加库表，原先的数据就不需要迁移了。但是这样的方式会带来一个 热点问题：当前的数据量达到某个库表的范围时，所有的插入操作，都集中在这个库/表了。\n所以在满足基本业务功能的前提下，分库分表方案应该尽量避免的两个问题：\n 数据迁移 热点  如何既能避免数据迁移又能避免插入更新的热点问题呢？\n结合离散分库/分表和连续分库/分表的优点，如果一定要写热点和新数据均匀分配在每个库，同时又保证易于水平扩展，可以考虑这样的模式：\n水平扩展scale-out方案 \u0026ndash; 模式一 阶段一 一个库 DB0 之内分4个表，id%4 ：\n阶段二 增加 DB1 库，t2和t3整表搬迁到 DB1\n阶段三 增加 DB2 和 DB3 库，t1 整表搬迁到 DB2 ，t3整表搬迁的 DB3：\n为了规则表达，通过内部名称映射或其他方式，我们将DB1和DB2的名称和位置互换得到下图：\ndbRule: “DB” + (id % 4) tbRule: “t” + (id % 4) 即逻辑上始终保持4库4表，每个表一个库。这种做法也是目前店铺线图片空间采用的做法。\n上述方案有一个缺点，就是在从一个库到 4 个库的过程中，单表的数据量一直在增长。当单表的数据量超过一定范围时，可能会带来性能问题。比如索引的问题，历史数据清理的问题。另外当开始预留的表个数用尽，到了 4 物理库每库 1 个表的阶段，再进行扩容的话，不可避免的要从表上下手。\n水平扩展scale-out方案 \u0026ndash; 模式二 阶段一 一个数据库，两个表，rule0 = id % 2\n分库规则dbRule: “DB0″ 分表规则tbRule: “t” + (id % 2) 阶段二 当单库的数据量接近 1千万，单表的数据量接近 500 万时，进行扩容（数据量只是举例，具体扩容量要根据数据库和实际压力状况决定）：增加一个数据库 DB1，将 DB0.t0 整表迁移到新库 DB1.t1。每个库各增加1个表，未来10M-20M的数据mod2分别写入这2个表：t0_1，t1_1：\n分库规则dbRule:\n“DB” + (id % 2) 分表规则tbRule:\n if(id \u0026lt; 1千万){ return \u0026quot;t\u0026quot;+ (id % 2); //1千万之前的数据，仍然放在t0和t1表。t1表从DB0搬迁到DB1库 }else if(id \u0026lt; 2千万){ return \u0026quot;t\u0026quot;+ (id % 2) +\u0026quot;_1\u0026quot;; //1千万之后的数据，各放到两个库的两个表中: t0_1,t1_1 }else{ throw new IllegalArgumentException(\u0026quot;id outof range[20000000]:\u0026quot; + id); } 这样 10M 以后的新生数据会均匀分布在 DB0 和 DB1; 插入更新和查询热点仍然能够在每个库中均匀分布。每个库中同时有老数据和不断增长的新数据。每表的数据仍然控制在 500万 以下。\n阶段三 当两个库的容量接近上限继续水平扩展时，进行如下操作：\n 新增加两个库：DB2和DB3，以id % 4分库。余数0、1、2、3分别对应DB的下标. t0和t1不变， 将DB0.t0_1整表迁移到DB2; 将DB1.t1_1整表迁移到DB3  20M-40M的数据 mod4 分为 4 个表：t0_2，t1_2，t2_2，t3_2，分别放到4个库中：\n新的分库分表规则如下：\n分库规则dbRule:\n if(id \u0026lt; 2千万){ //2千万之前的数据，4个表分别放到4个库 if(id \u0026lt; 1千万){ return \u0026quot;db\u0026quot;+ (id % 2); //原t0表仍在db0, t1表仍在db1 }else{ return \u0026quot;db\u0026quot;+ ((id % 2) +2); //原t0_1表从db0搬迁到db2; t1_1表从db1搬迁到db3 } }else if(id \u0026lt; 4千万){ return \u0026quot;db\u0026quot;+ (id % 4); //超过2千万的数据，平均分到4个库 }else{ throw new IllegalArgumentException(\u0026quot;id out of range. id:\u0026quot;+id); } 分表规则tbRule:\n if(id \u0026lt; 2千万){ //2千万之前的数据，表规则和原先完全一样，参见阶段二 if(id \u0026lt; 1千万){ return \u0026quot;t\u0026quot;+ (id % 2); //1千万之前的数据，仍然放在t0和t1表 }else{ return \u0026quot;t\u0026quot;+ (id % 2) +\u0026quot;_1\u0026quot;; //1千万之后的数据，仍然放在t0_1和t1_1表 } }else if(id \u0026lt; 4千万){ return \u0026quot;t\u0026quot;+ (id % 4)+\u0026quot;_2\u0026quot;; //超过2千万的数据分为4个表t0_2，t1_2，t2_2，t3_2 }else{ throw new IllegalArgumentException(\u0026quot;id out of range. id:\u0026quot;+id); } 随着时间的推移，当第一阶段的t0/t1，第二阶段的t0_1/t1_1逐渐成为历史数据，不再使用时，可以直接truncate掉整个表。省去了历史数据迁移的麻烦。\n水平扩展scale-out方案 \u0026ndash; 模式三 非倍数扩展：如果从上文的阶段二到阶段三不希望一下增加两个库呢？尝试如下方案：\n迁移前：\n新增库为DB2，t0、t1都放在 DB0，\nt0_1整表迁移到 DB1 t1_1整表迁移到 DB2 迁移后：\n这时 DB0 退化为旧数据的读库和更新库。新增数据的热点均匀分布在 DB1 和 DB2 4无法整除3，因此如果从4表2库扩展到3个库，不做行级别的迁移而又保证热点均匀分布看似无法完成。\n当然如果不限制每库只有两个表，也可以如下实现：\n小于 10M 的 t0 和 t1 都放到 DB0 ，以 mod2 分为两个表，原数据不变 10M-20M的，以 mod2 分为两个表 t0_1、t1_1，原数据不变，分别搬迁到 DB1 ，和 DB2 20M 以上的以 mod3 平均分配到 3 个 DB 库的 t_0、t_2、t_3表中\n这样 DB1 包含最老的两个表，和最新的 1/3 数据。DB1 和 DB2 都分表包含次新的两个旧表 t0_1、t1_1 和最新的 1/3 数据。新旧数据读写都可达到均匀分布。\n总结 总而言之，两种规则映射（函数）：\n 离散映射：如mod或dayofweek， 这种类型的映射能够很好的解决热点问题，但带来了数据迁移和历史数据问题。 连续映射；如按id或gmt_create_time的连续范围做映射。这种类型的映射可以避免数据迁移，但又带来热点问题。  离散映射和连续映射这两种相辅相成的映射规则，正好解决热点和迁移这一对相互矛盾的问题。\n我们之前只运用了离散映射，引入连续映射规则后，两者结合，精心设计，应该可以设计出满足避免热点和减少迁移之间任意权衡取舍的规则。\n基于以上考量，分库分表规则的设计和配置，长远说来必须满足以下要求\n 可以动态推送修改 规则可以分层级叠加，旧规则可以在新规则下继续使用，新规则是旧规则在更宽尺度上的拓展，以此支持新旧规则的兼容，避免数据迁移 用 mod 方式时，最好选 2 的指数级倍分库分表，这样方便以后切割。  分库分表后全局ID怎么做  数据库自增 id 设置数据库 sequence 或者表自增字段步长 UUID 获取系统当前时间 snowflake 算法  snowflake twitter 开源的分布式 id 生成算法，采用 Scala 语言实现，是把一个 64 位的 long 型的 id ，1 个 bit 是不用的，用其中的 41 bit 作为毫秒数，用 10 bit 作为工作机器 id ，12 bit 作为序列号。\n 1 bit：不用，为啥呢？因为二进制里第一个 bit 为如果是 1，那么都是负数，但是我们生成的 id 都是正数，所以第一个 bit 统一都是 0。 41 bit：表示的是时间戳，单位是毫秒。41 bit 可以表示的数字多达 2^41 - 1，也就是可以标识 2^41 - 1 个毫秒值，换算成年就是表示69年的时间。 10 bit：记录工作机器 id，代表的是这个服务最多可以部署在 2^10台机器上哪，也就是1024台机器。但是 10 bit 里 5 个 bit 代表机房 id，5 个 bit 代表机器 id。意思就是最多代表 2^5个机房（32个机房），每个机房里可以代表 2^5 个机器（32台机器）。 12 bit：这个是用来记录同一个毫秒内产生的不同 id，12 bit 可以代表的最大正整数是 2^12 - 1 = 4096，也就是说可以用这个 12 bit 代表的数字来区分同一个毫秒内的 4096 个不同的 id。  "});index.add({'id':45,'href':'/interview/docs/fromwork/netty/','title':"Netty",'content':"Netty Netty 是一个 异步 事件驱动 的网络应用框架，用于快速开发高性能、可扩展协议的服务器和客户端\nReactor 无论是 C++ 还是 Java 编写的网络框架，大多数都是基于 Reactor 模式进行设计和开发，Reactor 模式基于事件驱动，特别适合处理海量的 I/O 事件。\n反应器设计模式-维基百科 \u0026ndash; 反应器设计模式(Reactor pattern)是一种为处理服务请求并发 提交到一个或者多个服务处理程序的事件设计模式。当请求抵达后，服务处理程序使用解多路分配策略，然后同步地派发这些请求至相关的请求处理程序。\n单线程模型 Reactor 单线程模型，指的是所有的 IO 操作都在同一个 NIO 线程上面完成，NIO 线程的职责如下：\n 作为 NIO 服务端，接收客户端的 TCP 连接； 作为 NIO 客户端，向服务端发起 TCP 连接； 读取通信对端的请求或者应答消息； 向通信对端发送消息请求或者应答消息。  由于 Reactor 模式使用的是异步非阻塞 IO，所有的 IO 操作都不会导致阻塞，理论上一个线程可以独立处理所有 IO 相关的操作。从架构层面看，一个 NIO 线程确实可以完成其承担的职责。例如，通过 Acceptor 类接收客户端的 TCP 连接请求消息，链路建立成功之后，通过 Dispatch 将对应的 ByteBuffer 派发到指定的 Handler 上进行消息解码。用户线程可以通过消息编码通过 NIO 线程将消息发送给客户端。\n对于一些小容量应用场景，可以使用单线程模型。但是 对于高负载、大并发的应用场景却不合适。\n多线程模型 Rector 多线程模型与单线程模型最大的区别就是有一组 NIO 线程处理 IO 操作，它的原理图如下：\nReactor 多线程模型的特点：\n 有专门一个 NIO 线程 Acceptor 线程用于监听服务端，接收客户端的 TCP 连接请求； 网络 IO 操作 - 读、写等由一个 NIO 线程池负责，线程池可以采用标准的 JDK 线程池实现，它包含一个任务队列和 N 个可用的线程，由这些 NIO 线程负责消息的读取、解码、编码和发送； 1 个 NIO 线程可以同时处理 N 条链路，但是 1 个链路只对应 1 个 NIO 线程，防止发生并发操作问题。  主从多线程模型 主从 Reactor 线程模型的特点是：服务端用于接收客户端连接的不再是个 1 个单独的 NIO 线程，而是一个独立的 NIO 线程池。 Acceptor 接收到客户端 TCP 连接请求处理完成后（可能包含接入认证等），将新创建的 SocketChannel 注册到 IO 线程池（sub reactor 线程池）的某个 IO 线程上，由它负责 SocketChannel 的读写和编解码工作。 Acceptor 线程池仅仅只用于客户端的登陆、握手和安全认证，一旦链路建立成功，就将链路注册到后端 subReactor 线程池的 IO 线程上，由 IO 线程负责后续的 IO 操作。\n它的工作流程总结如下：\n 从主线程池中随机选择一个 Reactor 线程作为 Acceptor 线程，用于绑定监听端口，接收客户端连接； Acceptor 线程接收客户端连接请求之后创建新的 SocketChannel ，将其注册到主线程池的其它 Reactor 线程上，由其负责接入认证、IP 黑白名单过滤、握手等操作； 步骤 2 完成之后，业务层的链路正式建立，将 SocketChannel 从主线程池的 Reactor 线程的多路复用器上摘除，重新注册到 Sub 线程池的线程上，用于处理 I/O 的读写操作。  Netty 的优势  多路复用，并在 NIO 的基础上进行更高层次的抽象 事件机制 功能强大，预置了多种编解码功能，支持多种主流协议 定制能力强，可以通过ChannelHandler对通信框架进行灵活的扩展  Netty 为什么性能好？  纯异步：Reactor 线程模型 IO 多路复用 GC 优化：更少的分配内存、池化（Pooling）、复用、选择性的使用 sun.misc.Unsafe 更多的硬件相关优化（mechanical sympathy） 内存泄漏检测 \u0026ldquo;Zero Copy\u0026rdquo;  Zero Copy Netty 的 Zero-copy 体现在如下几个个方面:\n Netty 提供了 CompositeByteBuf 类, 它可以将多个 ByteBuf 合并为一个逻辑上的 ByteBuf , 避免了各个 ByteBuf 之间的拷贝. 通过 wrap 操作, 我们可以将 byte[] 数组、ByteBuf 、 ByteBuffer 等包装成一个 Netty ByteBuf 对象, 进而避免了拷贝操作. ByteBuf 支持 slice 操作, 因此可以将 ByteBuf 分解为多个共享同一个存储区域的 ByteBuf, 避免了内存的拷贝. 通过 FileRegion 包装的 FileChannel.tranferTo 实现文件传输, 可以直接将文件缓冲区的数据发送到目标 Channel , 避免了传统通过循环 write 方式导致的内存拷贝问题.  源码 ByteBuf  ByteBuf 扩容采用先倍增后步进的方式  DirectBuffer vs HeapBuffer 在执行网络IO或者文件IO时，如果是使用 DirectBuffer 就会少一次内存拷贝。如果是非 DirectBuffer ，JDK 会先创建一个 DirectBuffer ，再去执行真正的写操作。这是因为，当我们把一个地址通过 JNI 传递给底层的C库的时候，有一个基本的要求，就是这个地址上的内容不能失效。然而，在 GC 管理下的对象是会在 Java 堆中移动的。也就是说，有可能我把一个地址传给底层的 write ，但是这段内存却因为 GC 整理内存而失效了。所以我必须要把待发送的数据放到一个 GC 管不着的地方。这就是调用 native 方法之前，数据一定要在堆外内存的原因。\nNetty 启动以及链接建立过程 Epoll 触发 有两种模式，一是水平触发（LT），二是边缘触发（ET）。\n在LT模式下，只要某个fd还有数据没读完，那么下次轮询还会被选出。而在ET模式下，只有fd状态发生改变后，该fd才会被再次选出。ET模式的特殊性，使在ET模式下的一次轮询必须处理完本次轮询出的fd的所有数据，否则该fd将不会在下次轮询中被选出。\n NioChannel：是水平触发 EpollChannel：是边缘触发，Netty 为保证数据完整会在特定条件下自己触发 Epoll Event，来读取数据  JDK NIO BUG  正常情况下，selector.select() 操作是阻塞的，只有被监听的 fd 有读写操作时，才被唤醒 但是，在这个 bug 中，没有任何 fd 有读写请求，但是 select() 操作依旧被唤醒 很显然，这种情况下，selectedKeys() 返回的是个空数组 然后按照逻辑执行到 while(true) 处，循环执行，导致死循环。  Netty 解决方案：\nlong currentTimeNanos = System.nanoTime(); for (;;) { // 1.定时任务截止事时间快到了，中断本次轮询 ... // 2.轮询过程中发现有任务加入，中断本次轮询 ... // 3.阻塞式select操作 selector.select(timeoutMillis); // 4.解决jdk的nio bug long time = System.nanoTime(); if (time - TimeUnit.MILLISECONDS.toNanos(timeoutMillis) \u0026gt;= currentTimeNanos) { selectCnt = 1; } else if (SELECTOR_AUTO_REBUILD_THRESHOLD \u0026gt; 0 \u0026amp;\u0026amp; selectCnt \u0026gt;= SELECTOR_AUTO_REBUILD_THRESHOLD) { rebuildSelector(); selector = this.selector; selector.selectNow(); selectCnt = 1; break; } currentTimeNanos = time; ... } netty 会在每次进行 selector.select(timeoutMillis) 之前记录一下开始时间 currentTimeNanos ，在 select 之后记录一下结束时间，判断 select 操作是否至少持续了 timeoutMillis 秒。如果持续的时间大于等于 timeoutMillis ，说明就是一次有效的轮询，重置 selectCnt 标志，否则，表明该阻塞方法并没有阻塞这么长时间，可能触发了 jdk 的空轮询 bug ，当空轮询的次数超过一个阀值的时候，默认是 512 ，就开始重建 selector\n"});index.add({'id':46,'href':'/interview/docs/java/object/','title':"Object",'content':"Object getClass 返回该对象运行时的 class 对象，返回的 Class 对象是由所表示的类的静态同步方法锁定的对象。\nhashCode 返回该对象的 hashcode，该方法对hash表提供支持，例如 HashMap。 对于该方法有几点需要注意：\n 在运行中的Java应用，如果用在 equals 中进行比较的信息没有改变，那么不论何时调用都需要返回一致的int值。这个hash值在应用的两次执行中不需要保持一致。 如果两个对象根据 equals 方法认为是相等的，那么这两个对象也应该返回相等的 hashcode。 不要求两个不相等的对象，在调用 hashCode 方法返回的结果是必须是不同的。然而，程序员应该了解不同的对象产生不同的 hashcode 能够提升哈希表的效率。 Object的hashcode对不同的对象，尽可能返回不同的 hashcode 。这通常通过将对象的内部地址转换为整数来实现，但Java编程语言不需要此实现技术。  Arrays.hashCode Arrays.hashCode 是一个数组的浅哈希码实现，深哈希可以使用 deepHashCode。并且当数组长度为1时，Arrays.hashCode(object) = object.hashCode 不一定成立\n31 不论是String、Arrays在计算多个元素的哈希值的时候，都会有31这个数字。主要有以下两个原因：\n  31是一个不大不小的质数，是作为 hashCode 乘子的优选质数之一。\n 另外一些相近的质数，比如37、41、43等等，也都是不错的选择。那么为啥偏偏选中了31呢？请看第二个原因。\n   31可以被 JVM 优化，$$31 * i = (i \u0026laquo; 5) - i$$。\n  上面两个原因中，第一个需要解释一下，第二个比较简单，就不说了。一般在设计哈希算法时，会选择一个特殊的质数。至于为啥选择质数，我想应该是可以降低哈希算法的冲突率。\n在 Effective Java 中有一段相关的解释：\n 选择数字31是因为它是一个奇质数，如果选择一个偶数会在乘法运算中产生溢出，导致数值信息丢失，因为乘二相当于移位运算。选择质数的优势并不是特别的明显，但这是一个传统。同时，数字31有一个很好的特性，即乘法运算可以被移位和减法运算取代，来获取更好的性能：$$31 * i == (i \u0026laquo; 5) - i$$，现代的 Java 虚拟机可以自动的完成这个优化。\n equals 判定两个对象是否相等。equals和hashCode需要同时被overwrite\nclone 创建一个该对象的副本，并且对于对象 x 应当满足以下表达式：\nx.clone() != x x.clone().getClass() == x.getClass() x.clone().equals(x) toString wait 当前线程等待知道其他线程调用该对象的 notify 或者 notifyAll方法。当前线程必须拥有该对象的 monitor。线程释放该对象monitor的拥有权，并且等待到别的线程通知等待在该对象monitor上的线程苏醒。然后线程重新拥有monitor并继续执行。在某些jdk版本中，中断和虚假唤醒是存在的，所以wait方法需要放在循环中。\nsynchronized (obj) { while (\u0026lt;condition does not hold\u0026gt;) obj.wait(); ... // Perform action appropriate to condition } 该方法只能被拥有该对象monitor的线程调用。\n虚假唤醒（spurious wakeup） 虚假唤醒就是一些obj.wait()会在除了obj.notify()和obj.notifyAll()的其他情况被唤醒，而此时是不应该唤醒的。\n 注意 Lock 的 Conditon.await 也有虚假唤醒的问题\n 解决的办法是基于while来反复判断进入正常操作的临界条件是否满足\n 同时也可以使用同步数据结构：BlokingQueue\n 解释 虚假唤醒（spurious wakeup）是一个表象，即在多处理器的系统下发出 wait 的程序有可能在没有 notify 唤醒的情形下苏醒继续执行。\n以运行在 Linux 的 hotspot 虚拟机上的 java 程序为例， wait 方法在 jvm 执行时实质是调用了底层 pthread_cond_wait/pthread_cond_timedwait 函数，挂起等待条件变量来达到线程间同步通信的效果，而底层 wait 函数在设计之初为了不减慢条件变量操作的效率并没有去保证每次唤醒都是由 notify 触发，而是把这个任务交由上层应用去实现，即使用者需要定义一个循环去判断是否条件真能满足程序继续运行的需求，当然这样的实现也可以避免因为设计缺陷导致程序异常唤醒的问题。\nnotify 唤醒一个等待在该对象monitor上的线程。如果有多个线程等待，则会随机选择一个线程唤醒。线程等待是通过调用wait方法。\n唤醒的线程不会立即执行，直到当前线程放弃对象上的锁。唤醒的线程也会以通常的方式和竞争该对象锁的线程进行竞争。也就是说，唤醒的线程在对该对象的加锁中没有任何优先级。\n该方法只能被拥有该对象monitor的线程调用。线程拥有monitor有下面三种方式：\n 执行该对象的 synchronized 方法 执行以该对象作为同步语句的synchronized方法体 对于class对象，可以执行该对象的static synchronized方法  在同一时间只能有一个线程能够拥有该对象monitor\nfinalize 当 GC 认为该对象已经没有任何引用的时候，该方法被GC收集器调用。子类可以 overwrite 该方法来关闭系统资源或者其他清理任务。\nfinalize 的一般契约是，如果 Java 虚拟机确定不再有任何方法可以通过任何尚未死亡的线程访问此对象，除非由于某个操作，它将被调用通过最终确定准备完成的其他一些对象或类来完成。 finalize 方法可以采取任何操作，包括使该对象再次可用于其他线程；但是，finalize 的通常目的是在对象被不可撤销地丢弃之前执行清理操作。例如，表示输入/输出连接的对象的 finalize 方法可能会执行显式 I/O 事务，以在永久丢弃对象之前断开连接。\n类 Object 的 finalize 方法不执行任何特殊操作;它只是正常返回。 Object 的子类可以覆盖此定义。\nJava 编程语言不保证哪个线程将为任何给定对象调用 finalize 方法。但是，可以保证，调用 finalize 时，调用 finalize 的线程不会持有任何用户可见的同步锁。如果 finalize 方法抛出未捕获的异常，则忽略该异常并终止该对象的终止。在为对象调用 finalize 方法之后，在 Java 虚拟机再次确定不再有任何方法可以通过任何尚未死亡的线程访问此对象之前，不会采取进一步操作，包括可能的操作通过准备完成的其他对象或类，此时可以丢弃该对象。\n对于任何给定对象，Java 虚拟机永远不会多次调用 finalize 方法。 finalize 方法抛出的任何异常都会导致暂停此对象的终结，但会被忽略。\n缺陷  一些与 finalize 相关的方法，由于一些致命的缺陷，已经被废弃了，如 System.runFinalizersOnExit() 方法、Runtime.runFinalizersOnExit()方法。 System.gc() 与 System.runFinalization() 方法增加了finalize方法执行的机会，但不可盲目依赖它们。 Java 语言规范并不保证 finalize 方法会被及时地执行、而且根本不会保证它们会被执行。 finalize 方法可能会带来性能问题。因为JVM通常在单独的低优先级线程中完成finalize的执行。 对象再生问题： finalize 方法中，可将待回收对象赋值给GC Roots可达的对象引用，从而达到对象再生的目的。 finalize 方法至多由GC执行一次(用户当然可以手动调用对象的 finalize 方法，但并不影响GC对 finalize 的行为)。  "});index.add({'id':47,'href':'/interview/docs/basic/database/redis/','title':"Redis",'content':"Redis 线程模型 Redis 在处理网络请求是使用单线程模型，并通过 IO 多路复用来提高并发。但是在其他模块，比如：持久化，会使用多个线程。\nRedis 内部使用文件事件处理器 file event handler，这个文件事件处理器是单线程的，所以 redis 才叫做单线程的模型。它采用 IO 多路复用机制同时监听多个 socket ，将产生事件的 socket 压入内存队列中，事件分派器根据 socket 上的事件类型来选择对应的事件处理器进行处理。\n文件事件处理器的结构包含 4 个部分：\n 多个 socket IO 多路复用程序 文件事件分派器 事件处理器（连接应答处理器、命令请求处理器、命令回复处理器）  多个 socket 可能会并发产生不同的操作，每个操作对应不同的文件事件，但是 IO 多路复用程序会监听多个 socket ，会将产生事件的 socket 放入队列中排队，事件分派器每次从队列中取出一个 socket ，根据 socket 的事件类型交给对应的事件处理器进行处理。\n客户端与 redis 的一次通信过程：\n为啥 redis 单线程模型也能效率这么高？  纯内存操作 核心是基于非阻塞的 IO 多路复用机制 单线程反而避免了多线程的频繁上下文切换问题  数据结构 Redis的外围由一个键、值映射的字典构成。与其他非关系型数据库主要不同在于：Redis中值的类型不仅限于 字符串，还支持如下抽象数据类型：\n List：字符串列表 Set：无序不重复的字符串集合 Soret Set：有序不重复的字符串集合 HashTable：键、值都为字符串的哈希表  值的类型决定了值本身支持的操作。Redis支持不同无序、有序的列表，无序、有序的集合间的交集、并集等高级服务器端原子操作。\n持久化：  使用快照，一种半持久耐用模式。不时的将数据集以异步方式从内存以RDB格式写入硬盘。 1.1版本开始使用更安全的 AOF 格式替代，一种只能追加的日志类型。将数据集修改操作记录起来。Redis 能够在后台对只可追加的记录作修改来避免无限增长的日志。   1. aof文件比rdb更新频率高，优先使用aof还原数据。 2. aof比rdb更安全也更大 3. rdb性能比aof好 4. 如果两个都配了优先加载AOF 一致性哈希算法 一致哈希 是一种特殊的哈希算法。在使用一致哈希算法后，哈希表槽位数（大小）的改变平均只需要对 K/n 个关键字重新映射，其中 K 是关键字的数量，n 是槽位数量。然而在传统的哈希表中，添加或删除一个槽位的几乎需要对所有关键字进行重新映射。\n 一致哈希也可用于实现健壮缓存来减少大型 Web 应用中系统部分失效带来的负面影响\n 需求 在使用 n 台缓存服务器时，一种常用的负载均衡方式是，对资源 o 的请求使用 hash(o)= o mod n 来映射到某一台缓存服务器。当增加或减少一台缓存服务器时这种方式可能会改变所有资源对应的 hash 值，也就是所有的缓存都失效了，这会使得缓存服务器大量集中地向原始内容服务器更新缓存。\n因此需要一致哈希算法来避免这样的问题。 一致哈希尽可能使同一个资源映射到同一台缓存服务器。这种方式要求增加一台缓存服务器时，新的服务器尽量分担存储其他所有服务器的缓存资源。减少一台缓存服务器时，其他所有服务器也可以尽量分担存储它的缓存资源。\n一致哈希算法的主要思想是将每个缓存服务器与一个或多个哈希值域区间关联起来，其中区间边界通过计算缓存服务器对应的哈希值来决定。如果一个缓存服务器被移除，则它所对应的区间会被并入到邻近的区间，其他的缓存服务器不需要任何改变。\n实现 一致哈希将每个对象映射到圆环边上的一个点，系统再将可用的节点机器映射到圆环的不同位置。查找某个对象对应的机器时，需要用一致哈希算法计算得到对象对应圆环边上位置，沿着圆环边上查找直到遇到某个节点机器，这台机器即为对象应该保存的位置。\n当删除一台节点机器时，这台机器上保存的所有对象都要移动到下一台机器。添加一台机器到圆环边上某个点时，这个点的下一台机器需要将这个节点前对应的对象移动到新机器上。更改对象在节点机器上的分布可以通过调整节点机器的位置来实现。\n实践  假设有1000w个数据项，100个存储节点，请设计一种算法合理地将他们存储在这些节点上。\n 看一看普通Hash算法的原理：\nfor item in range(ITEMS): k = md5(str(item)).digest() h = unpack_from(\u0026quot;\u0026gt;I\u0026quot;, k)[0] # 通过取余的方式进行映射 n = h % NODES node_stat[n] += 1 普通的Hash算法均匀地将这些数据项打散到了这些节点上，并且分布最少和最多的存储节点数据项数目小于 1%。之所以分布均匀，主要是依赖 Hash 算法（实现使用的MD5算法）能够比较随机的分布。\n然而，我们看看存在一个问题，由于 该算法使用节点数取余的方法，强依赖 node 的数目，因此，当是 node 数发生变化的时候，item 所对应的 node 发生剧烈变化，而发生变化的成本就是我们需要在 node 数发生变化的时候，数据需要迁移，这对存储产品来说显然是不能忍的。\n一致性哈希 普通 Hash 算法的劣势，即当 node 数发生变化（增加、移除）后，数据项会被重新“打散”，导致大部分数据项不能落到原来的节点上，从而导致大量数据需要迁移。\n那么，一个亟待解决的问题就变成了：当 node 数发生变化时，如何保证尽量少引起迁移呢？即当增加或者删除节点时，对于大多数 item ，保证原来分配到的某个 node ，现在仍然应该分配到那个 node ，将数据迁移量的降到最低。\nfor n in range(NODES): h = _hash(n) ring.append(h) ring.sort() hash2node[h] = n for item in range(ITEMS): h = _hash(item) n = bisect_left(ring, h) % NODES node_stat[hash2node[ring[n]]] += 1 虽然一致性Hash算法解决了节点变化导致的数据迁移问题，但是，数据项分布的均匀性很差。\n主要是因为这 100 个节点 Hash 后，在环上分布不均匀，导致了每个节点实际占据环上的区间大小不一造成的。\n改进 \u0026ndash; 虚节点 当我们将 node 进行哈希后，这些值并没有均匀地落在环上，因此，最终会导致，这些节点所管辖的范围并不均匀，最终导致了数据分布的不均匀。\nfor n in range(NODES): for v in range(VNODES): h = _hash(str(n) + str(v)) # 构造ring ring.append(h) # 记录hash所对应节点 hash2node[h] = n ring.sort() for item in range(ITEMS): h = _hash(str(item)) # 搜索ring上最近的hash n = bisect_left(ring, h) % (NODES*VNODES) node_stat[hash2node[ring[n]]] += 1 通过增加虚节点的方法，使得每个节点在环上所“管辖”更加均匀。这样就既保证了在节点变化时，尽可能小的影响数据分布的变化，而同时又保证了数据分布的均匀。也就是靠增加“节点数量”加强管辖区间的均匀。\n集群 哨兵 \u0026ndash; Sentinel Redis-Sentinel 是 Redis 官方推荐的 高可用性( HA )解决方案，当用 Redis 做 Master-slave 的高可用方案时，假如 master 宕机了， Redis 本身(包括它的很多客户端)都没有实现自动进行主备切换，而 Redis-sentinel 本身也是一个独立运行的进程，它能监控多个 master-slave 集群，发现 master 宕机后能进行自懂切换。\n它的主要功能有以下几点\n 不时地监控 redis 是否按照预期良好地运行; 如果发现某个 redis 节点运行出现状况，能够通知另外一个进程(例如它的客户端); 能够进行自动切换。当一个 master 节点不可用时，能够选举出 master 的多个slave (如果有超过一个 slave 的话)中的一个来作为新的 master ,其它的 slave 节点会将它所追随的 master 的地址改为被提升为 master 的 slave 的新地址。  很显然，只使用单个 sentinel 进程来监控 redis 集群是不可靠的，当 sentinel 进程宕掉后( sentinel 本身也有单点问题，single-point-of-failure)整个集群系统将无法按照预期的方式运行。所以有必要将sentinel集群，这样有几个好处：\n 即使有一些sentinel进程宕掉了，依然可以进行redis集群的主备切换； 如果只有一个sentinel进程，如果这个进程运行出错，或者是网络堵塞，那么将无法实现redis集群的主备切换; 如果有多个sentinel，redis的客户端可以随意地连接任意一个sentinel来获得关于redis集群中的信息。  Redis Cluster Redis Cluster 是一种服务器 Sharding 技术，3.0版本开始正式提供。\nRedis Cluster中，Sharding 采用 slot(槽) 的概念，一共分成 16384 个槽，这有点儿类 pre sharding 思路。对于每个进入 Redis 的键值对，根据 key 进行散列，分配到这 16384 个 slot 中的某一个中。使用的hash算法也比较简单，就是 CRC16 后 16384 取模。要保证 16384 个槽对应的 node 都正常工作，如果某个 node 发生故障，那它负责的 slots 也就失效，整个集群将不能工作。\n 16384 = 2048 * 8 bit，2k 大小的 bit 数\n 为了增加集群的可访问性，官方推荐的方案是将 node 配置成 主从结构，即一个 master 主节点，挂 n 个 slave 从节点。这时，如果主节点失效，Redis Cluster 会根据选举算法从 slave 节点中选择一个上升为主节点，整个集群继续对外提供服务。\n对客户端来说，整个 cluster 被看做是一个整体，客户端可以连接任意一个 node 进行操作，就像操作单一 Redis 实例一样，当客户端操作的 key 没有分配到该 node 上时，Redis 会返回转向指令，指向正确的 node ，这有点儿像浏览器页面的 302 redirect 跳转。\nRedis Sharding 集群 Redis Sharding 是客户端 Sharding 的方案，其主要思想是采用哈希算法 将 Redis 数据的 key 进行散列，通过 hash 函数，特定的 key 会映射到特定的 Redis 节点上。这样，客户端就知道该向哪个 Redis 节点操作数据。\nJedis 的 Redis Sharding 实现具有如下特点：\n 采用一致性哈希算法(consistent hashing)，将key和节点name同时hashing，然后进行映射匹配，采用的算法是MURMUR_HASH。采用一致性哈希而不是采用简单类似哈希求模映射的主要原因是当增加或减少节点时，不会产生由于重新匹配造成的rehashing。一致性哈希只影响相邻节点key分配，影响量小。 为了避免一致性哈希只影响相邻节点造成节点分配压力， ShardedJedis 会对每个Redis 节点根据名字(没有，Jedis会赋予缺省名字)会 虚拟化出160个虚拟节点 进行散列。根据权重 weight ，也可虚拟化出160倍数的虚拟节点。用虚拟节点做映射匹配，可以在增加或减少 Redis 节点时，key 在各 Redis 节点移动再分配更均匀，而不是只有相邻节点受影响。 ShardedJedis 支持 keyTagPattern 模式，即抽取 key 的一部分 keyTag 做 sharding ，这样通过合理命名 key ，可以将一组相关联的key放入同一个 Redis 节点，这在避免跨节点访问相关数据时很重要。  string Redis 没有直接使用 C 语言传统的字符串表示（以空字符结尾的字符数组，以下简称 C 字符串）， 而是自己构建了一种名为简单动态字符串（simple dynamic string，SDS）的抽象类型， 并将 SDS 用作 Redis 的默认字符串表示。\n在 Redis 里面， C 字符串只会作为字符串字面量（string literal）， 用在一些无须对字符串值进行修改的地方， 比如打印日志。\n当 Redis 需要的不仅仅是一个字符串字面量， 而是一个可以被修改的字符串值时， Redis 就会使用 SDS 来表示字符串值： 比如在 Redis 的数据库里面， 包含字符串值的键值对在底层都是由 SDS 实现的。\n   C字符串 SDS     获取字符串长度的复杂度为 O(N) 。 获取字符串长度的复杂度为 O(1) 。   API 是不安全的，可能会造成缓冲区溢出。 API 是安全的，不会造成缓冲区溢出。   修改字符串长度 N 次必然需要执行 N 次内存重分配。 修改字符串长度 N 次最多需要执行 N 次内存重分配。   只能保存文本数据。 可以保存文本或者二进制数据。   可以使用所有 \u0026lt;string.h\u0026gt; 库中的函数。 可以使用一部分 \u0026lt;string.h\u0026gt; 库中的函数。    缓冲区溢出 因为 C 字符串不记录自身的长度， 所以 strcat 假定用户在执行这个函数时， 已经为 dest 分配了足够多的内存， 可以容纳 src 字符串中的所有内容， 而一旦这个假定不成立时， 就会产生缓冲区溢出。\n举个例子， 假设程序里有两个在内存中紧邻着的 C 字符串 s1 和 s2 ， 其中 s1 保存了字符串 \u0026quot;Redis\u0026quot; ， 而 s2 则保存了字符串 \u0026quot;MongoDB\u0026quot; ， 如图所示。\n如果一个程序员决定通过执行：\nstrcat(s1, \u0026quot; Cluster\u0026quot;); 将 s1 的内容修改为 \u0026quot;Redis Cluster\u0026quot; ， 但粗心的他却忘了在执行 strcat 之前为 s1 分配足够的空间， 那么在 strcat 函数执行之后， s1 的数据将溢出到 s2 所在的空间中， 导致 s2 保存的内容被意外地修改， 如图所示。\n与 C 字符串不同， SDS 的空间分配策略完全杜绝了发生缓冲区溢出的可能性： 当 SDS API 需要对 SDS 进行修改时， API 会先检查 SDS 的空间是否满足修改所需的要求， 如果不满足的话， API 会自动将 SDS 的空间扩展至执行修改所需的大小， 然后才执行实际的修改操作， 所以使用 SDS 既不需要手动修改 SDS 的空间大小， 也不会出现前面所说的缓冲区溢出问题。\n减少修改字符串时带来的内存重分配次数  空间预分配：解决 append 问题 惰性空间释放：解决 strim 问题  二进制安全 C 字符串中的字符必须符合某种编码（比如 ASCII）， 并且 除了字符串的末尾之外， 字符串里面不能包含空字符， 否则最先被程序读入的空字符将被误认为是字符串结尾 —— 这些限制使得 C 字符串只能保存文本数据， 而不能保存像图片、音频、视频、压缩文件这样的二进制数据。\nzset底层实现 跳跃表（skiplist）是一种有序数据结构， 它通过在每个节点中维持多个指向其他节点的指针， 从而达到快速访问节点的目的。\nRedis 使用跳跃表作为有序集合键的底层实现之一：\n 如果一个有序集合包含的元素数量比较多， 有序集合中元素的成员（member）是比较长的字符串时  Redis 就会使用跳跃表来作为有序集合键的底层实现。\n和链表、字典等数据结构被广泛地应用在 Redis 内部不同， Redis 只在两个地方用到了跳跃表， 一个是实现有序集合键， 另一个是在集群节点中用作内部数据结构， 除此之外， 跳跃表在 Redis 里面没有其他用途。\n缓存穿透、缓存击穿、缓存雪崩 缓存穿透 访问一个不存在的key，缓存不起作用，请求会穿透到 DB，流量大时 DB 会挂掉。\n解决方案   采用布隆过滤器，使用一个足够大的bitmap，用于存储可能访问的 key，不存在的key直接被过滤；\n  访问key未在DB查询到值，也将空值写进缓存，但可以设置较短过期时间。\n  缓存雪崩 大量的 key 设置了相同的过期时间，导致在缓存在同一时刻全部失效，造成瞬时DB请求量大、压力骤增，引起雪崩。\n解决方案 可以给缓存设置过期时间时加上一个随机值时间，使得每个 key 的过期时间分布开来，不会集中在同一时刻失效。\n缓存击穿 对于一些设置了过期时间的key，如果这些key可能会在某些时间点被超高并发地访问，是一种非常“热点”的数据。这个时候，需要考虑一个问题：缓存被“击穿”的问题，这个和缓存雪崩的区别在于这里针对某一 key 缓存，前者则是很多key。\n缓存在某个时间点过期的时候，恰好在这个时间点对这个Key有大量的并发请求过来，这些请求发现缓存过期一般都会从后端DB加载数据并回设到缓存，这个时候大并发的请求可能会瞬间把后端 DB 压垮。\n解决方案 在缓存失效的时候（判断拿出来的值为空），不是立即去 load db ，而是先使用缓存工具的某些带成功操作返回值的操作（比如Redis的 SETNX）去 set 一个 mutex key ，当操作返回成功时，再进行 load db 的操作并回设缓存；否则，就重试整个 get 缓存的方法。\nRedis分布式锁  加锁：redis.set(String key, String value, String nxxx, String expx, int time) 解锁：通过 Lua 脚本执行 if redis.call('get', KEYS[1]) == ARGV[1] then return redis.call('del', KEYS[1]) else return 0 end  数据淘汰机制 对象过期 Redis回收过期对象的策略：定期删除+惰性删除\n 惰性删除：当读/写一个已经过期的key时，会触发惰性删除策略，直接删除掉这个过期key 定期删除：由于惰性删除策略无法保证冷数据被及时删掉，所以Redis会定期主动淘汰一批已过期的key  内存淘汰 Redis提供了下面几种淘汰策略供用户选择，其中默认的策略为noeviction策略：\n noeviction：当内存使用达到阈值的时候，所有引起申请内存的命令会报错。 allkeys-lru：在主键空间中，优先移除最近未使用的key。 volatile-lru：在设置了过期时间的键空间中，优先移除最近未使用的key。 allkeys-random：在主键空间中，随机移除某个key。 volatile-random：在设置了过期时间的键空间中，随机移除某个key。 volatile-ttl：在设置了过期时间的键空间中，具有更早过期时间的key优先移除。   这里补充一下主键空间和设置了过期时间的键空间，举个例子，假设我们有一批键存储在Redis中，则有那么一个哈希表用于存储这批键及其值，如果这批键中有一部分设置了过期时间，那么这批键还会被存储到另外一个哈希表中，这个哈希表中的值对应的是键被设置的过期时间。设置了过期时间的键空间为主键空间的子集。\n 非精准的LRU 上面提到的LRU（Least Recently Used）策略，实际上 Redis 实现的 LRU 并不是可靠的 LRU，也就是名义上我们使用LRU算法淘汰键，但是实际上被淘汰的键并不一定是真正的最久没用的，这里涉及到一个权衡的问题，如果需要在全部键空间内搜索最优解，则必然会增加系统的开销，Redis是单线程的，也就是同一个实例在每一个时刻只能服务于一个客户端，所以耗时的操作一定要谨慎。\n为了在一定成本内实现相对的LRU，早期的 Redis 版本是 基于采样的 LRU ，也就是放弃全部键空间内搜索解改为采样空间搜索最优解。自从 Redis3.0 版本之后，Redis 作者对于基于采样的 LRU 进行了一些优化，目的是在一定的成本内让结果更靠近真实的 LRU。\n"});index.add({'id':48,'href':'/interview/docs/architecture/distributed/rpc/','title':"RPC",'content':"RPC 远程过程调用（英语：Remote Procedure Call，缩写为 RPC）是一个计算机通信协议。该协议允许运行于一台计算机的程序调用另一台计算机的子程序，而程序员无需额外地为这个交互作用编程。\n应用发展流程 单一应用架构 当网站流量很小时，只需一个应用，将所有功能都部署在一起，以减少部署节点和成本。此时，用于简化增删改查工作量的数据访问框架(ORM)是关键。\n垂直应用架构 当访问量逐渐增大，单一应用增加机器带来的加速度越来越小，将应用拆成互不相干的几个应用，以提升效率。此时，用于加速前端页面开发的Web框架(MVC)是关键。\n分布式服务架构 当垂直应用越来越多，应用之间交互不可避免，将核心业务抽取出来，作为独立的服务，逐渐形成稳定的服务中心，使前端应用能更快速的响应多变的市场需求。此时，用于提高业务复用及整合的分布式服务框架(RPC)是关键。\n流动计算架构 当服务越来越多，容量的评估，小服务资源的浪费等问题逐渐显现，此时需增加一个调度中心基于访问压力实时管理集群容量，提高集群利用率。此时，用于提高机器利用率的资源调度和治理中心(SOA)是关键。\n"});index.add({'id':49,'href':'/interview/docs/fromwork/spring/','title':"Spring 基本",'content':"Spring Spring Framework 是一个开源的Java／Java EE全功能栈（full-stack）的应用程序框架，其提供了一个简易的开发方式，这种开发方式，将避免那些可能致使底层代码变得繁杂混乱的大量的属性文件和帮助类。\nSpring中包含的关键特性  强大的基于JavaBeans的采用 控制反转 （Inversion of Control，IoC）原则的配置管理，使得应用程序的组建更加快捷简易。 一个可用于 Java EE 等运行环境的核心 Bean 工厂。 数据库事务的一般化抽象层，允许声明式（Declarative）事务管理器，简化事务的划分使之与底层无关。 内建的针对 JTA 和单个 JDBC 数据源的一般化策略，使 Spring 的事务支持不要求Java EE环境，这与一般的JTA或者EJB CMT相反。 JDBC 抽象层提供了有针对性的异常等级（不再从SQL异常中提取原始代码），简化了错误处理，大大减少了程序员的编码量。再次利用JDBC时，你无需再写出另一个'终止\u0026rsquo;（finally）模块。并且面向JDBC的异常与Spring通用数据访问对象（Data Access Object）异常等级相一致。 以资源容器，DAO实现和事务策略等形式与 Hibernate，JDO 和 MyBatis、SQL Maps 集成。利用众多的翻转控制方便特性来全面支持，解决了许多典型的 Hibernate 集成问题。所有这些全部遵从 Spring 通用事务处理和通用数据访问对象异常等级规范。 灵活的基于核心 Spring 功能的 MVC 网页应用程序框架。开发者通过策略接口将拥有对该框架的高度控制，因而该框架将适应于多种呈现（View）技术，例如 JSP、FreeMarker、Velocity、Thymeleaf 等。值得注意的是，Spring 中间层可以轻易地结合于任何基于 MVC 框架的网页层，例如 Struts、WebWork 或 Tapestry。 提供诸如事务管理等服务的AOP框架。  "});index.add({'id':50,'href':'/interview/docs/basic/database/sql/','title':"SQL",'content':"SQL语句 CRUD CREATE TABLE CREATE TABLE `user` ( `id` INT AUTO_INCREMENT, `name` VARCHAR (20), PRIMARY KEY (`id`) );  VARCHAR记得指定长度。\n UPDATE UPDATE 表名称 SET 列名称 = 新值 WHERE 列名称 = 某值 INSERT INSERT INTO 表名称 VALUES (值1, 值2,....) INSERT INTO table_name (列1, 列2,...) VALUES (值1, 值2,....) DELETE DELETE FROM 表名称 WHERE 列名称 = 值 修改表结构 ALTER TABLE table_name add column_name datatype ALTER TABLE table_name drop COLUMN column_name ALTER TABLE table_name modify COLUMN column_name datatype MySQL SQL 查询语句执行顺序  (7) - SELECT (8) - DISTINCT \u0026lt;select_list\u0026gt; (1) - FROM \u0026lt;left_table\u0026gt; (3) - \u0026lt;join_type\u0026gt; JOIN \u0026lt;right_table\u0026gt; (2) - ON \u0026lt;join_condition\u0026gt; (4) - WHERE \u0026lt;where_condition\u0026gt; (5) - GROUP BY \u0026lt;group_by_list\u0026gt; (6) - HAVING \u0026lt;having_condition\u0026gt; (9) - ORDER BY \u0026lt;order_by_condition\u0026gt; (10 - LIMIT \u0026lt;limit_number\u0026gt;  关于 SQL 语句的执行顺序，有三个值得我们注意的地方：\n FROM 才是 SQL 语句执行的第一步，并非 SELECT。 数据库在执行 SQL 语句的第一步是将数据从硬盘加载到数据缓冲区中，以便对这些数据进行操作。 SELECT 是在大部分语句执行了之后才执行的，严格的说是在 FROM 和 GROUP BY 之后执行的。理解这一点是非常重要的，这就是你不能在 WHERE 中使用在 SELECT 中设定别名的字段作为判断条件的原因。 无论在语法上还是在执行顺序上， UNION 总是排在在 ORDER BY 之前。很多人认为每个 UNION 段都能使用 ORDER BY 排序，但是根据 SQL 语言标准和各个数据库 SQL 的执行差异来看，这并不是真的。尽管某些数据库允许 SQL 语句对子查询（subqueries）或者派生表（derived tables）进行排序，但是这并不说明这个排序在 UNION 操作过后仍保持排序后的顺序。  虽然SQL的逻辑查询是根据上述进行查询，但是数据库也许并不会完全按照逻辑查询处理的方式来进行查询。MYSQL数据库有两个组件 Parser（分析SQL语句）和 Optimizer（优化）。\n从官方手册上看，可以理解为， MySQL 采用了基于开销的优化器，以确定处理查询的最解方式，也就是说执行查询之前，都会先选择一条自以为最优的方案，然后执行这个方案来获取结果。在很多情况下， MySQL 能够计算最佳的可能查询计划，但在某些情况下， MySQL 没有关于数据的足够信息，或者是提供太多的相关数据信息，估测就不那么友好了。\n存在索引的情况下，优化器优先使用条件用到索引且最优的方案。当 sql 条件有多个索引可以选择，mysql 优化器将直接使用效率最高的索引执行。\n子查询 子查询按使用场合分：\n 作为主查询的结果数据：select c1,(select f1 from tab2) as f11 from tab1; #这里子查询应该只有一个数据（一行一列，标量子查询） 作为主查询的条件数据：select c1 from tab1 where c1 in (select f1 from tab2); #这里子查询可以是多个数据（多行一列，列子查询，以及标量子查询，实际上行子查询也可能，但极少） 作为主查询的来源数据：select c1 from (select f1 as c1, f2 from tab2) as t2; #这里子查询可以是任意查询结果（表子查询）。  权限分配 grant select,insert on userdb.userinfo to\u0026#39;zhangsan\u0026#39;@\u0026#39;localhost\u0026#39; 模糊查询 %：表示任意0个或多个字符。可匹配任意类型和长度的字符，有些情况下若是中文，请使用两个百分号（%%）表示。\nselect * from test where text like \u0026#39;%1%\u0026#39;; _ ： 表示任意单个字符。匹配单个任意字符，它常用来限制表达式的字符长度语句。\n--倒数第三个字符为 1 ，且最小长度为 5 select * from test where text like \u0026#39;__%1__\u0026#39;; "});index.add({'id':51,'href':'/interview/docs/java/jvm/string-constant-pool/','title':"String 常量池",'content':"String 常量池 在 JAVA 语言中有 8 中基本类型和一种比较特殊的类型 String 。这些类型为了使他们在运行过程中速度更快，更节省内存，都提供了一种常量池的概念。常量池就类似一个 JAVA 系统级别提供的缓存。\nString 类型的常量池比较特殊。它的主要使用方法有两种：\n 直接使用双引号声明出来的 String 对象会直接存储在常量池中 如果不是用双引号声明的 String 对象，可以使用 String 提供的 intern 方法。 intern 方法会从字符串常量池中查询当前字符串是否存在，若不存在就会将当前字符串放入常量池中  intern  /** * Returns a canonical representation for the string object. * \u0026lt;p\u0026gt; * A pool of strings, initially empty, is maintained privately by the * class {@code String}. * \u0026lt;p\u0026gt; * When the intern method is invoked, if the pool already contains a * string equal to this {@code String} object as determined by * the {@link #equals(Object)} method, then the string from the pool is * returned. Otherwise, this {@code String} object is added to the * pool and a reference to this {@code String} object is returned. * \u0026lt;p\u0026gt; * It follows that for any two strings {@code s} and {@code t}, * {@code s.intern() == t.intern()} is {@code true} * if and only if {@code s.equals(t)} is {@code true}. * \u0026lt;p\u0026gt; * All literal strings and string-valued constant expressions are * interned. String literals are defined in section 3.10.5 of the * \u0026lt;cite\u0026gt;The Java\u0026amp;trade; Language Specification\u0026lt;/cite\u0026gt;. * * @return a string that has the same contents as this string, but is * guaranteed to be from a pool of unique strings. */ public native String intern(); JAVA 使用 jni 调用 c++ 实现的 StringTable 的 intern 方法, StringTable 跟 Java 中的 HashMap 的实现是差不多的, 只是 不能自动扩容。默认大小是 1009 。\n要注意的是， String 的 String Pool 是一个固定大小的 Hashtable ，默认值大小长度是 1009 ，如果放进 String Pool 的 String 非常多，就会造成 Hash 冲突严重，从而导致链表会很长，而链表长了后直接会造成的影响就是当调用 String.intern 时性能会大幅下降。\n在 JDK6 中 StringTable 是固定的，就是 1009 的长度，所以如果常量池中的字符串过多就会导致效率下降很快。在 jdk7 中， StringTable 的长度可以通过一个参数指定：\n-XX:StringTableSize=99991  在 JDK6 以及以前的版本中，字符串的常量池是放在堆的 Perm 区。在 JDK7 的版本中，字符串常量池已经从 Perm 区移到正常的 Java Heap 区域\n public static void main(String[] args) { String s = new String(\u0026quot;1\u0026quot;); s.intern(); String s2 = \u0026quot;1\u0026quot;; System.out.println(s == s2); String s3 = new String(\u0026quot;1\u0026quot;) + new String(\u0026quot;1\u0026quot;); s3.intern(); String s4 = \u0026quot;11\u0026quot;; System.out.println(s3 == s4); } 上述代码的执行结果：\n JDK6: false false JDK7: false true  public static void main(String[] args) { String s = new String(\u0026quot;1\u0026quot;); String s2 = \u0026quot;1\u0026quot;; s.intern(); System.out.println(s == s2); String s3 = new String(\u0026quot;1\u0026quot;) + new String(\u0026quot;1\u0026quot;); String s4 = \u0026quot;11\u0026quot;; s3.intern(); System.out.println(s3 == s4); } 上述代码的执行结果：\n JDK6: false false JDK7: false false  由于 JDK7 将字符串常量池移动到 Heap 中，导致上述版本差异，下面具体来分析下。\nJDK6  图中绿色线条代表 string 对象的内容指向，黑色线条代表地址指向\n 在 jdk6 中上述的所有打印都是 false ，因为 jdk6 中的常量池是放在 Perm 区中的， Perm 区和正常的 JAVA Heap 区域是完全分开的。上面说过如果是使用引号声明的字符串都是会直接在字符串常量池中生成，而 new 出来的 String 对象是放在 JAVA Heap 区域。所以拿一个 JAVA Heap 区域的对象地址和字符串常量池的对象地址进行比较肯定是不相同的，即使调用 String.intern 方法也是没有任何关系的。\nJDK7 因为字符串常量池移动到 JAVA Heap 区域后，再来解释为什么会有上述的打印结果。\n 在第一段代码中，先看 s3 和 s4 字符串。String s3 = new String(\u0026quot;1\u0026quot;) + new String(\u0026quot;1\u0026quot;);，这句代码中现在生成了 2个 最终对象，是字符串常量池中的 “1” 和 JAVA Heap 中的 s3 引用指向的对象。中间还有 2个 匿名的 new String(\u0026quot;1\u0026quot;) 我们不去讨论它们。此时 s3 引用对象内容是 ”11” ，但此时常量池中是没有 “11” 对象的。 接下来 s3.intern(); 这一句代码，是将 s3 中的 “11” 字符串放入 String 常量池中，因为此时常量池中不存在 “11” 字符串，因此常规做法是跟 jdk6 图中表示的那样，在常量池中生成一个 “11” 的对象，关键点是 jdk7 中常量池不在 Perm 区域了，这块做了调整。常量池中不需要再存储一份对象，可以直接存储堆中的引用。这份引用指向 s3 引用的对象。 也就是说引用地址是相同的。 最后 String s4 = \u0026quot;11\u0026quot;; 这句代码中 ”11” 是显示声明的，因此会直接去常量池中创建，创建的时候发现已经有这个对象了，此时也就是指向 s3 引用对象的一个引用。所以 s4 引用就指向和 s3 一样了。因此最后的比较 s3 == s4 是 true 。 再看 s 和 s2 对象。 String s = new String(\u0026quot;1\u0026quot;); 第一句代码，生成了2个对象。常量池中的 “1” 和 JAVA Heap 中的字符串对象。s.intern(); 这一句是 s 对象去常量池中寻找后发现 “1” 已经在常量池里了。 接下来 String s2 = \u0026quot;1\u0026quot;; 这句代码是生成一个 s2 的引用指向常量池中的 “1” 对象。 结果就是 s 和 s2 的引用地址明显不同。  接下来是第二段代码：\n 第一段代码和第二段代码的改变就是 s3.intern(); 的顺序是放在 String s4 = \u0026quot;11\u0026quot;; 后了。这样，首先执行 String s4 = \u0026quot;11\u0026quot;; 声明 s4 的时候常量池中是不存在 “11” 对象的，执行完毕后， “11“ 对象是 s4 声明产生的新对象。然后再执行 s3.intern(); 时，常量池中 “11” 对象已经存在了，因此 s3 和 s4 的引用是不同的。 第二段代码中的 s 和 s2 代码中，s.intern();，这一句往后放也不会有什么影响了，因为对象池中在执行第一句代码String s = new String(\u0026quot;1\u0026quot;); 的时候已经生成 “1” 对象了。下边的 s2 声明都是直接从常量池中取地址引用的。 s 和 s2 的引用地址是不会相等的。  小结 从上述的例子代码可以看出 jdk7 版本对 intern 操作和常量池都做了一定的修改。主要包括2点：\n 将 String 常量池 从 Perm 区移动到了 Java Heap 区 String#intern 方法时，如果存在堆中的对象，会直接保存对象的引用，而不会重新创建对象。  使用范例 static final int MAX = 1000 * 10000; static final String[] arr = new String[MAX]; public static void main(String[] args) throws Exception { Integer[] DB_DATA = new Integer[10]; Random random = new Random(10 * 10000); for (int i = 0; i \u0026lt; DB_DATA.length; i++) { DB_DATA[i] = random.nextInt(); } long t = System.currentTimeMillis(); for (int i = 0; i \u0026lt; MAX; i++) { //arr[i] = new String(String.valueOf(DB_DATA[i % DB_DATA.length])); arr[i] = new String(String.valueOf(DB_DATA[i % DB_DATA.length])).intern(); } System.out.println((System.currentTimeMillis() - t) + \u0026quot;ms\u0026quot;); System.gc(); } 运行的参数是：-Xmx2g -Xms2g -Xmn1500M 上述代码是一个演示代码，其中有两条语句不一样，一条是使用 intern，一条是未使用 intern。\n通过上述结果，我们发现不使用 intern 的代码生成了 1000w 个字符串，占用了大约 640m 空间。 使用了 intern 的代码生成了 1345 个字符串，占用总空间 133k 左右。其实通过观察程序中只是用到了 10 个字符串，所以准确计算后应该是正好相差 100w 倍。虽然例子有些极端，但确实能准确反应出 intern 使用后产生的巨大空间节省。\n细心的同学会发现使用了 intern 方法后时间上有了一些增长。这是因为程序中每次都是用了 new String 后，然后又进行 intern 操作的耗时时间，这一点如果在内存空间充足的情况下确实是无法避免的，但我们平时使用时，内存空间肯定不是无限大的，不使用 intern 占用空间导致 jvm 垃圾回收的时间是要远远大于这点时间的。 毕竟这里使用了 1000w 次 intern 才多出来1秒钟多的时间。\n不当使用 fastjson 中对所有的 json 的 key 使用了 intern 方法，缓存到了字符串常量池中，这样每次读取的时候就会非常快，大大减少时间和空间。而且 json 的 key 通常都是不变的。这个地方没有考虑到大量的 json key 如果是变化的，那就会给字符串常量池带来很大的负担。\n这个问题 fastjson 在1.1.24版本中已经将这个漏洞修复了。程序加入了一个最大的缓存大小，超过这个大小后就不会再往字符串常量池中放了。\n参考文档 深入解析String#intern\n"});index.add({'id':52,'href':'/interview/docs/java/string-builder/','title':"StringBuilder",'content':"StringBuilder StringBuilder类也封装了一个字符数组，定义如下：\n char[] value; 与String不同，它不是final的，可以修改。另外，与String不同，字符数组中不一定所有位置都已经被使用，它有一个实例变量，表示数组中已经使用的字符个数，定义如下：\n int count; StringBuilder继承自AbstractStringBuilder，它的默认构造方法是：\n public StringBuilder() { super(16); } 调用父类的构造方法，父类对应的构造方法是：\n AbstractStringBuilder(int capacity) { value = new char[capacity]; } 也就是说，new StringBuilder()这句代码，内部会创建一个长度为16的字符数组，count的默认值为0。\nappend的实现  public AbstractStringBuilder append(String str) { if (str == null) str = \u0026quot;null\u0026quot;; int len = str.length(); ensureCapacityInternal(count + len); str.getChars(0, len, value, count); count += len; return this; } append会直接拷贝字符到内部的字符数组中，如果字符数组长度不够，会进行扩展，实际使用的长度用count体现。具体来说，ensureCapacityInternal(count+len)会确保数组的长度足以容纳新添加的字符，str.getChars会拷贝新添加的字符到字符数组中，count+=len会增加实际使用的长度。\nensureCapacityInternal的代码如下：\n private void ensureCapacityInternal(int minimumCapacity) { if (minimumCapacity - value.length \u0026gt; 0) expandCapacity(minimumCapacity); } 如果字符数组的长度小于需要的长度，则调用expandCapacity进行扩展，expandCapacity的代码是：\n void expandCapacity(int minimumCapacity) { int newCapacity = value.length * 2 + 2; if (newCapacity - minimumCapacity \u0026lt; 0) newCapacity = minimumCapacity; if (newCapacity \u0026lt; 0) { if (minimumCapacity \u0026lt; 0) throw new OutOfMemoryError(); newCapacity = Integer.MAX_VALUE; } value = Arrays.copyOf(value, newCapacity); } 扩展的逻辑是，分配一个足够长度的新数组，然后将原内容拷贝到这个新数组中，最后让内部的字符数组指向这个新数组，这个逻辑主要靠下面这句代码实现：\n value = Arrays.copyOf(value, newCapacity); toString实现 字符串构建完后，我们来看toString代码：\n public String toString() { return new String(value, 0, count); } "});index.add({'id':53,'href':'/interview/docs/java/concurrent/synchronized/','title':"Synchronized",'content':"Synchronized原理 在多线程并发编程中Synchronized一直是元老级角色，很多人都会称呼它为重量级锁，但是随着Java SE1.6对Synchronized进行了各种优化之后，有些情况下它并不那么重了，本文详细介绍了Java SE1.6中为了减少获得锁和释放锁带来的性能消耗而引入的偏向锁和轻量级锁，以及锁的存储结构和升级过程。\nCAS(Compare and Swap)，用于在硬件层面上提供原子性操作。在 Intel 处理器中，比较并交换通过指令cmpxchg实现。比较是否和给定的数值一致，如果一致则修改，不一致则不修改。\n基础 Java中的每一个对象都可以作为锁。\n 对于同步方法，锁是当前实例对象。 对于静态同步方法，锁是当前对象的Class对象。 对于同步方法块，锁是Synchonized括号里配置的对象。  当一个线程试图访问同步代码块时，它首先必须得到锁，退出或抛出异常时必须释放锁。那么锁存在哪里呢？锁里面会存储什么信息呢？\n同步的原理 JVM规范规定JVM基于进入和退出 Monitor 对象来实现方法同步和代码块同步，但两者的实现细节不一样。代码块同步是使用monitorenter和monitorexit指令实现，而方法同步是使用另外一种方式实现的，细节在JVM规范里并没有详细说明，但是方法的同步同样可以使用这两个指令来实现。\nmonitorenter指令是在编译后插入到同步代码块的开始位置，而monitorexit是插入到方法结束处和异常处，JVM要保证每个monitorenter必须有对应的monitorexit与之配对。任何对象都有一个monitor与之关联，当且一个monitor被持有后，它将处于锁定状态。线程执行到 monitorenter 指令时，将会尝试获取对象所对应的 monitor 的所有权，即尝试获得对象的锁。\nJava对象头 锁存在Java对象头里。如果对象是数组类型，则虚拟机用3个Word（字宽）存储对象头，如果对象是非数组类型，则用2字宽存储对象头。在32位虚拟机中，一字宽等于四字节，即32bit。\n   长度 内容 说明     32/64bit Mark Word 存储对象的hashCode或锁信息等   32/64bit Class Metadata Address 存储到对象类型数据的指针   32/64bit Array length 数组的长度（如果当前对象是数组）    Java对象头里的Mark Word里默认存储对象的HashCode，分代年龄和锁标记位。32位JVM的Mark Word的默认存储结构如下：\n    25 bit 4bit 1bit是否是偏向锁 2bit锁标志位     无锁状态 对象的hashCode 对象分代年龄 0 01    在运行期间Mark Word里存储的数据会随着锁标志位的变化而变化。Mark Word可能变化为存储以下4种数据：\n锁的升级 Java SE1.6为了减少获得锁和释放锁所带来的性能消耗，引入了“偏向锁”和“轻量级锁”，所以在Java SE1.6里锁一共有四种状态，无锁状态，偏向锁状态，轻量级锁状态和重量级锁状态，它会随着竞争情况逐渐升级。锁可以升级但不能降级，意味着偏向锁升级成轻量级锁后不能降级成偏向锁，但是偏向锁状态可以被重置为无锁状态（锁撤销）。这种锁升级却不能降级的策略，目的是为了提高获得锁和释放锁的效率。\n   锁状态 优点 缺点 适用场景     偏向锁 加锁、解锁无额外消耗，和非同步方式近似 如果竞争线程多，会有额外锁撤销的消耗 基本没有线程竞争的场景   轻量级锁 竞争线程不会阻塞，使用自旋等待 如果长时间不能获取锁，会消耗CPU 少量线程竞争，且线程持有锁时间不长   重量级锁 竞争线程被阻塞，减少CPU空转 线程阻塞，响应时间长 很多线程竞争，锁持有时间长    锁粗化 同步块的作用范围应该尽可能小，仅在共享数据的实际作用域中才进行同步，这样做的目的是为了使需要同步的操作数量尽可能缩小，缩短阻塞时间，如果存在锁竞争，那么等待锁的线程也能尽快拿到锁。 但是加锁解锁也需要消耗资源，如果存在一系列的连续加锁解锁操作，可能会导致不必要的性能损耗。 锁粗化就是JVM将多个连续的加锁、解锁操作连接在一起，扩展成一个范围更大的锁，避免频繁的加锁解锁操作。\n锁消除 Java虚拟机在JIT编译时(可以简单理解为当某段代码即将第一次被执行时进行编译，又称即时编译)，通过对运行上下文的扫描，经过逃逸分析，去除不可能存在共享资源竞争的锁，通过这种方式消除没有必要的锁，可以节省毫无意义的请求锁时间\n线程对锁的竞争 当多个线程同时请求某个对象监视器时，对象监视器会设置几种状态用来区分请求的线程：\n Contention List：所有请求锁的线程将被首先放置到该竞争队列 Entry List：Contention List中那些有资格成为候选人的线程被移到Entry List Wait Set：那些调用wait方法被阻塞的线程被放置到Wait Set OnDeck：任何时刻最多只能有一个线程正在竞争锁，该线程称为OnDeck Owner：获得锁的线程称为Owner !Owner：释放锁的线程  当多个线程同时访问一段同步代码时，首先会进入 _EntryList 集合，当线程获取到对象的 monitor 后进入 _Owner 区域并把 monitor 中的 owner 变量设置为当前线程同时 monitor 中的计数器 count 加 1，若线程调用 wait() 方法，将释放当前持有的 monitor ， owner 变量恢复为 null ， count 自减1，同时该线程进入 WaitSet 集合中等待被唤醒。若当前线程执行完毕也将释放 monitor(锁)并复位变量的值，以便其他线程进入获取 monitor(锁)。\nContentionList ContentionList并不是一个真正的Queue，而只是一个虚拟队列，原因在于ContentionList是由 Node 及其 next 指针逻辑构成，并不存在一个 Queue 的数据结构。ContentionList是一个后进先出（LIFO）的队列，每次新加入 Node 时都会在队头进行，通过 CAS 改变第一个节点的的指针为新增节点，同时设置新增节点的 next 指向后续节点，而取得操作则发生在队尾。显然，该结构其实是个Lock-Free的队列。\n因为只有Owner线程才能从队尾取元素，也即线程出列操作无争用，当然也就避免了CAS的ABA问题。\nEntryList EntryList与ContentionList逻辑上同属等待队列，ContentionList会被线程并发访问，为了降低对ContentionList队尾的争用，而建立EntryList。Owner线程在unlock时会从ContentionList中迁移线程到EntryList，并会指定EntryList中的某个线程（一般为Head）为Ready（OnDeck）线程。Owner线程并不是把锁传递给OnDeck线程，只是把竞争锁的权利交给OnDeck，OnDeck线程需要重新竞争锁。这样做虽然牺牲了一定的公平性，但极大的提高了整体吞吐量，在Hotspot中把OnDeck的选择行为称之为“竞争切换”。\nOnDeck线程获得锁后即变为owner线程，无法获得锁则会依然留在EntryList中，考虑到公平性，在EntryList中的位置不发生变化（依然在队头）。如果Owner线程被wait方法阻塞，则转移到WaitSet队列；如果在某个时刻被notify/notifyAll唤醒，则再次转移到EntryList。\n"});index.add({'id':54,'href':'/interview/docs/basic/net/tcp/','title':"TCP",'content':"TCP TCP概述 TCP的特点  TCP是面向连接的传输层协议。 TCP连接是点对点的（套接字\u0026ndash;IP:Port到套接字）。 TCP提供可靠交付的服务。 TCP提供全双工通信。 面向字节流。  TCP与UDP的区别。 ||TCP |UDP | |\u0026mdash;| |是否连接| 面向连接 |面向非连接| |传输可靠性| 可靠| 不可靠| |应用场合| 传输大量数据| 少量数据| |速度| 慢| 快|\n基本概念：   发送缓存和接受缓存：用来临时保存双向通信的数据。在发送时，应用程序将数据传送给TCP发送缓存后，就可以做自己的事情，TCP在合适的时候发送数据；在接受数据时，TCP把发送的数据放入缓存，上层应用在合适的时候读取缓存即可。\n  滑动窗口：TCP的滑动窗口以字节为单位，用3个指针进行表示。当窗口内连续报文段被确认收到后，可以将窗口向前滑动。窗口大小应小于等于缓存区的大小。\n  滑动窗口协议：只有在接收窗口向前滑动时（与此同时也发送了确认），发送窗口才有可能向前滑动。收发两端的窗口按照以上规律不断地向前滑动，因此这种协议又称为滑动窗口协议。\n   当发送窗口和接收窗口的大小都等于 1时，就是停止等待协议。\n  当发送窗口大于1，接收窗口等于1时，就是回退N步协议。\n  当发送窗口和接收窗口的大小均大于1时，就是选择重发协议。\n TCP报文结构。  源端口、目的端口：16位长。标识出远端和本地的端口号。 序列号：32位长。表明了发送的数据报的顺序，不一定从0开始。 确认号：32位长。希望收到的下一个数据报的序列号，表明到序列号N-1为止的所有数据已经正确收到。 TCP协议数据报头长：4位长。表明TCP头中包含多少个32位字。 接下来的6位未用。 ACK：ACK位置1表明确认号是合法的。如果ACK为0，那么数据报不包含确认信息，确认字段被省略。 PSH：表示是带有PUSH标志的数据。接收方因此请求数据报一到便可送往应用程序而不必等到缓冲区装满时才传送。 RST：用于复位由于主机崩溃或其它原因而出现的错误的连接。还可以用于拒绝非法的数据报或拒绝连接请求。 SYN：用于建立连接。 FIN：用于释放连接。 窗口大小：16位长。窗口大小字段表示在确认了字节之后还可以发送多少个字节。 校验和：16位长。是为了确保高可靠性而设置的。它校验头部、数据和伪TCP头部之和。 紧急指针：URG=1时才有意义。 可选项：长度可变，最长40个字节。  MMS SACK：选择确认。 时间戳：计算往返时间；用于处理TCP序号超过2^32的情况，又称为防止序号回绕（PAWS）。     TCP最小长度为20个字节。\n 三次握手  第一次握手：建立连接时，客户端发送syn包（syn=j）到服务器，并进入SYN_SENT状态，等待服务器确认；SYN：同步序列编号（Synchronize Sequence Numbers）。 第二次握手：服务器收到syn包，必须确认客户的SYN（ack=j+1），同时自己也发送一个SYN包（syn=k），即SYN+ACK包，此时服务器进入SYN_RECV状态； 第三次握手：客户端收到服务器的SYN+ACK包，向服务器发送确认包ACK(ack=k+1），此包发送完毕，客户端和服务器进入ESTABLISHED（TCP连接成功）状态，完成三次握手。  内核对 TCP 的处理 Socket 是一个由 （源IP、源Port、目标IP、目标Port、协议） 组成的五元组，唯一标示一个 socket 连接。\nTCP 建立连接的整体流程：\n 服务器端在调用 listen 之后，内核会建立两个队列，SYN队列和ACCEPT队列，其中ACCPET队列的长度由backlog指定。 服务器端在调用 accpet 之后，将阻塞，等待 ACCPT 队列有元素。 客户端在调用 connect 之后，将开始发起 SYN 请求，请求与服务器建立连接，此时称为第一次握手。 服务器端在接受到 SYN 请求之后，把请求方放入 SYN 队列中，并给客户端回复一个确认帧 ACK ，此帧还会携带一个请求与客户端建立连接的请求标志，也就是 SYN ，这称为第二次握手 客户端收到 SYN+ACK 帧后， connect 返回，并发送确认建立连接帧 ACK 给服务器端。这称为第三次握手 服务器端收到 ACK 帧后，会把请求方从 SYN 队列中移出，放至 ACCEPT 队列中，而 accept 函数也等到了自己的资源，从阻塞中唤醒，从 ACCEPT 队列中取出请求方，重新建立一个新的 sockfd ，并返回。  在服务端如何分发多个连接的请求？\n由于 TCP/IP 协议栈是维护着一个接收和发送缓冲区的。在接收到来自客户端的数据包后，服务器端的 TCP/IP 协议栈应该会做如下处理：\n 如果收到的是请求连接的数据包，则传给监听着连接请求端口的 socetfd 套接字。 如果是已经建立过连接后的客户端数据包，则将数据放入接收缓冲区。这样，当服务器端需要读取指定客户端的数据时，则可以利用 socketfd_new 套接字通过 recv 或者 read 函数到缓冲区里面去取指定的数据（因为 socketfd_new 代表的 socket 对象记录了客户端IP和端口，因此可以鉴别）。  数据包如何找到相对应的 socket ，这个方法在 Linux Kernel 代码里也是有体现的：\nstatic inline struct sock *__inet_lookup(struct net *net, struct inet_hashinfo *hashinfo, const __be32 saddr, const __be16 sport, const __be32 daddr, const __be16 dport, const int dif) { u16 hnum = ntohs(dport); /* 先尝试查找处于连接成功的 socket */ struct sock *sk = __inet_lookup_established(net, hashinfo, saddr, sport, daddr, hnum, dif); /* 如果没有找到连接成功的socket，那么就去处于 listen 状态的 socket 查找 */ return sk ? : __inet_lookup_listener(net, hashinfo, daddr, hnum, dif); } 四次挥手  在Time_Wait阶段，主动端等待2*MSL时间，MSL建议为2分钟。\n 由于TCP连接是全双工的，因此每个方向都必须单独进行关闭。\n 客户端A发送一个FIN，用来关闭客户A到服务器B的数据传送（报文段4）。 服务器B收到这个FIN，它发回一个ACK，确认序号为收到的序号加1（报文段5）。和SYN一样，一个FIN将占用一个序号。 服务器B关闭与客户端A的连接，发送一个FIN给客户端A（报文段6）。 客户端A发回ACK报文确认，并将确认序号设置为收到序号加1（报文段7）   TCP采用四次挥手关闭连接如图所示为什么建立连接协议是三次握手，而关闭连接却是四次握手呢？\n  这是因为服务端的LISTEN状态下的SOCKET当收到SYN报文的建连请求后，它可以把ACK和SYN（ACK起应答作用，而SYN起同步作用）放在一个报文里来发送。但关闭连接时，当收到对方的FIN报文通知时，它仅仅表示对方没有数据发送给你了；但未必你所有的数据都全部发送给对方了，所以你可以未必会马上会关闭SOCKET,也即你可能还需要发送一些数据给对方之后，再发送FIN报文给对方来表示你同意现在可以关闭连接了，所以它这里的ACK报文和FIN报文多数情况下都是分开发送的。\n ARQ协议 ARQ协议（自动重传请求）是OSI模型中数据链路层和传输层的错误纠正协议之一。它通过使用确认和超时这两个机制，在不可靠服务的基础上实现可靠的信息传输。\n停止等待ARQ  发送点对接收点发送数据包，然后等待接收点回复ACK并且开始计时。 在等待过程中，发送点停止发送新的数据包。 当数据包没有成功被接收点接收时候，接收点不会发送ACK.这样发送点在等待一定时间后，重新发送数据包。 反复以上步骤直到收到从接收点发送的ACK。   这个协议的缺点是较长的等待时间导致低的数据传输速度。在低速传输时，对连接频道的利用率比较好，但是在高速传输时，频道的利用率会显著下降。\n 连续ARQ协议（累积确认） 为了克服停止并等待ARQ协议长时间等待ACK的缺点。这个协议会连续发送一组数据包，然后再等待这些数据包的ACK。\n 在连续ARQ协议中涉及到滑动窗口协议，这是TCP协议的精髓所在。\n 回退N重传  接收点丢弃从第一个没有收到的数据包开始的所有数据包。 发送点收到NACK后，从NACK中指明的数据包开始重新发送。  选择重传（SACK）  发送点连续发送数据包但对每个数据包都设有个一个计时器。 当在一定时间内没有收到某个数据包的ACK时，发送点只重新发送那个没有ACK的数据包。   相对于回退N重传来说，选择重传可以减少重传的数据。\n TCP流量控制 流量控制指点对点通信量的控制，是端到端正的问题。流量控制所要做的就是抑制发送端发送数据的速率，以便使接收端来得及接收。这里是通过滑动窗口机制来实现的。发送方的发送窗口不能超过接收方的接收窗口。TCP的窗口单位是字节，不是报文段。\n这上图中B一共进行了三次流量控制：第一次将窗口减小到300，第二次减小到100，最后减小到0，这时发送方暂停发送知道B发送一个新的窗口值为止。\n 如果B发送了一个新的窗口值到A，但是A并没有收到，就会造成死锁。为解决这个问题，TCP为每个链接设置有一个持续计时器。只要TCP收到一个0窗口，就启动计时器。若计时器设置的时间到了，就发送一个探测报文，而接收方在确认的时候会给出一个现在的窗口值。\n TCP拥塞控制 防止过多的数据注入到网络中，这样可以使网络中的路由器或链路不致过载。拥塞控制所要做的都有一个前提：网络能够承受现有的网络负荷。拥塞控制是一个全局性的过程，涉及到所有的主机、路由器，以及与降低网络传输性能有关的所有因素。\n慢开始和拥塞避免 发送方维持一个拥塞窗口cwnd的状态变量。发送方让自己的发送窗口小于等于拥塞窗口。\n  慢开始：由小到大的逐渐增大拥塞窗口。首先将cwnd设置为一个最大报文段MMS，在收到一个对新的报文段的确认后，把拥塞窗口增加一个MMS。——指数增长\n  拥塞避免：当慢开始到门限值（ssthresh）后，使用拥塞避免算法（cwnd每次加1）。当发现网络拥塞后，将cwnd置为1，ssthresh减半，再次执行慢开始。\n  快重传和快恢复   快重传：当接收方收到一个失序报文段后就立即发送重复确认而不要等到自己发送数据时捎带确认。当发送方连续收到三个重复确认时，应立即重传接收方尚未收到的报文段。\n  快恢复：与快重传结合使用。\n  在连续收到三个重复确认时，将慢开始的ssthresh减半，这是为了防止网络拥塞（ ** 接下来并不执行慢开始 ** ）。\n  由于发送方现在认为 网络很可能没有拥塞，于是接下来不执行慢开始，而是将cwnd值设置为ssthresh减半后的值，然后执行拥塞避免。\n    "});index.add({'id':55,'href':'/interview/docs/java/concurrent/threadlocal/','title':"ThreadLocal",'content':"Threadlocal原理 ThreadLocal 为解决多线程程序的并发问题提供了一种新的思路。使用这个工具类可以很简洁地编写出优美的多线程程序。当使用 ThreadLocal 维护变量时，ThreadLocal 为每个使用该变量的线程提供独立的变量副本，所以每一个线程都可以独立地改变自己的副本，而不会影响其它线程所对应的副本。\n每个线程中都保有一个ThreadLocalMap的成员变量，ThreadLocalMap 内部采用WeakReference数组保存，数组的key即为ThreadLocal 内部的Hash值。\n内存泄漏 ThreadLocalMap 使用 ThreadLocal 的弱引用作为 key ，如果一个 ThreadLocal 没有外部强引用来引用它，那么系统 GC 的时候，这个 ThreadLocal 势必会被回收，这样一来，ThreadLocalMap 中就会出现 key 为 null 的 Entry ，就没有办法访问这些 key 为 null 的 Entry 的 value，如果当前线程再迟迟不结束的话，这些 key 为 null 的 Entry 的 value 就会一直存在一条强引用链：Thread Ref -\u0026gt; Thread -\u0026gt; ThreaLocalMap -\u0026gt; Entry -\u0026gt; value 永远无法回收，造成内存泄漏。\nstatic class Entry extends WeakReference\u0026lt;ThreadLocal\u0026lt;?\u0026gt;\u0026gt; { /** The value associated with this ThreadLocal. */ Object value; Entry(ThreadLocal\u0026lt;?\u0026gt; k, Object v) { super(k); value = v; } } 其实，ThreadLocalMap 的设计中已经考虑到这种情况，也加上了一些防护措施：在 ThreadLocal 的 get(),set(),remove()的时候都会清除线程 ThreadLocalMap 里所有 key 为 null 的 value\n"});index.add({'id':56,'href':'/interview/docs/java/concurrent/volatile/','title':"Volatile",'content':"Volatile原理 计算机内存模型 计算机在执行程序时，每条指令都是在CPU中执行的，而执行指令过程中，势必涉及到数据的读取和写入。由于程序运行过程中的临时数据是存放在主存（物理内存）当中的，这时就存在一个问题，由于CPU执行速度很快，而从内存读取数据和向内存写入数据的过程跟CPU执行指令的速度比起来要慢的多，因此如果任何时候对数据的操作都要通过和内存的交互来进行，会大大降低指令执行的速度。因此在CPU里面就有了高速缓存。当程序在运行过程中，会将运算需要的数据从主存复制一份到CPU的高速缓存当中，那么CPU进行计算时就可以直接从它的高速缓存读取数据和向其中写入数据，当运算结束之后，再将高速缓存中的数据刷新到主存当中。举个简单的例子，比如下面的这段代码：\ni = i + 1;  当线程执行这个语句时，会先从主存当中读取i的值，然后复制一份到高速缓存当中，然后 CPU 执行指令对i进行加1操作，然后将数据写入高速缓存，最后将高速缓存中i最新的值刷新到主存当中。\n 这个代码在单线程中运行是没有任何问题的，但是在多线程中运行就会有问题了。在多核 CPU 中，每条线程可能运行于不同的 CPU 中，因此 每个线程运行时有自己的高速缓存（对单核CPU来说，其实也会出现这种问题，只不过是以线程调度的形式来分别执行的）。比如同时有两个线程执行这段代码，假如初始时i的值为0，那么我们希望两个线程执行完之后i的值变为2。但是事实会是这样吗？\n可能出现这种情况：初始时，两个线程分别读取i的值存入各自所在的 CPU 的高速缓存当中，然后 线程1 进行加1操作，然后把i的最新值1写入到内存。此时线程2的高速缓存当中i的值还是0，进行加1操作之后，i的值为1，然后线程2把i的值写入内存。最终结果i的值是1，而不是2。这就是著名的缓存一致性问题。通常称这种被多个线程访问的变量为共享变量。\n为了解决缓存不一致性问题，通常来说有以下两种解决方法：\n 通过在总线加LOCK#锁的方式 通过 缓存一致性协议   这两种方式都是硬件层面上提供的方式。\n 在早期的 CPU 当中，是通过在总线上加LOCK#锁的形式来解决缓存不一致的问题。因为 CPU 和其他部件进行通信都是通过总线来进行的，如果对总线加LOCK#锁的话，也就是说阻塞了其他 CPU 对其他部件访问（如内存），从而使得只能有一个 CPU 能使用这个变量的内存。比如上面例子中 如果一个线程在执行 i = i +1，如果在执行这段代码的过程中，在总线上发出了LCOK#锁的信号，那么只有等待这段代码完全执行完毕之后，其他CPU才能从变量i所在的内存读取变量，然后进行相应的操作。这样就解决了缓存不一致的问题。但是上面的方式会有一个问题，由于在锁住总线期间，其他CPU无法访问内存，导致效率低下。\n所以就出现了缓存一致性协议。最出名的就是 Intel 的MESI协议，MESI协议保证了每个缓存中使用的共享变量的副本是一致的。它核心的思想是：当CPU写数据时，如果发现操作的变量是共享变量，即在其他CPU中也存在该变量的副本，会发出信号通知其他CPU将该变量的缓存行置为无效状态，因此当其他CPU需要读取这个变量时，发现自己缓存中缓存该变量的缓存行是无效的，那么它就会从内存重新读取。\n##　Java内存模型\n在Java虚拟机规范中试图定义一种Java内存模型（Java Memory Model，JMM）来屏蔽各个硬件平台和操作系统的内存访问差异，以实现让Java程序在各种平台下都能达到一致的内存访问效果。那么Java内存模型规定了程序中变量的访问规则，往大一点说是定义了程序执行的次序。注意，为了获得较好的执行性能，Java内存模型并没有限制执行引擎使用处理器的寄存器或者高速缓存来提升指令执行速度，也没有限制编译器对指令进行重排序。也就是说，在java内存模型中，也会存在缓存一致性问题和指令重排序的问题。\nJava内存模型规定所有的变量都是存在主存当中（类似于前面说的物理内存），每个线程都有自己的工作内存（类似于前面的高速缓存）。线程对变量的所有操作都必须在工作内存中进行，而不能直接对主存进行操作。并且每个线程不能访问其他线程的工作内存。\n在Java中，执行下面这个语句：\ni = 10; 执行线程必须先在自己的工作线程中对变量i所在的缓存行进行赋值操作，然后再写入主存当中。而不是直接将数值10写入主存当中。那么Java语言本身对 原子性、可见性以及有序性提供了哪些保证呢？\n原子性  即一个操作或者多个操作 要么全部执行并且执行的过程不会被任何因素打断，要么就都不执行。\n 在Java中，对基本数据类型的变量的读取和赋值操作是原子性操作，即这些操作是不可被中断的，要么执行，要么不执行。上面一句话虽然看起来简单，但是理解起来并不是那么容易。看下面一个例子i：请分析以下哪些操作是原子性操作：\nx = 10; //语句1 y = x; //语句2 x++; //语句3 x = x + 1; //语句4 咋一看，有些朋友可能会说上面的4个语句中的操作都是原子性操作。其实只有语句1是原子性操作，其他三个语句都不是原子性操作。\n 语句1是直接将数值10赋值给x，也就是说线程执行这个语句的会直接将数值10写入到工作内存中。 语句2实际上包含2个操作，它先要去读取x的值，再将x的值写入工作内存，虽然读取x的值以及 将x的值写入工作内存 这2个操作都是原子性操作，但是合起来就不是原子性操作了。 同样的，x++和 x = x+1包括3个操作：读取x的值，进行加1操作，写入新的值。  也就是说，只有简单的读取、赋值（而且必须是将数字赋值给某个变量，变量之间的相互赋值不是原子操作）才是原子操作。不过这里有一点需要注意：在32位平台下，对64位数据的读取和赋值是需要通过两个操作来完成的，不能保证其原子性。但是好像在最新的JDK中，JVM已经保证对64位数据的读取和赋值也是原子性操作了。\n从上面可以看出，Java内存模型只保证了基本读取和赋值是原子性操作，如果要实现更大范围操作的原子性，可以通过synchronized和Lock来实现。由于synchronized和Lock能够保证任一时刻只有一个线程执行该代码块，那么自然就不存在原子性问题了，从而保证了原子性。\n可见性  可见性是指当多个线程访问同一个变量时，一个线程修改了这个变量的值，其他线程能够立即看得到修改的值。\n 对于可见性，Java提供了volatile关键字来保证可见性。当一个共享变量被volatile修饰时，它会保证修改的值会立即被更新到主存，当有其他线程需要读取时，它会去内存中读取新值。而普通的共享变量不能保证可见性，因为普通共享变量被修改之后，什么时候被写入主存是不确定的，当其他线程去读取时，此时内存中可能还是原来的旧值，因此无法保证可见性。\n另外，通过synchronized和Lock也能够保证可见性，synchronized和Lock能保证同一时刻只有一个线程获取锁然后执行同步代码，并且在释放锁之前会将对变量的修改刷新到主存当中。因此可以保证可见性。\n有序性  即程序执行的顺序按照代码的先后顺序执行。\n  指令重排序，一般来说，处理器为了提高程序运行效率，可能会对输入代码进行优化，它不保证程序中各个语句的执行先后顺序同代码中的顺序一致，但是它会保证程序最终执行结果和代码顺序执行的结果是一致的。\n 处理器在进行重排序时是会考虑指令之间的数据依赖性，如果一个指令Instruction 2必须用到Instruction 1的结果，那么处理器会保证Instruction 1会在Instruction 2之前执行。\n在Java内存模型中，允许编译器和处理器对指令进行重排序，但是重排序过程不会影响到单线程程序的执行，却会影响到多线程并发执行的正确性。\n在Java里面，可以通过volatile关键字来保证一定的“有序性”（具体原理在下一节讲述）。另外可以通过synchronized和Lock来保证有序性，很显然，synchronized和Lock保证每个时刻是有一个线程执行同步代码，相当于是让线程顺序执行同步代码，自然就保证了有序性。\n另外，Java内存模型具备一些先天的“有序性”，即不需要通过任何手段就能够得到保证的有序性，这个通常也称为 happens-before 原则，若线程 A 和线程 B 满足 happens-before 关系，则线程 A 执行操作的结果对线程 B 是可见的。如果两个操作的执行次序无法从happens-before原则推导出来，那么它们就不能保证它们的有序性，虚拟机可以随意地对它们进行重排序。\n下面就来具体介绍下happens-before原则（先行发生原则）：\n 程序次序规则：一个线程内，按照代码顺序，书写在前面的操作先行发生于书写在后面的操作 锁定规则：一个unLock操作先行发生于后面对同一个锁额lock操作 volatile变量规则：对一个变量的写操作先行发生于后面对这个变量的读操作 传递规则：如果操作A先行发生于操作B，而操作B又先行发生于操作C，则可以得出操作A先行发生于操作C 线程启动规则：Thread对象的start()方法先行发生于此线程的每个一个动作 线程中断规则：对线程interrupt()方法的调用先行发生于被中断线程的代码检测到中断事件的发生 线程终结规则：线程中所有的操作都先行发生于线程的终止检测，我们可以通过Thread.join()方法结束、Thread.isAlive()的返回值手段检测到线程已经终止执行 对象终结规则：一个对象的初始化完成先行发生于他的finalize()方法的开始  对于程序次序规则来说，我的理解就是一段程序代码的执行在单个线程中看起来是有序的。注意，虽然这条规则中提到“书写在前面的操作先行发生于书写在后面的操作”，这个应该是程序看起来执行的顺序是按照代码顺序执行的，因为虚拟机可能会对程序代码进行指令重排序。虽然进行重排序，但是最终执行的结果是与程序顺序执行的结果一致的，它只会对不存在数据依赖性的指令进行重排序。因此，在单个线程中，程序执行看起来是有序执行的，这一点要注意理解。事实上，这个规则是用来保证程序在单线程中执行结果的正确性，但无法保证程序在多线程中执行的正确性。\n第二条规则也比较容易理解，也就是说无论在单线程中还是多线程中，同一个锁如果出于被锁定的状态，那么必须先对锁进行了释放操作，后面才能继续进行lock操作。\n第三条规则是一条比较重要的规则，也是后文将要重点讲述的内容。直观地解释就是，如果一个线程先去写一个变量，然后一个线程去进行读取，那么写入操作肯定会先行发生于读操作。\n第四条规则实际上就是体现happens-before原则具备传递性。\n深入剖析Volatile关键字 Volatile 的语义 一旦一个共享变量（类的成员变量、类的静态成员变量）被volatile修饰之后，那么就具备了两层语义：\n 保证了不同线程对这个变量进行操作时的可见性，即一个线程修改了某个变量的值，这新值对其他线程来说是立即可见的 禁止进行指令重排序  先看一段代码，假如线程1先执行，线程2后执行：\n//线程1 boolean stop = false; while(!stop){ doSomething(); } //线程2 stop = true; 这段代码是很典型的一段代码，很多人在中断线程时可能都会采用这种标记办法。但是事实上，这段代码会完全运行正确么？即一定会将线程中断么？不一定，也许在大多数时候，这个代码能够把线程中断，但是也有可能会导致无法中断线程（虽然这个可能性很小，但是只要一旦发生这种情况就会造成死循环了）。\n下面解释一下这段代码为何有可能导致无法中断线程。在前面已经解释过，每个线程在运行过程中都有自己的工作内存，那么线程1在运行的时候，会将stop变量的值拷贝一份放在自己的工作内存当中。\n那么当线程2更改了stop变量的值之后，但是还没来得及写入主存当中，线程2转去做其他事情了，那么线程1由于不知道线程2对stop变量的更改，因此还会一直循环下去。但是用volatile修饰之后就变得不一样了：\n 使用volatile关键字会强制将修改的值立即写入主存； 使用volatile关键字的话，当线程2进行修改时，会导致线程1的工作内存中缓存变量stop的缓存行无效（反映到硬件层的话，就是CPU的L1或者L2缓存中对应的缓存行无效）； 由于线程1的工作内存中缓存变量stop的缓存行无效，所以线程1再次读取变量stop的值时会去主存读取。 那么在线程2修改stop值时（当然这里包括2个操作，修改线程2工作内存中的值，然后将修改后的值写入内存），会使得线程1的工作内存中缓存变量stop的缓存行无效，然后线程1读取时，发现自己的缓存行无效，它会等待缓存行对应的主存地址被更新之后，然后去对应的主存读取最新的值。  那么线程1读取到的就是最新的正确的值。\nVolatile与原子性 从上面知道volatile关键字保证了操作的可见性，但是volatile能保证对变量的操作是原子性吗？\n下面看一个例子：\npublic class Test { public volatile int inc = 0; public void increase() { inc++; } public static void main(String[] args) { final Test test = new Test(); for(int i=0;i\u0026lt;10;i++){ new Thread(){ public void run() { for(int j=0;j\u0026lt;1000;j++) test.increase(); }; }.start(); } while(Thread.activeCount()\u0026gt;1) //保证前面的线程都执行完 Thread.yield(); System.out.println(test.inc); } } 大家想一下这段程序的输出结果是多少？也许有些朋友认为是10000。但是事实上运行它会发现每次运行结果都不一致，都是一个小于10000的数字。可能有的朋友就会有疑问，不对啊，上面是对变量inc进行自增操作，由于volatile保证了可见性，那么在每个线程中对inc自增完之后，在其他线程中都能看到修改后的值啊，所以有10个线程分别进行了1000次操作，那么最终inc的值应该是1000*10=10000。\n这里面就有一个误区了，volatile关键字能保证可见性没有错，但是上面的程序错在没能保证原子性。可见性只能保证每次读取的是最新的值，但是volatile没办法保证对变量的操作的原子性。\n在前面已经提到过，自增操作是不具备原子性的，它包括读取变量的原始值、进行加1操作、写入工作内存。那么就是说自增操作的三个子操作可能会分割开执行，就有可能导致下面这种情况出现：\n假如某个时刻变量inc的值为10， 线程1对变量进行自增操作，线程1先读取了变量inc的原始值，然后线程1被阻塞了； 然后线程2对变量进行自增操作，线程2也去读取变量inc的原始值，由于线程1只是对变量inc进行读取操作，而没有对变量进行修改操作，所以不会导致线程2的工作内存中缓存变量inc的缓存行无效，所以线程2会直接去主存读取inc的值，发现inc的值时10，然后进行加1操作，并把11写入工作内存，最后写入主存。 然后线程1接着进行加1操作，由于已经读取了inc的值，注意此时在线程1的工作内存中inc的值仍然为10，所以线程1对inc进行加1操作后inc的值为11，然后将11写入工作内存，最后写入主存。 那么两个线程分别进行了一次自增操作后，inc只增加了1。 解释到这里，可能有朋友会有疑问，不对啊，前面不是保证一个变量在修改volatile变量时，会让缓存行无效吗？然后其他线程去读就会读到新的值，对，这个没错。这个就是上面的happens-before规则中的volatile变量规则，但是要注意，线程1对变量进行读取操作之后，被阻塞了的话，并没有对inc值进行修改。然后虽然volatile能保证线程2对变量inc的值读取是从内存中读取的，但是线程1没有进行修改，所以线程2根本就不会看到修改的值。\n根源就在这里，自增操作不是原子性操作，而且volatile也无法保证对变量的任何操作都是原子性的。解决的方法也就是对提供原子性的自增操作即可。\n在Java 1.5的java.util.concurrent.atomic包下提供了一些原子操作类，即对基本数据类型的 自增（加1操作），自减（减1操作）、以及加法操作（加一个数），减法操作（减一个数）进行了封装，保证这些操作是原子性操作。atomic是利用CAS来实现原子性操作的（Compare And Swap），CAS实际上是利用处理器提供的 CMPXCHG 指令实现的，而处理器执行 CMPXCHG 指令是一个原子性操作。\nVolatile与有序性 在前面提到volatile关键字能禁止指令重排序，所以volatile能在一定程度上保证有序性。volatile关键字禁止指令重排序有两层意思：\n 当程序执行到volatile变量的读操作或者写操作时，在其前面的操作的更改肯定全部已经进行，且结果已经对后面的操作可见，在其后面的操作肯定还没有进行； 在进行指令优化时，不能将在对volatile变量访问的语句放在其后面执行，也不能把volatile变量后面的语句放到其前面执行。  可能上面说的比较绕，举个简单的例子：\n//x、y为非volatile变量 //flag为volatile变量 x = 2; //语句1 y = 0; //语句2 flag = true; //语句3 x = 4; //语句4 y = -1; //语句5 由于flag变量为volatile变量，那么在进行指令重排序的过程的时候，不会将语句3放到语句1、语句2前面，也不会讲语句3放到语句4、语句5后面。但是要注意语句1和语句2的顺序、语句4和语句5的顺序是不作任何保证的。\n并且volatile关键字能保证，执行到语句3时，语句1和语句2必定是执行完毕了的，且语句1和语句2的执行结果对语句3、语句4、语句5是可见的。\nVolatile的原理和实现机制 前面讲述了源于volatile关键字的一些使用，下面我们来探讨一下volatile到底如何保证可见性和禁止指令重排序的。下面这段话摘自《深入理解Java虚拟机》：\n 观察加入volatile关键字和没有加入volatile关键字时所生成的汇编代码发现，加入volatile关键字时，会多出一个lock前缀指令\n lock前缀指令实际上相当于一个 内存屏障（也成内存栅栏），内存屏障会提供3个功能：\n 它 确保指令重排序时不会把其后面的指令排到内存屏障之前的位置，也不会把前面的指令排到内存屏障的后面；即在执行到内存屏障这句指令时，在它前面的操作已经全部完成； 它会 强制将对缓存的修改操作立即写入主存； 如果是写操作，它会导致其他CPU中对应的缓存行无效。  "});index.add({'id':57,'href':'/interview/docs/architecture/distributed/zk/','title':"Zookeeper",'content':"Zookeeper  ZK 不是解决分布式问题的银弹\n 分布式应用 分布式应用可以在给定时间（同时）在网络中的多个系统上运行，通过协调它们以快速有效的方式完成特定任务。通常来说，对于复杂而耗时的任务，非分布式应用（运行在单个系统中）需要几个小时才能完成，而分布式应用通过使用所有系统涉及的计算能力可以在几分钟内完成。\n通过将分布式应用配置为在更多系统上运行，可以进一步减少完成任务的时间。分布式应用正在运行的一组系统称为 集群，而在集群中运行的每台机器被称为 节点。\n分布式应用的优点  可靠性：单个或几个系统的故障不会使整个系统出现故障。 可扩展性：可以在需要时增加性能，通过添加更多机器，在应用程序配置中进行微小的更改，而不会有停机时间。 透明性：隐藏系统的复杂性，并将其显示为单个实体/应用程序。  分布式应用的挑战  竞争条件：两个或多个机器尝试执行特定任务，实际上只需在任意给定时间由单个机器完成。例如，共享资源只能在任意给定时间由单个机器修改。 死锁：两个或多个操作等待彼此无限期完成。 不一致：数据的部分失败。  ZooKeeper基础 Apache ZooKeeper是由集群（节点组）使用的一种服务，用于在自身之间协调，并通过稳健的同步技术维护共享数据。ZooKeeper本身是一个分布式应用程序，为写入分布式应用程序提供服务。\nZooKeeper 的好处：\n 简单的分布式协调过程 同步：服务器进程之间的相互排斥和协作。 有序性 序列化：根据特定规则对数据进行编码(Jute)。 可靠性 原子性：数据转移完全成功或完全失败，但没有事务是部分的。  架构 一个 ZooKeeper 集群通常由一组机器组成，一般 3 台以上就可以组成一个可用的 ZooKeeper 集群了。组成 ZooKeeper 集群的每台机器都会在内存中维护当前的服务器状态，并且每台机器之间都会互相保持通信。 ZooKeeper 本身就是一个 复制和分布式 应用程序，其目的作为服务运行，类似于我们运行 DNS 或任何其他集中式服务的方式。\n ZK 集群 半数以上存活 即可用\n ZooKeeper 的客户端程序会选择和集群中的任意一台服务器创建一个 TCP 连接，而且一旦客户端和服务器断开连接，客户端就会自动连接到集群中的其他服务器。\n   部分 描述     Client（客户端） 客户端是我们的分布式应用集群中的一个节点，从服务器访问信息。对于特定的时间间隔，每个客户端向服务器发送消息以使服务器知道客户端是活跃的。类似地，当客户端连接时，服务器发送确认码。如果连接的服务器没有响应，客户端会自动将消息重定向到另一个服务器。   Server（服务器） 服务器，我们的ZooKeeper总体中的一个节点，为客户端提供所有的服务。向客户端发送确认码以告知服务器是活跃的。   ZooKeeper Service ZooKeeper服务器组。形成 Service 所需的最小节点数为3。   Leader 服务器节点，如果任何连接的节点失败，则执行自动恢复。Leader在服务启动时被选举。   Follower 用于接受客户端请求并向客户端返回结果，在选主过程中参与投票   Observer 接受客户端连接，将写请求转发给leader，但 observer 不参与 投票过程，只同步 leader 的状态， observer 的目的是为了扩展系统，提高读取速度    数据模型 到znode是一个标准的文件系统，层次结构很像一棵树。 需要注意的一些要点如下：\n 根节点有一个名为 /zoo 的子节点，它又有三个 znode 。 ZooKeeper 树中的每个 znode 都由一个路径标识，路径元素由/分隔。 这些节点被称为数据寄存器，因为它们可以存储数据。 因此，一个 znode 可以有子节点以及与之相关的数据。 这与文件系统可以把文件作为路径很类似。  znode 中的数据通常以字节格式存储，每个 znode 中的最大数据大小不超过1 MB。 ZooKeeper 是为协调而设计的，几乎所有形式的协调数据都比较小， 因此，对数据大小的限制是强制的。\n与文件系统中的文件一样， znode 维护一个 stat 结构，其中包含数据更改的 版本号 以及随更改相关的时间戳而更改的 访问控制列表（ACL）。 只要 znode 的数据发生变化，版本号就会增加。 ZooKeeper 使用版本号以及相关的时间戳来验证它的核心内缓存。 znode 版本号还允许客户端通过 ZooKeeper API 更新或删除特定的 znode。 如果指定的版本号与 znode 的当前版本不匹配，则操作失败。 但是，执行 znode 更新或删除操作时，可以通过指定 0 作为版本号来覆盖。\nZnode  persistent：即使在创建该特定znode的客户端断开连接后，持久节点仍然存在。默认情况下，除非另有说明，否则所有znode都是持久的。 ephemeral：客户端活跃时，临时节点就是有效的。当客户端与 ZooKeeper 集合断开连接时，临时节点会自动删除。因此，只有临时节点不允许有子节点。如果临时节点被删除，则下一个合适的节点将填充其位置。临时节点在 leader 选举中起着重要作用。 sequential：顺序节点可以是持久的或临时的。当一个新的 znode 被创建为一个顺序节点时，ZooKeeper 通过将 10位 的序列号附加到原始名称来设置 znode 的路径。例如，如果将具有路径 /myapp 的znode创建为顺序节点，则ZooKeeper会将路径更改为 /myapp0000000001 ，并将下一个序列号设置为0000000002。如果两个顺序节点是同时创建的，那么 ZooKeeper 不会对每个znode使用相同的数字。顺序节点在锁定和同步中起重要作用。  Sessions 会话对于 ZooKeeper 的操作非常重要。会话中的请求按 FIFO 顺序执行。一旦客户端连接到服务器，将建立会话并向客户端分配 会话ID 。\n客户端 以特定的时间间隔发送心跳 以保持会话有效。如果 ZooKeeper 集合在超过服务器开启时指定的期间（会话超时）都没有从客户端接收到心跳，则它会判定客户端死机。\n会话超时通常以毫秒为单位。当会话由于任何原因结束时，在该会话期间创建的临时节点也会被删除。\nWatcher ZooKeeper 的设计是一种可伸缩的、健壮的集中式服务。在客户端访问此类服务时，常见的设计模式是通过轮询或拉式（pull）模型。当在大型和复杂的分布式系统中实现时，拉模型常常会受到可伸缩性问题的影响。为了解决这个问题，ZooKeeper设计了一种机制，客户端可以从 ZooKeeper 服务中获取通知。客户端接收到这个消息通知之后，需要主动到服务端获取最新的数据。\n客户可以使用 ZooKeeper 服务注册与 znode 相关的任何更改。 这种注册被称为在 ZooKeeper 术语中的 znode 上设置 watch。 监视允许客户以任何方式更改 znode 时收到通知。 Watcher 是一次性操作，这意味着它只触发一个通知。 要继续接收通知，客户必须在收到每个事件通知后重新注册一个监视。\n监视触发：\n 对 znode 数据的任何更改，例如使用 setData 操作将新数据写入 znode 的数据字段时。 对 znode 的子节点的任何更改。 例如，一个 znode 的子节点被删除。 正在创建或删除的 znode ，如果将新的 znode 添加到路径中或现有的 znode 被删除，则可能发生这种情况。  同样，ZooKeeper 针对监视和通知声明以下保证：\n ZooKeeper 确保监视始终以先进先出（FIFO）方式排序，并且通知总是按顺序发送 在对同一个 znode 进行任何其他更改之前，监视会将通知发送给客户端 监视事件的顺序是按照 ZooKeeper 服务的更新顺序排列的  Zookeeper 工作流程 一旦 ZooKeeper 集合启动，它将等待客户端连接。客户端将连接到 ZooKeeper 集合中的一个节点。它可以是 leader 或 follower 节点。一旦客户端被连接，节点将向特定客户端分配 会话ID 并向该客户端发送确认。如果客户端没有收到确认，它将尝试连接 ZooKeeper 集合中的另一个节点。 一旦连接到节点，客户端将以有规律的间隔向节点发送 心跳，以确保连接不会丢失。\n  如果客户端想要读取特定的znode，它将会向具有znode路径的节点发送读取请求，并且节点通过从其自己的数据库获取来返回所请求的znode。为此，在ZooKeeper集合中读取速度很快。\n  如果客户端想要将数据存储在ZooKeeper集合中，则会将 znode 路径和数据发送到服务器。连接的服务器将该请求转发给 leader，然后leader将向所有的follower重新发出写入请求。如果只有大部分节点成功响应，而写入请求成功，则成功返回代码将被发送到客户端。 否则，写入请求失败。绝大多数节点被称为 Quorum 。\n  ZooKeeper Service 节点数量的影响  如果我们有 单个节点，则当该节点故障时，ZooKeeper Service 将故障。即“单点故障\u0026rdquo;，不建议在生产环境中使用。 如果我们有 两个节点 而一个节点故障，我们没有占多数，ZooKeeper Service 故障，因为两个中的一个不是多数。 如果我们有 三个节点 而一个节点故障，那么我们有大多数，因此，这是 最低要求。ZooKeeper集合在实际生产环境中必须至少有三个节点。 如果我们有 四个节点 而两个节点故障，它将再次故障。类似于有三个节点，额外节点不用于任何目的，因此，最好添加奇数的节点，例如 3，5，7。  我们知道写入过程比 ZooKeeper 集合中的读取过程要耗时，因为 所有节点都需要在数据库中写入相同的数据。因此，对于平衡的环境拥有较少数量（例如3，5，7）的节点比拥有大量的节点要好。\n选举 下面任何一种情况，都会触发 Leader 选举：\n 启动时，集群服务器刚启动 运行时，Leader 崩溃  服务器的状态流转：\nLeader 选举过程，本质就是 广播优先级消息 的过程，选出 数据最新的服务节点，选出优先级最高的服务节点，基本步骤：\n 各个服务器节点，广播自己的优先级标识 (sid，zxid) 服务器节点收到其他广播消息后，跟自己的优先级对比，自己优先级低，则变更当前节点投票的优先级(sid，zxid) ，并广播变更后的结果 当任意一个服务器节点收到的投票数，超过了法定数量(quorum)，则，升级为 Leader，并广播结果。    由于网络延时，节点得不到足够多广播信息时，会做出错误的投票判断，纠正过程更耗时 选举过程中，服务器节点会等待一定时间，再广播投票信息，时间间隔一般设定为 200 ms 上面 Leader 选举，采取事件触发 Push 方式 广播消息，称为 快速 Leader 选举，因为之前的 Leader 选举，采用 Pull 方式，每隔 1s 拉取一次。   应用场景 发布订阅 通过 Zookeeper 进行数据的发布与订阅其实可以说是它提供的最基本功能，它能够允许多个客户端同时订阅某一个节点的变更并在变更发生时执行我们预先设置好的回调函数，在运行时改变服务的配置和行为\n命名服务 除了实现服务配置数据的发布与订阅功能，Zookeeper 还能帮助分布式系统实现命名服务，在每一个分布式系统中，客户端应用都有根据指定名字获取资源、服务器地址的需求，在这时就要求整个集群中的全部服务有着唯一的名字。\n在大型分布式系统中，有两件事情非常常见，一是不同服务之间的可能拥有相同的名字，另一个是同一个服务可能会在集群中部署很多的节点，Zookeeper 就可以通过文件系统和顺序节点解决这两个问题。\n协调分布式事务 Zookeeper 的另一个作用就是担任分布式事务中的协调者角色，在之前介绍 分布式事务 的文章中我们曾经介绍过分布式事务本质上都是通过 2PC 来实现的，在两阶段提交中就需要一个协调者负责协调分布式事务的执行。\n所有的事务参与者会向当前节点中写入提交或者终止，一旦当前的节点改变了事务的状态，其他节点就会得到通知，如果出现一个写入终止的节点，所有的节点就会回滚对分布式事务进行回滚。\n分布式锁 在数据库中，锁的概念其实是非常重要的，常见的关系型数据库就会对排他锁和共享锁进行支持，而 Zookeeper 提供的 API 也可以让我们非常简单的实现分布式锁。\n作为分布式协调服务，Zookeeper 的应用场景非常广泛，不仅能够用于服务配置的下发、命名服务、协调分布式事务以及分布式锁，还能够用来实现微服务治理中的服务注册以及发现等功能，这些其实都源于 Zookeeper 能够提供高可用的分布式协调服务，能够为客户端提供分布式一致性的支持。\nZooKeeper 的缺陷 zookeeper 不是为高可用性设计的  由于要跨机房容灾，很多系统实际上是需要跨机房部署的。出于性价比的考虑我们通常会让多个机房同时工作，而不会搭建N倍的冗余。也就是说单个机房肯定撑不住全流量（你能设想谷歌在全球只剩下一个机房在干活吗）。由于 zookeeper 集群只能有一个 master，因此一旦机房之间连接出现故障，zookeeper master 就只能照顾一个机房，其他机房运行的业务模块由于没有 master 都只能停掉。于是所有流量集中到有 master 的那个机房，于是系统 crash。 即使是在同一个机房里面，由于网段的不同，在调整机房交换机的时候偶尔也会发生网段隔离的情况。实际上机房每个月基本上都会发生短暂的网络隔离之类的子网段调整。在那个时刻 zookeeper 将处于不可用状态。如果整个业务系统基于 zookeeper （比如要求每个业务请求都先去 zookeeper 获取业务系统的master地址），则系统的可用性将非常脆弱。 由于 zookeeper 对于网络隔离的极度敏感，导致 zookeeper 对于网络的任何风吹草动都会做出激烈反应。这使得 zookeeper 的‘不可用’时间比较多，我们不能让 zookeeper 的‘不可用’，变成系统的不可用。  zookeeper 的选举过程速度很慢  这是一个很难从理论分析上看到的弱点，但是你一旦遇到就会痛不欲生。 前面我们已经说过，网络实际上常常是会出现隔离等不完整状态的，而 zookeeper 对那种情况非常敏感。一旦出现网络隔离， zookeeper 就要发起选举流程。 zookeeper 的选举流程通常耗时 30 到 120 秒，期间 zookeeper 由于没有master，都是不可用的。 对于网络里面偶尔出现的，比如半秒一秒的网络隔离，zookeeper 会由于选举过程，而把不可用时间放大几十倍。  zookeeper 的性能是有限的  典型的 zookeeper 的 tps(transaction peer secondes) 大概是一万多，无法覆盖系统内部每天动辄几十亿次的调用。因此每次请求都去 zookeeper 获取业务系统 master 信息是不可能的。 因此 zookeeper 的 client 必须自己缓存业务系统的 master 地址。 因此 zookeeper 提供的‘强一致性’实际上是不可用的。如果我们需要强一致性，还需要其他机制来进行保障：比如用自动化脚本把业务系统的 old master 给 kill 掉，但是那会有很多陷阱。  zookeeper 无法进行有效的权限控制  zookeeper 的权限控制非常薄弱 在大型的复杂系统里面，使用 zookeeper 必须自己再额外的开发一套权限控制系统，通过那套权限控制系统再访问 zookeeper 额外的权限控制系统不但增加了系统复杂性和维护成本，而且降低了系统的总体性能  即使有了 zookeeper 也很难避免业务系统的数据不一致  前面已经讨论过了，由于 zookeeper 的性能限制，我们无法让每次系统内部调用都走 zookeeper ，因此总有某些时刻，业务系统会存在两个 master（业务系统 client 那边缓存的业务系统 master 信息是定时从 zookeeper 更新的，因此会有更新不同步的问题）。 如果要在业务系统 client 的 master 信息不一致的情况下，仍要保持系统的数据一致性的方法是 先 kill 掉老 master ，再在 zookeeper 上更新 master 信息。但是在是否要 kill current master 这个问题上，程序是无法完全自动决定的（因为网络隔离的时候zookeeper已经不可用了，自动脚本没有全局信息，不管怎么做都可能是错的，什么都不做也可能是错的。当网络故障的时候，只有运维人员才有全局信息，程序是无法接电话得知其他机房的情况的）。因此系统无法自动的保障数据一致性，必须要人工介入。而人工介入的典型时间是半个小时以上，我们不能让系统这么长时间不可用。因此我们必须在某个方向上进行妥协，最常见的妥协方式是放弃 ‘强一致性’，而接受‘最终一致性’。 如果我们需要人工介入才能保证‘可靠的强一致性’，那么 zookeeper 的价值就大打折扣。  Zookeeper 并不保证读取的是最新数据 ZooKeeper 并不保证在每个实例中，两个不同的客户端将具有相同的 ZooKeeper 数据的视图。由于诸如网络延迟的因素，一个客户端可以在另一客户端被通知该改变之前执行更新，考虑两个客户端A和B的场景。如果客户端A将 /a 的值从 0 设置为 1 ，客户端B读取 /a ，客户端 B 可以读取旧值 0，这取决于它连接到的服务器。如果客户端A 和客户端B 读取相同的值很重要，则客户端B应该在执行读取之前从 ZooKeeper API 方法调用 sync() 方法。\n对于 Zookeeper 来说，它实现了A可用性、P分区容错性、C中的写入强一致性，丧失的是C中的读取一致性。\n我们能做什么  我们或者选择人工介入的强一致性，或者选择程序自动化进行的弱一致性。需要进行取舍。 最终一致性甚至未必是程序来做的，有时候人工修正数据反而在灵活、可靠、低成本上有优势。这需要权衡。 不要迷信zookeeper，有时候不妨考虑一下主备数据库。数据库自带权限控制，用起来比zookeeper方便多了。 zookeeper 比较有价值的东西也许是内容变化的时候，可以阻塞回调的方式通知所有在线的 client 实时更新信息，但这个功能用处不大。  FAQ 这段时间来，也在和公司里的一些同学交流使用zk的心得，整理了一些常见的zookeeper问题。这个页面的目标是解答一些zk常见的使用问题，同时也让大家明确zk不能干什么。页面会一直更新。\n客户端对 ServerList 的轮询机制是什么 随机，客户端在初始化( new ZooKeeper(String connectString, int sessionTimeout, Watcher watcher) )的过程中，将所有 Server 保存在一个 List 中，然后随机打散，形成一个环。之后从 0 号位开始一个一个使用。两个注意点：\n Server地址能够重复配置，这样能够弥补客户端无法设置Server权重的缺陷，但是也会加大风险。（比如: 192.168.1.1:2181,192.168.1.1:2181,192.168.1.2:2181). 如果客户端在进行 Server 切换过程中耗时过长，那么将会收到 SESSION_EXPIRED . 这也是上面第1点中的加大风险之处。  客户端如何正确处理 CONNECTIONLOSS (连接断开) 和 SESSIONEXPIRED (Session 过期)两类连接异常 在 ZooKeeper 中，服务器和客户端之间维持的是一个 长连接，在 SESSION_TIMEOUT 时间内，服务器会确定客户端是否正常连接(客户端会定时向服务器发送 heart_beat ),服务器重置下次 SESSION_TIMEOUT 时间。因此，在正常情况下， Session 一直有效，并且 zk 集群所有机器上都保存这个 Session 信息。在出现问题情况下，客户端与服务器之间连接断了（客户端所连接的那台zk机器挂了，或是其它原因的网络闪断），这个时候客户端会主动在地址列表（初始化的时候传入构造方法的那个参数 connectString ）中选择新的地址进行连接。\n好了，上面基本就是服务器与客户端之间维持长连接的过程了。在这个过程中，用户可能会看到两类异常 CONNECTIONLOSS (连接断开) 和 SESSIONEXPIRED (Session 过期)。\n CONNECTIONLOSS ：应用在进行操作A时，发生了 CONNECTIONLOSS ，此时用户不需要关心我的会话是否可用，应用所要做的就是等待客户端帮我们自动连接上新的 zk 机器，一旦成功连接上新的 zk 机器后，确认刚刚的操作A是否执行成功了。 SESSIONEXPIRED ：这个通常是zk客户端与服务器的连接断了，试图连接上新的 zk 机器，这个过程如果耗时过长，超过 SESSION_TIMEOUT 后还没有成功连接上服务器，那么服务器认为这个 session 已经结束了（服务器无法确认是因为其它异常原因还是客户端主动结束会话），开始清除和这个会话有关的信息，包括这个会话创建的临时节点和注册的 Watcher 。在这之后，客户端重新连接上了服务器在，但是很不幸，服务器会告诉客户端 SESSIONEXPIRED 。此时客户端要做的事情就看应用的复杂情况了，总之，要重新实例 zookeeper 对象，重新操作所有临时数据（包括临时节点和注册 Watcher ）。  一个客户端修改了某个节点的数据，其它客户端能够马上获取到这个最新数据吗 ZooKeeper 不能确保任何客户端能够获取（即 Read Request ）到一样的数据，除非客户端自己要求：方法是客户端在获取数据之前调用org.apache.zookeeper.AsyncCallback.VoidCallback, java.lang.Object) sync.\n通常情况下（这里所说的通常情况满足：1. 对获取的数据是否是最新版本不敏感，2. 一个客户端修改了数据，其它客户端需要不需要立即能够获取最新），可以不关心这点。\n在其它情况下，最清晰的场景是这样：ZK 客户端 A 对 /my_test 的内容从 v1-\u0026gt;v2, 但是 ZK 客户端 B 对 /my_test 的内容获取，依然得到的是 v1. 请注意，这个是实际存在的现象，当然延时很短。解决的方法是客户端B先调用 sync(), 再调用 getData().\nZK为什么不提供一个永久性的Watcher注册机制 不支持用持久Watcher的原因很简单，ZK无法保证性能。\n使用watch需要注意的几点  Watches 通知是一次性的，必须重复注册. 发生 CONNECTIONLOSS 之后，只要在 session_timeout 之内再次连接上（即不发生 SESSIONEXPIRED ），那么这个连接注册的 watches 依然在。 节点数据的版本变化会触发 NodeDataChanged ，注意，这里特意说明了是版本变化。存在这样的情况，只要成功执行了 setData()方法，无论内容是否和之前一致，都会触发 NodeDataChanged 。 对某个节点注册了 watch ，但是节点被删除了，那么注册在这个节点上的 watches 都会被移除。 同一个 zk 客户端对某一个节点注册相同的 watch ，只会收到一次通知。 Watcher 对象只会保存在客户端，不会传递到服务端。  我能否收到每次节点变化的通知 如果节点数据的更新频率很高的话，不能。\n原因在于：当一次数据修改，通知客户端，客户端再次注册 watch ，在这个过程中，可能数据已经发生了许多次数据修改，因此，千万不要做这样的测试：\u0026ldquo;数据被修改了n次，一定会收到n次通知\u0026quot;来测试 server 是否正常工作。\n能为临时节点创建子节点吗 不能。\n是否可以拒绝单个IP对ZK的访问,操作 ZK 本身不提供这样的功能，它仅仅提供了对单个 IP 的连接数的限制。你可以通过修改 iptables 来实现对单个 ip 的限制。\n在[getChildren(String path, boolean watch)]注册对节点子节点的变化，那么子节点的子节点变化能通知吗 不能\n创建的临时节点什么时候会被删除，是连接一断就删除吗？延时是多少？ 连接断了之后，ZK 不会马上移除临时数据，只有当 SESSIONEXPIRED 之后，才会把这个会话建立的临时数据移除。因此，用户需要谨慎设置 Session_TimeOut\nzookeeper是否支持动态进行机器扩容？如果目前不支持，那么要如何扩容呢？ 3.4.3版本的zookeeper，还不支持这个功能，在3.5.0版本开始，支持动态加机器了。\nZooKeeper集群中个服务器之间是怎样通信的？ Leader服务器会和每一个 Follower/Observer 服务器都建立TCP连接，同时为每个 F/O 都创建一个叫做 LearnerHandler 的实体。LearnerHandler 主要负责 Leader 和 F/O 之间的网络通讯，包括数据同步，请求转发和 Proposal 提议的投票等。Leader 服务器保存了所有 F/O 的 LearnerHandler 。\nzookeeper是否会自动进行日志清理？如果进行日志清理？ zk自己不会进行日志清理，需要运维人员进行日志清理\n参考文档  ZooKeeper FAQ Apache ZooKeeper数据模型 Zookeeper并不保证读取的是最新数据 详解分布式协调服务 ZooKeeper ZooKeeper 架构 阿里巴巴为什么不用ZooKeeper 做服务发现？ ZooKeeper 技术内幕：Leader 选举 Zookeeper: 分布式过程协同技术详解  "});index.add({'id':58,'href':'/interview/docs/leetcode/threeSum/','title':"三数之和",'content':"头条重点\n题目 给定一个包含 n 个整数的数组 nums ，判断 nums 中是否存在三个元素 a，b，c ，使得 a + b + c = 0 ？找出所有满足条件且不重复的三元组。\n注意：答案中不可以包含重复的三元组。\n例如, 给定数组 nums = [-1, 0, 1, 2, -1, -4]， 满足要求的三元组集合为： [ [-1, 0, 1], [-1, -1, 2] ] 解题思路  将数组排序 固定一位数，然后通过两个指针对撞，寻找总和为 0 的三个数  public static List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; threeSum(int[] nums) { if (nums.length \u0026lt; 3) { return Collections.emptyList(); } Set\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; res = new HashSet\u0026lt;\u0026gt;(); Arrays.sort(nums); int zCount = 0; for (int num : nums) { if (num == 0) { zCount++; } } for (int i = 0; i \u0026lt; nums.length \u0026amp;\u0026amp; nums[i] \u0026lt; 0; i++) { int first = nums[i]; int j = i + 1; int k = nums.length - 1; while (j \u0026lt; k) { int t = nums[j] + nums[k] + first; if (t == 0) { List\u0026lt;Integer\u0026gt; list = new ArrayList\u0026lt;\u0026gt;(); list.add(first); list.add(nums[j]); list.add(nums[k]); res.add(list); j++; k--; } else if (t \u0026gt; 0) { k--; } else { j++; } } } if (zCount \u0026gt;= 3) { List\u0026lt;Integer\u0026gt; list = new ArrayList\u0026lt;\u0026gt;(); list.add(0); list.add(0); list.add(0); res.add(list); } return new ArrayList\u0026lt;\u0026gt;(res); } "});index.add({'id':59,'href':'/interview/docs/offer/Add/','title':"不用加减乘除做加法",'content':"题目 牛客网\n写一个函数，求两个整数之和，要求在函数体内不得使用+、-、*、/四则运算符号。\n解题思路  将加法分解成两步 两个数不计算进位相加得到 sum，计算进位 carry 再将进位加上：sum = sum + carry 直到没有进位为止  public int Add(int num1, int num2) { int sum, carry; do { sum = num1 ^ num2; carry = (num1 \u0026amp; num2) \u0026lt;\u0026lt; 1; num1 = sum; num2 = carry; } while (num2 != 0); return sum; } "});index.add({'id':60,'href':'/interview/docs/offer/GetUglyNumber/','title':"丑数",'content':"牛客网\n把只包含质因子2、3和5的数称作丑数（Ugly Number）。例如6、8都是丑数，但14不是，因为它包含质因子7。 习惯上我们把1当做是第一个丑数。求按从小到大的顺序的第N个丑数。\n解题思路  通过保存已有丑数的方式，用空间换时间 对于已有丑数 $$M$$ ，那么下一个丑数 $$M=\\min(M_{2}\\times2,M_{3}\\times3,M_{5}\\times5)$$ $$M_{max}$$ 是目前最大的丑数，那么 $$M_{2}$$ 是已有丑数中 $$M_{2}\\times2$$ 第一个大于 $$M_{max}$$ 的丑数  public int GetUglyNumber_Solution(int index) { if (index == 0) { return 0; } if (index == 1) { return 1; } ArrayList\u0026lt;Integer\u0026gt; list = new ArrayList\u0026lt;\u0026gt;(index); list.add(1); int preIndex2 = 0; int preIndex3 = 0; int preIndex5 = 0; for (int i = 0; i \u0026lt; index; i++) { int next2 = list.get(preIndex2) * 2; int next3 = list.get(preIndex3) * 3; int next5 = list.get(preIndex5) * 5; int nextV = Math.min(Math.min(next2, next3), next5); list.add(nextV); while (preIndex2 \u0026lt; list.size() - 1 \u0026amp;\u0026amp; list.get(preIndex2) * 2 \u0026lt;= nextV) preIndex2++; while (preIndex3 \u0026lt; list.size() - 1 \u0026amp;\u0026amp; list.get(preIndex3) * 3 \u0026lt;= nextV) preIndex3++; while (preIndex5 \u0026lt; list.size() - 1 \u0026amp;\u0026amp; list.get(preIndex5) * 5 \u0026lt;= nextV) preIndex5++; } return list.get(index - 1); } "});index.add({'id':61,'href':'/interview/docs/offer/FindFirstCommonNode/','title':"两个链表的第一个公共结点",'content':"题目 牛客网\n输入两个链表，找出它们的第一个公共结点。\n解决思路 空间复杂度 O(n) 的算法  使用辅助容器，保存第一个链表的所有元素 遍历第二个链表，并对比当前节点是否在辅助容器中  /** * 空间 O(n) * * @param pHead1 * @param pHead2 * @return */ public ListNode FindFirstCommonNode_1(ListNode pHead1, ListNode pHead2) { Set\u0026lt;ListNode\u0026gt; node1s = new HashSet\u0026lt;\u0026gt;(); while (pHead1 != null) { node1s.add(pHead1); pHead1 = pHead1.next; } while (pHead2 != null) { if (node1s.contains(pHead2)) { return pHead2; } pHead2 = pHead2.next; } return null; } 空间复杂度 O(1) 的算法  由于两个链表有可能不一样长，首先通过遍历找到他们的长度 移动较长的那个链表，使得两个链表长度一致 同步遍历两个链表   原理：如果两个链表相交，那么它们一定有相同的尾节点\n /** * 空间 O(1) * * @param pHead1 * @param pHead2 * @return */ public ListNode FindFirstCommonNode_2(ListNode pHead1, ListNode pHead2) { int len1 = 0, len2 = 0; ListNode cursor1 = pHead1, cursor2 = pHead2; while (cursor1 != null) { cursor1 = cursor1.next; len1++; } while (cursor2 != null) { cursor2 = cursor2.next; len2++; } cursor1 = pHead1; cursor2 = pHead2; if (len1 \u0026gt; len2) { int i = len1; while (i != len2) { cursor1 = cursor1.next; i--; } } else if (len1 \u0026lt; len2) { int i = len2; while (i != len1) { cursor2 = cursor2.next; i--; } } while (cursor1 != null \u0026amp;\u0026amp; cursor2 != null) { if (cursor1 == cursor2) { return cursor1; } cursor1 = cursor1.next; cursor2 = cursor2.next; } return null; } "});index.add({'id':62,'href':'/interview/docs/leetcode/addTwoNumbers/','title':"两数相加",'content':"两数相加 头条重点\n题目 给出两个 非空 的链表用来表示两个非负的整数。其中，它们各自的位数是按照 逆序 的方式存储的，并且它们的每个节点只能存储 一位 数字。\n如果，我们将这两个数相加起来，则会返回一个新的链表来表示它们的和。\n您可以假设除了数字 0 之外，这两个数都不会以 0 开头。\n示例： 输入：(2 -\u0026gt; 4 -\u0026gt; 3) + (5 -\u0026gt; 6 -\u0026gt; 4) 输出：7 -\u0026gt; 0 -\u0026gt; 8 原因：342 + 465 = 807 解题思路 public ListNode addTwoNumbers(ListNode l1, ListNode l2) { if (l1 == null || l2 == null) { return null; } StringBuilder builder1 = new StringBuilder(); while (l1 != null) { builder1.append(l1.val); l1 = l1.next; } StringBuilder builder2 = new StringBuilder(); while (l2 != null) { builder2.append(l2.val); l2 = l2.next; } BigDecimal bigDecimal1 = new BigDecimal(builder1.reverse().toString()); BigDecimal bigDecimal2 = new BigDecimal(builder2.reverse().toString()); String resStr = bigDecimal1.add(bigDecimal2).toPlainString(); ListNode head = new ListNode(Integer.parseInt(String.valueOf(resStr.charAt(resStr.length() - 1)))); ListNode cur = head; for (int i = resStr.length() - 2; i \u0026gt;= 0; i--) { cur.next = new ListNode(Integer.parseInt(String.valueOf(resStr.charAt(i)))); cur = cur.next; } return head; } "});index.add({'id':63,'href':'/interview/docs/offer/SumOfNDice/','title':"个骰子的点数",'content':"题目 把 n 个骰子扔在地上，所有骰子朝上一面的和为 s，输入 n，打印 s 所有可能值的概率\n解题思路  首先考虑一个骰子的情况，那么有 1～6 出现的次数均为 1 再增加一个骰子时，由于各个点数出现的概率一致。用 $$f(n,s)=f(n-1,s-1)+f(n-1,s-2)+f(n-1,s-3)+f(n-1,s-4)+f(n-1,s-5)+f(n-1,s-6)$$ 使用两个数组循环求解  public void SumOfNDice(int n) { if (n \u0026lt; 1) { return; } int[][] nums = new int[2][n * 6 + 1]; int flag = 0; //初始化第一个骰子各总和出现的次数 int maxLen = nums[0].length; for (int i = 1; i \u0026lt; maxLen; i++) { nums[flag][i] = 1; } for (int i = 2; i \u0026lt;= n; i++) { int newFlag = flag ^ 0x01; Arrays.fill(nums[newFlag], 0); for (int j = i; j \u0026lt; maxLen; j++) { int sum = 0; for (int k = 1; k \u0026lt;= 6 \u0026amp;\u0026amp; (j - k \u0026gt;= 0); k++) { sum += nums[flag][j - k]; } nums[newFlag][j] = sum; } flag = newFlag; } //debug out System.out.println(Arrays.toString(nums[flag])); int sum = 0; for (int i : nums[flag]) { sum += i; } for (int i = 0; i \u0026lt; nums[flag].length; i++) { System.out.println(i + \u0026quot;:\u0026quot; + nums[flag][i] * 1.0 / sum); } } "});index.add({'id':64,'href':'/interview/docs/basic/os/interrupt/','title':"中断",'content':"中断 中断（英语：Interrupt）是指 处理器接收到来自硬件或软件的信号，提示发生了某个事件，应该被注意，这种情况就称为中断。\n通常，在接收到来自外围硬件（相对于中央处理器和内存）的异步信号，或来自软件的同步信号之后，处理器将会进行相应的 硬件／软件 处理。发出这样的信号称为进行中断请求（interrupt request，IRQ）。硬件中断导致处理器通过一个运行信息切换（context switch）来保存执行状态（以程序计数器和程序状态字等寄存器信息为主）；软件中断则通常作为CPU指令集中的一个指令，以可编程的方式直接指示这种运行信息切换，并将处理导向一段中断处理代码。中断在计算机多任务处理，尤其是即时系统中尤为有用。\n中断分类 硬件中断 由硬件发出或产生的中断称为硬中断，按硬中断事件的来源和实现手段可将中断划分为外中断和内中断：\n 外中断：又称为中断或异步中断，是指 来自处理器以外的中断信号，包括时钟中断、键盘中断、外部设备中断等。外中断又分为可屏蔽中断和不可屏蔽中断，各个中断具有不同的优先级，表示事件的紧急程度，在处理高一级中断时，往往会部分或全部屏蔽低等级中断。 内中断：又称为异常或同步中断（产生时必须考虑与处理器时钟同步），是指 来自处理器内部的中断信号，通常是由于程序执行过程中，发现与当前指令关联的、不正常的或错误的事件。内中断可以细分为：  访管中断，由执行系统调用而引起的。 硬件故障中断，如电源失效、总线超时等。 程序性中断，如非法操作、地址越界、除数为0和浮点溢出等。    软件中断 软件中断：是一条CPU指令，用以自陷一个中断。由于 软中断指令通常要运行一个切换CPU至内核态（Kernel Mode/Ring 0）的子例程，它常被用作实现系统调用（System call）。\n处理器通常含有一个内部中断屏蔽位，并允许通过软件来设定。一旦被设定，所有外部中断都将被系统忽略。这个屏蔽位的访问速度显然快于中断控制器上的中断屏蔽寄存器，因此可提供更快速地中断屏蔽控制。\n中断尽管可以提高计算机处理性能，但 过于密集的中断请求／响应反而会影响系统性能。这类情形被称作中断风暴（interrupt storm）。\n"});index.add({'id':65,'href':'/interview/docs/leetcode/maxProfit/','title':"买卖股票的最佳时机",'content':"头条重点\n题目 给定一个数组，它的第 i 个元素是一支给定股票第 i 天的价格。如果你最多只允许完成一笔交易（即买入和卖出一支股票），设计一个算法来计算你所能获取的最大利润。\n注意你不能在买入股票前卖出股票。\n示例 1: 输入: [7,1,5,3,6,4] 输出: 5 解释: 在第 2 天（股票价格 = 1）的时候买入，在第 5 天（股票价格 = 6）的时候卖出，最大利润 = 6-1 = 5 。 注意利润不能是 7-1 = 6, 因为卖出价格需要大于买入价格。 示例 2: 输入: [7,6,4,3,1] 输出: 0 解释: 在这种情况下, 没有交易完成, 所以最大利润为 0。 解题思路  要先买入才能卖出，先找最低价格点 再找最低价格之后的最高价格，用 res 表示最大利润  public int maxProfit(int[] prices) { if (prices.length \u0026lt;= 1) { return 0; } int res = 0; int minBuy = prices[0]; for (int price : prices) { res = Math.max(res, price - minBuy); minBuy = Math.min(minBuy, price); } return res; } "});index.add({'id':66,'href':'/interview/docs/leetcode/maxProfit2/','title':"买卖股票的最佳时机",'content':"头条重点\n题目 给定一个数组，它的第 i 个元素是一支给定股票第 i 天的价格。设计一个算法来计算你所能获取的最大利润。你可以尽可能地完成更多的交易（多次买卖一支股票）。\n注意：你不能同时参与多笔交易（你必须在再次购买前出售掉之前的股票）。\n示例 1: 输入: [7,1,5,3,6,4] 输出: 7 解释: 在第 2 天（股票价格 = 1）的时候买入，在第 3 天（股票价格 = 5）的时候卖出, 这笔交易所能获得利润 = 5-1 = 4 。 随后，在第 4 天（股票价格 = 3）的时候买入，在第 5 天（股票价格 = 6）的时候卖出, 这笔交易所能获得利润 = 6-3 = 3 。 示例 2: 输入: [1,2,3,4,5] 输出: 4 解释: 在第 1 天（股票价格 = 1）的时候买入，在第 5 天 （股票价格 = 5）的时候卖出, 这笔交易所能获得利润 = 5-1 = 4 。 注意你不能在第 1 天和第 2 天接连购买股票，之后再将它们卖出。 因为这样属于同时参与了多笔交易，你必须在再次购买前出售掉之前的股票。 示例 3: 输入: [7,6,4,3,1] 输出: 0 解释: 在这种情况下, 没有交易完成, 所以最大利润为 0。 解题思路  贪心算法，尽可能的多进行交易  public int maxProfit(int[] prices) { if (prices.length \u0026lt;= 1) { return 0; } int res = 0; for (int i = 1; i \u0026lt; prices.length; i++) { int profit = prices[i] - prices[i - 1]; if (profit \u0026gt; 0) { res += profit; } } return res; } "});index.add({'id':67,'href':'/interview/docs/basic/database/transaction/','title':"事务",'content':"事务 事务的特性 所谓事务，它是一个操作序列，这些操作要么都执行，要么都不执行，它是一个不可分割的工作单位。\nAtomicity（原子性） 原子性是指事务是一个不可再分割的工作单位，事务中的操作要么都发生，要么都不发生。\nConsistency（一致性） 一致性是指在事务开始之前和事务结束以后，数据库的完整性约束没有被破坏。这是说数据库事务不能破坏关系数据的完整性以及业务逻辑上的一致性。\nIsolation（隔离性） 多个事务并发访问时，事务之间是隔离的，一个事务不应该影响其它事务运行效果。\n这指的是在并发环境中，当不同的事务同时操纵相同的数据时，每个事务都有各自的完整数据空间。由并发事务所做的修改必须与任何其他并发事务所做的修改隔离。事务查看数据更新时，数据所处的状态要么是另一事务修改它之前的状态，要么是另一事务修改它之后的状态，事务不会查看到中间状态的数据。\nDurability（持久性） 持久性，意味着在事务完成以后，该事务所对数据库所作的更改便持久的保存在数据库之中，并不会被回滚。\n事务隔离级别 数据库是要被广大客户所共享访问的，那么在数据库操作过程中很可能出现以下几种不确定情况：\n  丢失修改：两个事务T1，T2读入同一数据并修改，T2提交的结果被T1破坏了，导致T1的修改丢失。（订票系统）\n  不可重复读：事务T1读取数据后，事务T2执行更新操作，使T1无法再次读取结果。\n   可以通过“读锁”和“写锁”解决不可重复读。读取数据的事务将会禁止写事务（但允许读事务），写事务则禁止任何其他事务。\n   读脏数据：事务T1修改某个数据并写回磁盘，事务T2读取同一数据，但T1由于某种原因撤销了，这时T1修改过的数据恢复原来的值，T2读取的数据就与数据库中的数据不一致。\n  幻读：事务在操作过程中进行两次查询，第二次查询结果包含了第一次查询中未出现的数据（这里并不要求两次查询SQL语句相同）这是因为在两次查询过程中有另外一个事务插入数据造成的。\n  为了避免上面出现几种情况在标准SQL规范中定义了4个事务隔离级别，不同隔离级别对事务处理不同 。\n未提交读（Read Uncommitted） 未提交读(READ UNCOMMITTED)是最低的隔离级别。允许脏读(dirty reads)，但不允许更新丢失，事务可以看到其他事务“尚未提交”的修改。\n提交读（Read Committed） 允许不可重复读取，但不允许脏读取。这可以通过“瞬间共享读锁”和“排他写锁”实现。读取数据的事务允许其他事务继续访问该行数据，但是未提交的写事务将会禁止其他事务访问该行。\n可重复读（Repeatable Read） 禁止不可重复读取和脏读取，但是有时可能出现幻读数据。这可以通过“共享读锁”和“排他写锁”实现。读取数据的事务将会禁止写事务（但允许读事务），写事务则禁止任何其他事务。\n可序列化(Serializable) 最高的隔离级别，它要求事务序列化执行，事务只能一个接着一个地执行，不能并发执行。仅仅通过“行级锁”是无法实现事务序列化的，必须通过其他机制保证新插入的数据不会被刚执行查询操作的事务访问到。\n隔离级别越高，越能保证数据的完整性和一致性，但是对并发性能的影响也越大。\n隔离级别的实现 数据库对于隔离级别的实现就是使用并发控制机制对在同一时间执行的事务进行控制，限制不同的事务对于同一资源的访问和更新，而最重要也最常见的并发控制机制，在这里我们将简单介绍三种最重要的并发控制器机制的工作原理。\n锁 锁是一种最为常见的并发控制机制，在一个事务中，我们并不会将整个数据库都加锁，而是只会锁住那些需要访问的数据项， MySQL 和常见数据库中的锁都分为两种，共享锁（Shared）和互斥锁（Exclusive），前者也叫读锁，后者叫写锁。\n读锁保证了读操作可以并发执行，相互不会影响，而写锁保证了在更新数据库数据时不会有其他的事务访问或者更改同一条记录造成不可预知的问题。\n时间戳 除了锁，另一种实现事务的隔离性的方式就是通过时间戳，使用这种方式实现事务的数据库，例如 PostgreSQL 会为每一条记录保留两个字段；读时间戳中包括了所有访问该记录的事务中的最大时间戳，而记录行的写时间戳中保存了将记录改到当前值的事务的时间戳。\n使用时间戳实现事务的隔离性时，往往都会使用乐观锁，先对数据进行修改，在写回时再去判断当前值，也就是时间戳是否改变过，如果没有改变过，就写入，否则，生成一个新的时间戳并再次更新数据，乐观锁其实并不是真正的锁机制，它只是一种思想，在这里并不会对它进行展开介绍。\n多版本和快照隔离 通过维护多个版本的数据，数据库可以允许事务在数据被其他事务更新时对旧版本的数据进行读取，很多数据库都对这一机制进行了实现；因为 所有的读操作不再需要等待写锁的释放，所以能够显著地提升读的性能， MySQL 和 PostgreSQL 都对这一机制进行自己的实现，也就是 MVCC ，虽然各自实现的方式有所不同，MySQL 就通过提到的 undo log 实现了 MVCC，保证事务并行执行时能够不等待互斥锁的释放直接获取数据。\nACID vs CAP 数据库对于 ACID 中的一致性的定义是这样的：如果一个事务原子地在一个一致地数据库中独立运行，那么在它执行之后，数据库的状态一定是一致的。对于这个概念，它的第一层意思就是对于数据完整性的约束，包括主键约束、引用约束以及一些约束检查等等，在事务的执行的前后以及过程中不会违背对数据完整性的约束，所有对数据库写入的操作都应该是合法的，并不能产生不合法的数据状态。\nCAP 定理中的数据一致性，其实是说分布式系统中的各个节点中对于同一数据的拷贝有着相同的值；而 ACID 中的一致性是指数据库的规则，如果 schema 中规定了一个值必须是唯一的，那么一致的系统必须确保在所有的操作中，该值都是唯一的，由此来看 CAP 和 ACID 对于一致性的定义有着根本性的区别。\n 数据库的一致性是：应用系统从一个正确的状态到另一个正确的状态.而 ACID 就是说事务能够通过 AID 来保证这个 C 的过程. C 是目的, AID 都是手段.\n 使用事务 在MySQL中使用START TRANSACTION 或 BEGIN开启事务，提交事务使用COMMIT，ROLLBACK用来放弃事务。MySQL默认设置了事务的自动提交，即一条SQL语句就是一个事务。\n总结 事务的（ACID）特性是由关系数据库管理系统（RDBMS，数据库系统）来实现的。数据库管理系统采用日志来保证事务的原子性、一致性和持久性。日志记录了事务对数据库所做的更新，如果某个事务在执行过程中发生错误，就可以根据日志，撤销事务对数据库已做的更新，使数据库退回到执行事务前的初始状态。\n数据库管理系统采用锁机制来实现事务的隔离性。当多个事务同时更新数据库中相同的数据时，只允许持有锁的事务能更新该数据，其他事务必须等待，直到前一个事务释放了锁，其他事务才有机会更新该数据。\n"});index.add({'id':68,'href':'/interview/docs/offer/BST-Link-Convert/','title':"二叉搜索树与双向链表",'content':"题目 牛客网\n输入一棵二叉搜索树，将该二叉搜索树转换成一个排序的双向链表。要求不能创建任何新的结点，只能调整树中结点指针的指向。\n解题思路  由于 BST 的特性，采用中序遍历正好符合排序 要考虑 root 节点要与 左节点的最大值连接，与右节点的最小值连接 增加一个已排序链表的指针，指向最后一个已排序节点  public TreeNode Convert(TreeNode pRootOfTree) { if (pRootOfTree == null) { return null; } TreeNode[] nodeList = {new TreeNode(-1)}; ConvertToLink(pRootOfTree, nodeList); TreeNode cursor = pRootOfTree; while (cursor.left != null) { cursor = cursor.left; } cursor.right.left = null; return cursor.right; } private void ConvertToLink(TreeNode root, TreeNode[] nodeList) { if (root == null) { return; } ConvertToLink(root.left, nodeList); root.left = nodeList[0]; nodeList[0].right = root; nodeList[0] = root; ConvertToLink(root.right, nodeList); } "});index.add({'id':69,'href':'/interview/docs/offer/VerifySquenceOfBST/','title':"二叉搜索树的后序遍历序列",'content':"题目 牛客网\n输入一个整数数组，判断该数组是不是某二叉搜索树的后序遍历的结果。如果是则输出 Yes ,否则输出 No 。假设输入的数组的任意两个数字都互不相同。\n解题思路  后序遍历中，最后一个节点为 root 节点 由于 BST 的左子树都小于 root，右子树都大于 root，那么可以判定该节点是否为 BST 依次类推，通过递归方式，再判定左右子树  public boolean VerifySquenceOfBST(int[] sequence) { if (sequence.length == 0) { return false; } if (sequence.length == 1) { return true; } return isBST(sequence, 0, sequence.length - 1); } private boolean isBST(int[] sequence, int start, int end) { if (start \u0026lt; 0 || end \u0026lt; 0 || start \u0026gt;= end) { return true; } int rootV = sequence[end]; int rightIndex = -1, rightV = Integer.MIN_VALUE; for (int i = start; i \u0026lt; end; i++) { if (rightV == Integer.MIN_VALUE \u0026amp;\u0026amp; sequence[i] \u0026gt; rootV) { rightV = sequence[i]; rightIndex = i; continue; } if (rightV != Integer.MIN_VALUE \u0026amp;\u0026amp; sequence[i] \u0026lt; rootV) { return false; } } return isBST(sequence, start, rightIndex - 1) \u0026amp;\u0026amp; isBST(sequence, rightIndex, end - 1); } "});index.add({'id':70,'href':'/interview/docs/offer/BSTKthNode/','title':"二叉搜索树的第",'content':"题目 给定一棵二叉搜索树，请找出其中的第 k 小的结点。例如，5，3，7，2，4，6，8 中，按结点数值大小顺序第三小结点的值为4。\n牛客网\n解题思路  BST 中序遍历的结果就是排序后的结果  public TreeNode KthNode(TreeNode pRoot, int k) { TreeNode[] nodes = new TreeNode[1]; int[] ints = {0}; KthNode(pRoot, k, nodes, ints); return nodes[0]; } private void KthNode(TreeNode root, int k, TreeNode[] res, int[] cursor) { if (root == null) return; if (res[0] != null) return; KthNode(root.left, k, res, cursor); cursor[0]++; if (cursor[0] == k) { res[0] = root; return; } KthNode(root.right, k, res, cursor); } "});index.add({'id':71,'href':'/interview/docs/offer/FindPath/','title':"二叉树中和为某一值的路径",'content':"题目 二叉树中和为某一值的路径\n输入一颗二叉树的跟节点和一个整数，打印出二叉树中结点值的和为输入整数的所有路径。路径定义为从树的根结点开始往下一直到叶结点所经过的结点形成一条路径。(注意: 在返回值的 list 中，数组长度大的数组靠前)\n解题思路  将走过的路径记录下来，当走过路径总和 = target 并且当前节点是叶子节点时，该路径符合要求 通过递归遍历所有可能的路径  public ArrayList\u0026lt;ArrayList\u0026lt;Integer\u0026gt;\u0026gt; FindPath(TreeNode root, int target) { ArrayList\u0026lt;ArrayList\u0026lt;Integer\u0026gt;\u0026gt; res = new ArrayList\u0026lt;\u0026gt;(); FindPath(res, new LinkedList\u0026lt;\u0026gt;(), root, 0, target); res.sort(Comparator.comparingInt(list -\u0026gt; -list.size())); return res; } private void FindPath(ArrayList\u0026lt;ArrayList\u0026lt;Integer\u0026gt;\u0026gt; res, LinkedList\u0026lt;Integer\u0026gt; path, TreeNode node, int pathSum, int target) { if (node == null) { return; } if (pathSum \u0026gt; target) { return; } if (pathSum + node.val == target \u0026amp;\u0026amp; node.right == null \u0026amp;\u0026amp; node.left == null) { ArrayList\u0026lt;Integer\u0026gt; resPath = new ArrayList\u0026lt;\u0026gt;(path); resPath.add(node.val); res.add(resPath); return; } path.addLast(node.val); if (node.left != null) { FindPath(res, path, node.left, pathSum + node.val, target); } if (node.right != null) { FindPath(res, path, node.right, pathSum + node.val, target); } path.removeLast(); } "});index.add({'id':72,'href':'/interview/docs/offer/GetNext/','title':"二叉树的下一个结点",'content':"题目 给定一个二叉树和其中的一个结点，请找出中序遍历顺序的下一个结点并且返回。注意，树中的结点不仅包含左右子结点，同时包含指向父结点的指针。\n解题思路 考虑以下二叉树：\n其中序遍历的结果是：d,b,h,e,i,a,f,c,g\n 当前节点有右子树：右子树的最左节点 \u0026ndash; b 节点 当前节点无右子树并且为父节点的左子树：父节点 \u0026ndash; d 节点 当前节点无右子树并且为父节点的右子树：第一个祖先节点为左节点的节点 \u0026ndash; i 节点  public TreeLinkNode GetNext(TreeLinkNode pNode) { if (pNode == null) return null; TreeLinkNode parent = pNode.next; if (pNode.right == null) { if (parent == null) { return null; } //右节点 if (parent.right == pNode) { TreeLinkNode cursor = parent; while (true) { TreeLinkNode p = cursor.next; if (p == null) return null; if (cursor == p.left) return p; cursor = p; } } else { return parent; } } else { TreeLinkNode cursor = pNode.right; while (cursor.left != null) { cursor = cursor.left; } return cursor; } } "});index.add({'id':73,'href':'/interview/docs/leetcode/lowestCommonAncestor/','title':"二叉树的最近公共祖先",'content':"题目 给定一个二叉树, 找到该树中两个指定节点的最近公共祖先。\n百度百科中最近公共祖先的定义为：“对于有根树 T 的两个结点 p、q，最近公共祖先表示为一个结点 x，满足 x 是 p、q 的祖先且 x 的深度尽可能大（一个节点也可以是它自己的祖先）。”\n示例 1: 输入: root = [3,5,1,6,2,0,8,null,null,7,4], p = 5, q = 1 输出: 3 解释: 节点 5 和节点 1 的最近公共祖先是节点 3。 解题思路  通过 DFS 找到节点的路径 从头开始遍历两个节点的路径，找到最后一个相等的节点  public TreeNode lowestCommonAncestor(TreeNode root, TreeNode p, TreeNode q) { LinkedList\u0026lt;TreeNode\u0026gt; pathP = new LinkedList\u0026lt;\u0026gt;(); LinkedList\u0026lt;TreeNode\u0026gt; pathQ = new LinkedList\u0026lt;\u0026gt;(); findNodePath(pathP, root, p); findNodePath(pathQ, root, q); TreeNode last = null; while (!pathP.isEmpty() \u0026amp;\u0026amp; !pathQ.isEmpty()) { TreeNode pi = pathP.pollFirst(); TreeNode qi = pathQ.pollFirst(); if (qi==pi) { last = pi; }else break; } return last; } private void findNodePath(LinkedList\u0026lt;TreeNode\u0026gt; path, TreeNode root, TreeNode target) { if (root == null) { return; } if (!path.isEmpty() \u0026amp;\u0026amp; path.getLast().val == target.val) { return; } path.addLast(root); findNodePath(path, root.left, target); if (!path.isEmpty() \u0026amp;\u0026amp; path.getLast().val == target.val) { return; } findNodePath(path, root.right, target); if (!path.isEmpty() \u0026amp;\u0026amp; path.getLast().val != target.val) { path.removeLast(); } } "});index.add({'id':74,'href':'/interview/docs/offer/TreeDepth/','title':"二叉树的深度",'content':"题目 牛客网\n输入一棵二叉树，求该树的深度。从根结点到叶结点依次经过的结点（含根、叶结点）形成树的一条路径，最长路径的长度为树的深度。\n解题思路  深度优先遍历  public int TreeDepth(TreeNode root) { int[] max = {0}; depth(root, max, 1); return max[0]; } private void depth(TreeNode root, int[] max, int curDepth) { if (root == null) return; if (curDepth \u0026gt; max[0]) max[0] = curDepth; depth(root.left, max, curDepth + 1); depth(root.right, max, curDepth + 1); } "});index.add({'id':75,'href':'/interview/docs/offer/number-of-one/','title':"二进制中",'content':"题目 [](https://www.nowcoder.com/practice/8ee967e43c2c4ec193b040ea7fbb10b8?tpId=13\u0026amp;tqId=11164\u0026amp;rp=1\u0026amp;ru=%2Fta%2Fcoding-interviews\u0026amp;qru=%2Fta%2Fcoding-interviews%2Fquestion-ranking\u0026amp;tPage=1)\n输入一个整数，输出该数二进制表示中1的个数。其中负数用补码表示。\n解题思路  负数是补码表示 \u0026gt;\u0026gt;\u0026gt; 为无符号右移，\u0026gt;\u0026gt;为有符号右移，当 n 为负数是会增加多余的1  public int NumberOf1(int n) { int mask = 0x01; int res = 0; int t = n; while (t != 0) { if ((t \u0026amp; mask) == 1) { res++; } t = t \u0026gt;\u0026gt;\u0026gt; 1; } return res; } "});index.add({'id':76,'href':'/interview/docs/offer/PrintFromTopToBottom/','title':"从上往下打印二叉树",'content':"题目 牛客网\n从上往下打印出二叉树的每个节点，同层节点从左至右打印。\n解题思路  层次遍历，通过队列进行辅助遍历  public ArrayList\u0026lt;Integer\u0026gt; PrintFromTopToBottom(TreeNode root) { ArrayList\u0026lt;Integer\u0026gt; res = new ArrayList\u0026lt;\u0026gt;(); LinkedList\u0026lt;TreeNode\u0026gt; nodeQueue = new LinkedList\u0026lt;\u0026gt;(); if (root == null) { return res; } nodeQueue.addLast(root); while (!nodeQueue.isEmpty()) { TreeNode node = nodeQueue.pollFirst(); if (node == null) { continue; } nodeQueue.addLast(node.left); nodeQueue.addLast(node.right); res.add(node.val); } return res; } "});index.add({'id':77,'href':'/interview/docs/offer/print-link-from-tail/','title':"从尾到头打印链表",'content':"题目 牛客网\n输入一个链表，按链表值从尾到头的顺序返回一个ArrayList。\n解题思路  栈  public ArrayList\u0026lt;Integer\u0026gt; printListFromTailToHead(ListNode listNode) { LinkedList\u0026lt;Integer\u0026gt; stack = new LinkedList\u0026lt;\u0026gt;(); while (listNode != null) { stack.addLast(listNode.val); listNode = listNode.next; } ArrayList\u0026lt;Integer\u0026gt; res = new ArrayList\u0026lt;\u0026gt;(); while (!stack.isEmpty()) { res.add(stack.pollLast()); } return res; } 递归：当链表过长时，会导致栈溢出  public ArrayList\u0026lt;Integer\u0026gt; printListFromTailToHead(ListNode listNode) { ArrayList\u0026lt;Integer\u0026gt; res = new ArrayList\u0026lt;\u0026gt;(); print(res,listNode); return res; } private void print(ArrayList\u0026lt;Integer\u0026gt; res, ListNode listNode) { if (listNode == null) return; print(res, listNode.next); res.add(listNode.val); } "});index.add({'id':78,'href':'/interview/docs/leetcode/AllOne/','title':"全 O(1) 的数据结构",'content':"全 O(1) 的数据结构 题目 实现一个数据结构支持以下操作：\n Inc(key) - 插入一个新的值为 1 的 key。或者使一个存在的 key 增加一，保证 key 不为空字符串。 Dec(key) - 如果这个 key 的值是 1，那么把他从数据结构中移除掉。否者使一个存在的 key 值减一。如果这个 key 不存在，这个函数不做任何事情。key 保证不为空字符串。 GetMaxKey() - 返回 key 中值最大的任意一个。如果没有元素存在，返回一个空字符串\u0026rdquo;\u0026quot;。 GetMinKey() - 返回 key 中值最小的任意一个。如果没有元素存在，返回一个空字符串\u0026rdquo;\u0026quot;。  挑战：以 O(1) 的时间复杂度实现所有操作。\n解题思路  设计一个 Bucket 保存所有值为 value 的 key 并且有临近 value 的 Bucket 指针  class AllOne { /** Initialize your data structure here. */ public AllOne() { } private static class Bucket { private int value; private Set\u0026lt;String\u0026gt; keys = new HashSet\u0026lt;\u0026gt;(); private Bucket next; private Bucket pre; public Bucket(int value) { this.value = value; } @Override public String toString() { return \u0026quot;Bucket{\u0026quot; + \u0026quot;value=\u0026quot; + value + \u0026quot;, keys=\u0026quot; + keys + '}'; } } private Map\u0026lt;String, Bucket\u0026gt; data = new HashMap\u0026lt;\u0026gt;(); private List\u0026lt;Bucket\u0026gt; bucketList = new ArrayList\u0026lt;\u0026gt;(); /** * Inserts a new key \u0026lt;Key\u0026gt; with value 1. Or increments an existing key by 1. */ public void inc(String key) { if (data.containsKey(key)) { Bucket bucket = data.get(key); bucket.keys.remove(key); if (bucket.next == null) { bucket.next = new Bucket(bucket.value + 1); bucket.next.pre = bucket; bucketList.add(bucket.next); } bucket.next.keys.add(key); data.put(key, bucket.next); } else { if (bucketList.size() == 0) { bucketList.add(new Bucket(1)); } Bucket bucket = bucketList.get(0); bucket.keys.add(key); data.put(key, bucket); } } /** * Decrements an existing key by 1. If Key's value is 1, remove it from the data structure. */ public void dec(String key) { if (!data.containsKey(key)) { return; } Bucket bucket = data.get(key); if (bucket.pre == null) { bucket.keys.remove(key); data.remove(key); } else { bucket.keys.remove(key); bucket.pre.keys.add(key); data.put(key, bucket.pre); } } /** * Returns one of the keys with maximal value. */ public String getMaxKey() { if (bucketList.size() == 0) { return \u0026quot;\u0026quot;; } for (int i = bucketList.size() - 1; i \u0026gt;= 0; i--) { Bucket bucket = bucketList.get(i); if (!bucket.keys.isEmpty()) { Iterator\u0026lt;String\u0026gt; iterator = bucket.keys.iterator(); if (iterator.hasNext()) { return iterator.next(); } else { return \u0026quot;\u0026quot;; } } } return \u0026quot;\u0026quot;; } /** * Returns one of the keys with Minimal value. */ public String getMinKey() { if (bucketList.size() == 0) { return \u0026quot;\u0026quot;; } for (Bucket bucket : bucketList) { if (!bucket.keys.isEmpty()) { Iterator\u0026lt;String\u0026gt; iterator = bucket.keys.iterator(); if (iterator.hasNext()) { return iterator.next(); } else { return \u0026quot;\u0026quot;; } } } return \u0026quot;\u0026quot;; } } "});index.add({'id':79,'href':'/interview/docs/basic/os/memory/','title':"内存管理",'content':"内存管理 存储器工作原理 应用程序如何在计算机系统上运行的呢？首先，用编程语言编写和编辑应用程序，所编写的程序称为源程序，源程序不能再计算机上直接被运行，需要通过三个阶段的处理：编译程序处理源程序并生成目标代码，链接程序把他们链接为一个可重定位代码，此时该程序处于逻辑地址空间中；下一步装载程序将可执行代码装入物理地址空间，直到此时程序才能运行。\n程序编译 源程序经过编译程序的处理生成目标模块（目标代码）。一个程序可由独立编写且具有不同功能的多个源程序模块组成，由于模块包含外部引用（即指向其他模块中的数据或指令地址，或包含对库函数的引用），编译程序负责记录引用发生的位置，其处理结果将产生相应的多个目标模块，每个模块都附有供引用使用的内部符号表和外部符号表。符号表中依次给出各个符号名及在本目标模块中的名字地址，在模块链接时进行转换。\n程序链接 链接程序(Linker)的作用是根据目标模块之间的调用和依赖关系，将主调模块、被调模块以及所用到的库函数装配和链接成一个完整的可装载执行模块。根据程序链接发生的时间和链接方式，程序链接可分为以下三种方式：\n  静态链接：在程序装载到内存和运行前，就已将它所有的目标模块及所需要的库函数进行链接和装配成一个完整的可执行程序且此后不再拆分。\n  动态链接：在程序装入内存前并未事先进行程序各目标模块的链接，而是在程序装载时一边装载一边链接，生成一个可执行程序。在装载目标模块时，若发生外部模块调用，将引发响应外部目标模块的搜索、装载和链接。\n  运行时链接：在程序执行过程中，若发现被调用模块或库函数尚未链接，先在内存中进行搜索以查看是否装入内存；若已装入，则直接将其链接到调用程序中，否则进行该模块在外存上的搜索，以及装入内存和进行链接，生成一个可执行程序。\n  运行时链接将链接推迟到程序执行时，可以很好的提高系统资源的利用率和系统效率。\n程序装载 程序装载就是将可执行程序装入内存，这里有三种方式：\n  绝对装载：装载模块中的指令地址始终与其内存中的地址相同，即模块中出现的所有地址均为绝对地址。\n  可重定位装载：根据内存当时的使用情况，决定将装载代码模块放入内存的物理位置。模块内使用的都是相对地址。\n  动态运行时装载：为提高内存利用率，装入内存的程序可换出到磁盘上，适当时候再换入内存中，对换前后程序在内存中的位置可能不同，即允许进程的内存映像在不同时候处于不同位置，此时模块内使用的地址必定为相对地址。\n  磁盘中的装载模块所使用的是逻辑地址，其逻辑地址集合称为进程的逻辑地址空间。进程运行时，其装载代码模块将被装入物理地址空间中，此时程序和数据的实际地址不可能同原来的逻辑一致。可执行程序逻辑地址转换为物理地址的过程被称为 “地址重定位”。\n  静态地址重定位：由装载程序实现装载代码模块的加载和物理地址转换，把它装入分配给进程的内存指定区域，其中所有逻辑地址修改为物理地址。地址转换在进程执行前一次完成，易于实现，但不允许程序在执行过程中移动位置。\n  动态地址重定位：由装载程序实现装载代码模块的加载，把它装入分配给进程的内存指定区域，但对链接程序处理过的程序的逻辑地址不做任何改变，程序内存起始地址被置入硬件专用寄存器 —— 重定位寄存器。程序执行过程中，每当CPU引用内存地址时，有硬件截取此逻辑地址，并在它被发送到内存之前加上重定位寄存器的值，以实现地址转换。\n  运行时链接地址重定位：对于静态和动态地址重定位装载方式而言，装载代码模块是由整个程序的所有目标模块及库函数经链接和整合构成的可执行程序，即在程序启动执行前已经完成了程序的链接过程。可见，装载代码的正文结构是静态的，在程序运行期间保持不变。运行时链接装载方式必然采用运行时链接地址重定位。\n   重定位寄存器：用于保存程序内存起始地址。\n 连续存储管理 固定分区存储管理 固定分区存储管理又称为静态分区模式，基本思想是：内存空间被划分成数目固定不变的分区，各分区大小不等，每个分区装入一个作业，若多个分区中都有作业，则他们可以并发执行。\n为说明各分区分配和使用情况，需要设置一张内存分配表，记录内存中划分的分区及其使用情况。内存分配表中指出各分区起始地址和长度，占用标志用来指示此分区是否被使用。\n可变分区存储管理 可变分区存储管理按照作业大小来划分分区，但划分的时间、大小、位置都是动态的。系统把作业装入内存时，根据其所需要的内存容量查看是否有足够空间，若有则按需分割一个分区分配给此作业；若无则令此作业等待内存资源。\n在可变分区模式下，内存中分区数目和大小随作业的执行而不断改变，为了方便内存空间的分配和去配，用于管理的数据结构可由两张表组成：已分配区表和未分配区表。当装入新作业时，从未分配区表中找出一个足够容纳它的空闲区，将此区分为两个部分，一部分用来装入作业，成为已分配区；另一部分仍是空闲区（若有）。这时，应从已分配区表中找出一个空栏目登记新作业的起始地址、占用长度，同时修改未分配区表中空闲区的长度和起始地址。当作业撤离时，已分配区表中的相应状态改为空闲，而将收回的分区登记到为分配区中，若有相邻空闲区再将其连接后登记。\n常用的可变分区分配算法   最先适应分配算法：该算法顺序查找未分配区表，直到找到第一个能满足长度要求的空闲区为止，分割此分区，一部分分配给作业，另一部分仍为空闲区。\n  下次适应分配算法：该算法总是从未分配区的上次扫描结束处顺序查找未分配区表，直到找到第一个能满足长度要求的空闲区为止。\n  最优适应分配算法：该算法扫描整个未分配区表，从空闲区中挑选一个能满足用户进程要求的最小分区进行分配。\n  最坏适应分配算法：该算法扫描整个未分配区表，总是挑选一个最大的空闲区分割给作业使用，其优点是使剩下的空闲区不至于过小。\n  快速适应分配算法：该算法为那些经常用到的长度的空闲区设立单独的空闲区链表。\n  分页存储管理  逻辑空间等分为页；并从0开始编号 内存空间等分为块，与页面大小相同；从0开始编号 分配内存时，以块为单位将进程中的若干个页分别装入到多个可以不相邻接的物理块中。  分段存储管理  逻辑空间分为若干个段，每个段定义了一组有完整逻辑意义的信息（如主程序Main()）。 内存空间为每个段分配一个连续的分区。  分页和分段的主要区别：\n 分页：信息的物理单位。大小一样，由系统决定。地址空间是一维的。 分段：信息的逻辑单位。大小不一样，由程序员决定。地址空间是二维的。  段页式存储管理 用户程序先分段，每个段内部再分页（内部原理同基本的分页、分段相同）\n内存分配   虚拟地址：用户编程时将代码（或数据）分成若干个段，每条代码或每个数据的地址由段名称 + 段内相对地址构成，这样的程序地址称为虚拟地址\n  逻辑地址：虚拟地址中，段内相对地址部分称为逻辑地址\n  物理地址：实际物理内存中所看到的存储地址称为物理地址\n  逻辑地址空间：在实际应用中，将虚拟地址和逻辑地址经常不加区分，通称为逻辑地址。逻辑地址的集合称为逻辑地址空间\n  线性地址空间：CPU地址总线可以访问的所有地址集合称为线性地址空间\n  物理地址空间：实际存在的可访问的物理内存地址集合称为物理地址空间\n  MMU(Memery Management Unit内存管理单元)：实现将用户程序的虚拟地址（逻辑地址） -\u0026gt; 物理地址映射的CPU中的硬件电路\n  基地址：在进行地址映射时，经常以段或页为单位并以其最小地址（即起始地址）为基值来进行计算\n  偏移量：在以段或页为单位进行地址映射时，相对于基地址的地址值\n  虚拟地址先经过分段机制映射到线性地址，然后线性地址通过分页机制映射到物理地址。\n虚拟内存\u0026ndash;请求分页虚拟存储管理 请求调页，也称按需调页，即对不在内存中的“页”，当进程执行时要用时才调入，否则有可能到程序结束时也不会调入。\n页面置换算法   FIFO算法：先入先出，即淘汰最早调入的页面。\n  OPT(MIN)算法：选未来最远将使用的页淘汰，是一种最优的方案，可以证明缺页数最小。可惜，MIN需要知道将来发生的事，只能在理论中存在，实际不可应用。\n  LRU(Least-Recently-Used)算法：用过去的历史预测将来，选最近最长时间没有使用的页淘汰(也称最近最少使用)。性能最接近OPT。与页面使用时间有关。\n  LFU(Least Frequently Used)算法：即最不经常使用页置换算法，要求在页置换时置换引用计数最小的页，因为经常使用的页应该有一个较大的引用次数。与页面使用次数有关。\n  Clock：给每个页帧关联一个使用位，当该页第一次装入内存或者被重新访问到时，将使用位置为1。每次需要替换时，查找使用位被置为0的第一个帧进行替换。在扫描过程中，如果碰到使用位为1的帧，将使用位置为0，在继续扫描。如果所谓帧的使用位都为0，则替换第一个帧。\n  内存抖动现象：页面的频繁更换，导致整个系统效率急剧下降，这个现象称为内存抖动（或颠簸）。抖动一般是内存分配算法不好，内存太小引或者程序的算法不佳引起的。\n交换区：用于保存请求分页淘汰出来的页面。\n"});index.add({'id':80,'href':'/interview/docs/architecture/distributed/session/','title':"分布式 Session",'content':"分布式 Session 当一个带有会话表示的 Http 请求到 Web 服务器后，需求在请求中的处理过程中找到 session 数据。而问题就在于， session 是保存在单机上的。 假设我们有应用A和应用B，现在一位用户第一次访问网站， session 数据保存在 应用A 中。如果我们不做处理，怎么保障接下来的请求每次都请求到 应用A 呢? 如请求到了 应用B 中，就会发现没有这位用户的 session 数据，这绝对是不能容忍的。\n解决方案有Session Stick，Session复制，Session集中管理，基于Cookie管理，下面一一说明。\nSession Stick 在单机情况， session 保存在单机上，请求也是到这台单机上，不会有问题。变成多台后，如果能保障每次请求都到同一台服务，那就和单机一样了。 这需要在负载均衡设备上修改。这就是 Session Stick ，这种方式也会有问题：\n 如果某一台服务器宕机或重启，那么这台服务器上的 session 数据就丢失了。如果 session 数据中还有登录状态信息，那么用户需要重现登录。 负载均衡要处理具体的 session 到服务器的映射。  Session复制 Session 复制顾名思义，就是每台应用服务，都保存会话 session 数据，一般的应用容器都支持。与 Session Stick 相比， sessioon 复制对负载均衡 没有太多的要求。不过这个方案还是有缺点：\n 同步 session 数据带来都网络开销。只要 session 数据变化，就需要同步到所有机器上，机器越多，网络开销越大。 由于每台服务器都保存 session 数据，如果集群的 session 数据很多，比如 90万 人在访问网站，每台机器用于保存 session 数据的内容占用很严重。  这就是 Session 复制，这个方案是靠应用容器来完成，并不依赖应用，如果应用服务数量并不是很多，可以考虑。\nSession集中管理 这个也很好理解，再加一台服务，专门来管理 session 数据，每台应用服务都从专门的 session 管理服务中取会话 session 数据。可以使用数据库，NOSQL数据库等。 和Session复制相比，减少了每台应用服务的内存使用，同步session带来的网络开销问题。但还是有缺点：\n 读写 session 引入了网络操作，相对于本机读写 session ，带来了延时和不稳定性。 如 Session 集中服务有问题，会影响应用。  基于Cookie管理 最后一个是基于 Cookie 管理，我们把 session 数据存放在 cookie 中，然后请求过来后，从 cookie 中获取 session 数据。与集中管理相比，这个方案并不依赖外部 的存储系统，读写 session 数据带来的网络操作延时和不稳定性。但依然有缺点：\n Cookie有长度限制，这会影响session数据的长度。 安全性：session数据本来存储在服务端的，而这个方案是让 session 数据转到外部网络或客户端中，所以会有安全性问题。不过可以对写入 Cookie 的 session 数据做加密。 带宽消耗：由于加了session数据，带宽当然也会增加一点。 性能消耗：每次Http请求和响应都带有Session数据，对于Web服务器来说，在同样的处理情况下，响应的结果输出越少，支持的并发请求越多。  "});index.add({'id':81,'href':'/interview/docs/architecture/distributed/transaction/','title':"分布式事务",'content':"分布式事务 系统之间的通信可靠性从单一系统中的可靠变成了微服务架构之间的不可靠，分布式事务其实就是在不可靠的通信下实现事务的特性。无论是事务还是分布式事务实现原子性都无法避免对持久存储的依赖，事务使用磁盘上的日志记录执行的过程以及上下文，这样无论是需要回滚还是补偿都可以通过日志追溯，而分布式事务也会依赖 数据库、Zookeeper 或者 ETCD 等服务追踪事务的执行过程，总而言之，各种形式的日志是保证事务几大特性的 重要 手段。\n2PC 与 3PC 2PC 两阶段提交的执行过程就跟它的名字一样分为两个阶段，投票阶段和提交阶段，在投票阶段中，协调者（Coordinator）会向事务的参与者（Cohort）询问是否可以执行操作的请求，并等待其他参与者的响应，参与者会执行相对应的事务操作并 记录重做和回滚日志，所有执行成功的参与者会向协调者发送 AGREEMENT 或者 ABORT 表示执行操作的结果。\n当所有的参与者都返回了确定的结果（同意或者终止）时，两阶段提交就进入了提交阶段，协调者会根据投票阶段的返回情况向所有的参与者发送提交或者回滚的指令。\n当事务的所有参与者都决定提交事务时，协调者会向参与者发送 COMMIT 请求，参与者在完成操作并释放资源之后向协调者返回完成消息，协调者在收到所有参与者的完成消息时会结束整个事务；与之相反，当有参与者决定 ABORT 当前事务时，协调者会向事务的参与者发送回滚请求，参与者会根据之前执行操作时的回滚日志对操作进行回滚并向协调者发送完成的消息，在提交阶段，无论当前事务被提交还是回滚，所有的资源都会被释放并且事务也一定会结束。\n两阶段提交协议是一个阻塞协议，也就是说在两阶段提交的执行过程中，除此之外，如果事务的执行过程中协调者永久宕机，事务的一部分参与者将永远无法完成事务，它们会等待协调者发送 COMMIT 或者 ROLLBACK 消息，甚至会出现多个参与者状态不一致的问题。\n3PC 为了解决两阶段提交在协议的一些问题，三阶段提交引入了超时机制和准备阶段，如果协调者或者参与者在规定的之间内没有接受到来自其他节点的响应，就会根据当前的状态选择提交或者终止整个事务，准备阶段的引入其实让事务的参与者有了除回滚之外的其他选择。\n当参与者向协调者发送 ACK 后，如果长时间没有得到协调者的响应，在默认情况下，参与者会自动将超时的事务进行提交，不会像两阶段提交中被阻塞住；上述的图片非常清楚地说明了在不同阶段，协调者或者参与者的超时会造成什么样的行为。\n消息服务 分布式事务带来复杂度的原因其实就是由于各个模块之间的通信不稳定，当我们发出一个网络请求时，可能的返回结果是成功、失败或者超时。\n网络无论是返回成功还是失败其实都是一个确定的结果，当网络请求超时的时候其实非常不好处理，在这时调用方并不能确定这一次请求是否送达而且不会知道请求的结果，但是 消息服务 可以保证某条信息一定会送达到调用方；大多数消息服务都会提供两种不同的 QoS ，也就是服务的等级。\n最常见的两种服务等级就是 At-Most-Once 和 At-Least-Once 。\n At-Most-Once：能够保证发送方不对接收方是否能收到消息作保证，消息要么会被投递一次，要么不会被投递，这其实跟一次普通的网络请求没有太多的区别； At-Least-Once：能够解决消息投递失败的问题，它要求发送者检查投递的结果，并在失败或者超时时重新对消息进行投递，发送者会持续对消息进行推送，直到接受者确认消息已经被收到   相比于 At-Most-Once，At-Least-Once 因为能够确保消息的投递会被更多人使用。\n 除了这两种常见的服务等级之外，还有另一种服务等级，也就是 Exactly-Once，这种服务等级不仅对发送者提出了要求，还对消费者提出了要求，它需要接受者对接收到的所有消息进行去重，发送者和接受者一方对消息进行重试，另一方对消息进行去重，两者分别部署在不同的节点上，这样对于各个节点上的服务来说，它们之间的通信就是 Exactly-Once 的，但是需要注意的是，Exacly-Once 一定需要接收方的参与。\n使用消息服务实现分布式事务在底层的原理上与其他的方法没有太多的差别，只是 消息服务能够帮助我们实现的消息的持久化以及重试等功能，能够为我们提供一个比较合理的 API 接口，方便开发者使用。\n"});index.add({'id':82,'href':'/interview/docs/architecture/distributed/cache/','title':"分布式缓存",'content':"分布式缓存 高并发环境下，例如典型的淘宝双11秒杀，几分钟内上亿的用户涌入淘宝，这个时候如果访问不加拦截，让大量的读写请求涌向数据库，由于磁盘的处理速度与内存显然不在一个量级，服务器马上就要宕机。从减轻数据库的压力和提高系统响应速度两个角度来考虑，都会在数据库之前加一层缓存，访问压力越大的，在缓存之前就开始 CDN 拦截图片等访问请求。\n并且由于最早的单台机器的内存资源以及承载能力有限，如果大量使用本地缓存，也会使相同的数据被不同的节点存储多份，对内存资源造成较大的浪费，因此，才催生出了分布式缓存。\n应用场景  页面缓存：用来缓存Web 页面的内容片段,包括HTML、CSS 和图片等; 应用对象缓存：缓存系统作为ORM 框架的二级缓存对外提供服务,目的是减轻数据库的负载压力,加速应用访问;解决分布式Web部署的 session 同步问题，状态缓存.缓存包括Session 会话状态及应用横向扩展时的状态数据等,这类数据一般是难以恢复的,对可用性要求较高,多应用于高可用集群。 并行处理：通常涉及大量中间计算结果需要共享; 云计算领域提供分布式缓存服务  常见问题和挑战 缓存雪崩 缓存雪崩我们可以简单的理解为：由于原有缓存失效、新缓存未到之间(例如：我们设置缓存时采用了相同的过期时间，在同一时刻出现大面积的缓存过期)，所有原本应该访问缓存的请求都去查询数据库了，而对数据库CPU和内存造成巨大压力，严重的会造成数据库宕机。从而形成一系列连锁反应，造成整个系统崩溃。\n缓存穿透 缓存穿透是指用户查询数据，在数据库没有，自然在缓存中也不会有。这样就导致用户查询的时候，在缓存中找不到，每次都要去数据库再查询一遍，然后返回空（相当于进行了两次无用的查询）。这样请求就绕过缓存直接查数据库，这也是经常提的缓存命中率问题。\n缓存预热 缓存预热这个应该是一个比较常见的概念，相信很多小伙伴都应该可以很容易的理解，缓存预热就是系统上线后，将相关的缓存数据直接加载到缓存系统。这样就可以避免在用户请求的时候，先查询数据库，然后再将数据缓存的问题！用户直接查询事先被预热的缓存数据！\n缓存更新 除了缓存服务器自带的缓存失效策略之外，我们还可以根据具体的业务需求进行自定义的缓存淘汰，常见的策略有两种：\n 定时去清理过期的缓存； 当有用户请求过来时，再判断这个请求所用到的缓存是否过期，过期的话就去底层系统得到新数据并更新缓存。  两者各有优劣，第一种的缺点是维护大量缓存的key是比较麻烦的，第二种的缺点就是每次用户请求过来都要判断缓存失效，逻辑相对比较复杂！具体用哪种方案，大家可以根据自己的应用场景来权衡。\n缓存降级 当访问量剧增、服务出现问题（如响应时间慢或不响应）或非核心服务影响到核心流程的性能时，仍然需要保证服务还是可用的，即使是有损服务。系统可以根据一些关键数据进行自动降级，也可以配置开关实现人工降级。\n降级的最终目的是 保证核心服务可用，即使是有损的。而且有些服务是无法降级的（如加入购物车、结算）。\n在进行降级之前要对系统进行梳理，看看系统是不是可以丢卒保帅；从而梳理出哪些必须誓死保护，哪些可降级；比如可以参考日志级别设置预案：\n 一般：比如有些服务偶尔因为网络抖动或者服务正在上线而超时，可以自动降级； 警告：有些服务在一段时间内成功率有波动（如在95~100%之间），可以自动降级或人工降级，并发送告警； 错误：比如可用率低于90%，或者数据库连接池被打爆了，或者访问量突然猛增到系统能承受的最大阀值，此时可以根据情况自动降级或者人工降级； 严重错误：比如因为特殊原因数据错误了，此时需要紧急人工降级。  缓存与数据库不一致问题 首先，缓存由于其高并发和高性能的特性，已经在项目中被广泛使用。在读取缓存方面，大家没啥疑问，都是按照下图的流程来进行业务操作。\n但是在更新缓存方面，对于更新完数据库，是更新缓存呢，还是删除缓存。又或者是先删除缓存，再更新数据库，其实大家存在很大的争议。\n从理论上来说，给 缓存设置过期时间，是保证最终一致性的解决方案。这种方案下，我们可以对存入缓存的数据设置过期时间，所有的写操作以数据库为准，对缓存操作只是尽最大努力即可。也就是说如果数据库写成功，缓存更新失败，那么只要到达过期时间，则后面的读请求自然会从数据库中读取新值然后回填缓存。\n先删除缓存，再更新数据库 该方案会导致不一致的原因是。同时有一个请求A进行更新操作，另一个请求B进行查询操作。那么会出现如下情形:\n 请求A进行写操作，删除缓存 请求B查询发现缓存不存在 请求B去数据库查询得到旧值 请求B将旧值写入缓存 请求A将新值写入数据库  上述情况就会导致不一致的情形出现。而且，如果不采用给缓存设置过期时间策略，该数据永远都是脏数据。\n可以通过：\n 更新操作数据库后，再次更新缓存来实现 缓存设置过期时间，等待过期时间后，数据恢复  "});index.add({'id':83,'href':'/interview/docs/architecture/distributed/lock/','title':"分布式锁",'content':"分布式锁 实现基于数据库的乐观锁 提交数据更新之前，每个事务会先检查在该事务读取数据后，有没有其他事务又修改了该数据。如果其他事务有更新的话，正在提交的事务会进行回滚。\nConnection conn = DriverManager.getConnection(url, user, password); conn.setAutoCommit(false); Statement stmt = conn.createStatement(); // step 1 int oldVersion = getOldVersion(stmt); // step 2 // 用这个数据库连接做其他的逻辑 // step 3 可用预编译语句 int i = stmt.executeUpdate( \u0026quot;update optimistic_lock set version = \u0026quot; + (oldVersion + 1) + \u0026quot; where version = \u0026quot; + oldVersion); // step 4 if (i \u0026gt; 0) { conn.commit(); // 更新成功表明数据没有被修改，提交事务。 } else { conn.rollback(); // 更新失败，数据被修改，回滚。 } 乐观锁的缺点：\n 会带来大数量的无效更新请求、事务回滚，给DB造成不必要的额外压力。 无法保证先到先得，后面的请求可能由于并发压力小了反而有可能处理成功。  基于 Redis 的分布式锁 Redis\n"});index.add({'id':84,'href':'/interview/docs/offer/CutRope/','title':"剪绳子",'content':"题目 给定一根长度为n的绳子，请把绳子剪成m段（m、n都是整数，n\u0026gt;1并且m\u0026gt;1），每段绳子的长度记为k[0],k[1],…,k[m]。请问k[0]* k[1] * … *k[m]可能的最大乘积是多少？\n解题思路  尽可能剪长度为 3 的绳子 当长度剩下的为 4 时，不能再减去 3，而是 2*2  public int cutRope(int n) { if (n \u0026lt; 2) return 0; if (n == 2) return 1; if (n == 3) return 2; int timesOf3 = n / 3; if (n - timesOf3 * 3 == 1) { timesOf3 = 1; } int timesOf2 = (n - (timesOf3 * 3)) / 2; return (int) (Math.pow(3, timesOf3) * Math.pow((2), timesOf2)); } "});index.add({'id':85,'href':'/interview/docs/offer/MinStack/','title':"包含",'content':"题目 牛客网\n定义栈的数据结构，请在该类型中实现一个能够得到栈中所含最小元素的 min 函数（时间复杂度应为O（1））。\n解题思路  通过增加最小栈来记录当前最小节点  private LinkedList\u0026lt;Integer\u0026gt; stack = new LinkedList\u0026lt;\u0026gt;(); private LinkedList\u0026lt;Integer\u0026gt; min = new LinkedList\u0026lt;\u0026gt;(); public void push(int node) { stack.addLast(node); if (min.isEmpty()) { min.addLast(node); return; } if (node \u0026lt; min.peekLast()) { min.addLast(node); } else { min.addLast(min.peekLast()); } } public void pop() { if (stack.isEmpty()) { return; } stack.removeLast(); min.removeLast(); } public int top() { if (stack.peekLast() == null) { return 0; } return stack.peekLast(); } public int min() { if (min.peekLast() == null) { return 0; } return min.peekLast(); } "});index.add({'id':86,'href':'/interview/docs/offer/Singleton/','title':"单例",'content':"题目 设计一个类，我们只能生成该类的一个实例\n解题思路  线程安全 延迟加载 序列化与反序列化安全  /** * 需要额外的工作(Serializable、transient、readResolve())来实现序列化，否则每次反序列化一个序列化的对象实例时都会创建一个新的实例。 * \u0026lt;p\u0026gt; * 可能会有人使用反射强行调用我们的私有构造器（如果要避免这种情况，可以修改构造器，让它在创建第二个实例的时候抛异常）。 * * @author haoyang.shi */ public class Singleton { private Singleton() { } public static Singleton getInstance() { return Holder.instance; } private static final class Holder { private static Singleton instance = new Singleton(); } } /** * 使用枚举除了线程安全和防止反射强行调用构造器之外，还提供了自动序列化机制，防止反序列化的时候创建新的对象。 * \u0026lt;p\u0026gt; * 因此，Effective Java推荐尽可能地使用枚举来实现单例。 */ enum SingletonEnum { INSTANCE; private String name; public String getName() { return name; } public void setName(String name) { this.name = name; } } "});index.add({'id':87,'href':'/interview/docs/leetcode/reverseList/','title':"反转链表",'content':"头条重点\n题目 反转一个单链表。\n示例: 输入: 1-\u0026gt;2-\u0026gt;3-\u0026gt;4-\u0026gt;5-\u0026gt;NULL 输出: 5-\u0026gt;4-\u0026gt;3-\u0026gt;2-\u0026gt;1-\u0026gt;NULL 解题思路  三个指针进行反转  public ListNode reverseList(ListNode head) { if (head == null) { return head; } if (head.next == null) { return head; } ListNode pre = head; ListNode cur = head.next; while (cur != null) { ListNode next = cur.next; cur.next = pre; pre = cur; cur = next; } head.next = null; return pre; } "});index.add({'id':88,'href':'/interview/docs/offer/revert-link/','title':"反转链表",'content':"题目 牛客网\n输入一个链表，反转链表后，输出新链表的表头。\n解题思路  三个指针  public ListNode ReverseList(ListNode head) { if (head == null || head.next == null) { return head; } ListNode pre = head, cur = head.next, next; pre.next = null; while (cur != null) { next = cur.next; cur.next = pre; pre = cur; cur = next; } return pre; } "});index.add({'id':89,'href':'/interview/docs/leetcode/mergeKLists/','title':"合并",'content':"头条重点\n题目 合并 k 个排序链表，返回合并后的排序链表。请分析和描述算法的复杂度。\n示例: 输入: [ 1-\u0026gt;4-\u0026gt;5, 1-\u0026gt;3-\u0026gt;4, 2-\u0026gt;6 ] 输出: 1-\u0026gt;1-\u0026gt;2-\u0026gt;3-\u0026gt;4-\u0026gt;4-\u0026gt;5-\u0026gt;6 解题思路  通过小根堆，将所有元素放入小根堆 从小根堆依次取出数据  public ListNode mergeKLists(ListNode[] lists) { if (lists == null) { return null; } Queue\u0026lt;ListNode\u0026gt; set = new PriorityQueue\u0026lt;\u0026gt;(Comparator.comparingInt(o -\u0026gt; o.val)); for (ListNode node : lists) { while (node != null) { set.add(node); node = node.next; } } ListNode head = new ListNode(-1); ListNode res = head; ListNode cur; while ((cur = set.poll()) != null) { head.next = cur; head = head.next; } head.next = null; return res.next; } "});index.add({'id':90,'href':'/interview/docs/offer/merge-sort-link/','title':"合并两个排序的链表",'content':"题目 牛客网\n输入两个单调递增的链表，输出两个链表合成后的链表，当然我们需要合成后的链表满足单调不减规则。\n解题思路  双指针指向两个链表 循环选取最小值，加入结果集  public ListNode Merge(ListNode list1, ListNode list2) { ListNode head = new ListNode(-1); ListNode cursor = head; while (list1 != null || list2 != null) { if (list1 == null) { while (list2 != null) { cursor.next = list2; cursor = cursor.next; list2 = list2.next; } continue; } if (list2 == null) { while (list1 != null) { cursor.next = list1; cursor = cursor.next; list1 = list1.next; } continue; } if (list1.val \u0026lt; list2.val) { cursor.next = list1; cursor = cursor.next; list1 = list1.next; } else { cursor.next = list2; cursor = cursor.next; list2 = list2.next; } } return head.next; } "});index.add({'id':91,'href':'/interview/docs/leetcode/mergeTwoLists/','title':"合并两个有序链表",'content':"题目 将两个有序链表合并为一个新的有序链表并返回。新链表是通过拼接给定的两个链表的所有节点组成的。\n示例： 输入：1-\u0026gt;2-\u0026gt;4, 1-\u0026gt;3-\u0026gt;4 输出：1-\u0026gt;1-\u0026gt;2-\u0026gt;3-\u0026gt;4-\u0026gt;4 解题思路 public ListNode mergeTwoLists(ListNode l1, ListNode l2) { if (l1 == null \u0026amp;\u0026amp; l2 == null) { return null; } if (l1 == null) { return l2; } if (l2 == null) { return l1; } ListNode head; if (l1.val \u0026gt; l2.val) { head = l2; l2 = l2.next; } else { head = l1; l1 = l1.next; } ListNode res = head; while (true) { ListNode cur; if (l1 == null \u0026amp;\u0026amp; l2 == null) { break; } if (l1 == null) { cur = l2; l2 = l2.next; } else if (l2 == null) { cur = l1; l1 = l1.next; } else if (l1.val \u0026gt; l2.val) { cur = l2; l2 = l2.next; } else { cur = l1; l1 = l1.next; } head.next = cur; head = head.next; } return res; } "});index.add({'id':92,'href':'/interview/docs/leetcode/mergeRagen/','title':"合并区间",'content':"题目 给出一个区间的集合，请合并所有重叠的区间。\n示例 1: 输入: [[1,3],[2,6],[8,10],[15,18]] 输出: [[1,6],[8,10],[15,18]] 解释: 区间 [1,3] 和 [2,6] 重叠, 将它们合并为 [1,6]. 示例 2: 输入: [[1,4],[4,5]] 输出: [[1,5]] 解释: 区间 [1,4] 和 [4,5] 可被视为重叠区间。 解题思路  将区间按起始地址排序 遍历所有区间，如果 Last 与当前区间没有重合，则将当前区间加入结果集合。 如果重合，并且 last.end \u0026lt; t.end，修改 Last 的边界  public List\u0026lt;Interval\u0026gt; merge(List\u0026lt;Interval\u0026gt; intervals) { if (intervals.size() \u0026lt;= 1) { return intervals; } intervals.sort(Comparator.comparingInt(o -\u0026gt; o.start)); List\u0026lt;Interval\u0026gt; res = new ArrayList\u0026lt;\u0026gt;(); res.add(intervals.get(0)); for (int i = 1; i \u0026lt; intervals.size(); i++) { Interval t = intervals.get(i); Interval last = res.get(res.size() - 1); if (last.end \u0026gt;= t.start) { if (last.end \u0026lt; t.end) last.end = t.end; } else { res.add(t); } } return res; } "});index.add({'id':93,'href':'/interview/docs/offer/FindContinuousSequence/','title':"和为",'content':"题目 牛客网\n输出所有和为S的连续正数序列。序列内按照从小至大的顺序，序列间按照开始数字从小到大的顺序\n解题思路  与上一个题目类似，需要确定的是序列的最大值，不超过 sum 使用窗口模式，两个指针定义一个窗口，和为 t  public ArrayList\u0026lt;ArrayList\u0026lt;Integer\u0026gt;\u0026gt; FindContinuousSequence(int sum) { ArrayList\u0026lt;ArrayList\u0026lt;Integer\u0026gt;\u0026gt; res = new ArrayList\u0026lt;\u0026gt;(); if (sum == 1) { return res; } int start = 1, end = 2; int t = start + end; while (start \u0026lt; end) { if (t == sum) { ArrayList\u0026lt;Integer\u0026gt; ints = new ArrayList\u0026lt;\u0026gt;(); for (int i = start; i \u0026lt;= end; i++) { ints.add(i); } res.add(ints); t -= start; start++; } else if (t \u0026gt; sum) { t -= start; start++; } else { if (end \u0026gt;= sum) break; end++; t += end; } } return res; } "});index.add({'id':94,'href':'/interview/docs/offer/FindNumbersWithSum/','title':"和为",'content':"题目 牛客网\n输入一个递增排序的数组和一个数字S，在数组中查找两个数，使得他们的和正好是S，如果有多对数字的和等于S，输出两个数的乘积最小的。\n 对应每个测试案例，输出两个数，小的先输出。\n 解题思路  利用二分查找的思想，由于是排序数组，通过两个指针来进行遍历  public ArrayList\u0026lt;Integer\u0026gt; FindNumbersWithSum(int[] array, int sum) { ArrayList\u0026lt;Integer\u0026gt; res = new ArrayList\u0026lt;\u0026gt;(); if (array == null || array.length == 1) { return res; } int start = 0, end = array.length - 1; int minMulti = Integer.MAX_VALUE; int a = -1, b = -1; while (start \u0026lt; end) { int t = array[start] + array[end]; if (t == sum) { int multi = array[start] * array[end]; if (multi \u0026lt; minMulti) { a = array[start]; b = array[end]; minMulti = multi; } start++; end--; } else if (t \u0026gt; sum) end--; else start++; } if (a == -1 || b == -1) { return res; } res.add(a); res.add(b); return res; } "});index.add({'id':95,'href':'/interview/docs/basic/algo/hash/','title':"哈希",'content':"Hash 哈希表（Hash Table，也叫散列表），是根据关键码值 (Key-Value) 而直接进行访问的数据结构。也就是说，它通过把关键码值映射到表中一个位置来访问记录，以加快查找的速度。哈希表的实现主要需要解决两个问题，哈希函数和冲突解决。\n哈希函数 哈希函数也叫散列函数，它对不同的输出值得到一个固定长度的消息摘要。理想的哈希函数对于不同的输入应该产生不同的结构，同时散列结果应当具有同一性（输出值尽量均匀）和雪崩效应（微小的输入值变化使得输出值发生巨大的变化）。\n冲突解决  开放地址法：以发生冲突的哈希地址为输入，通过某种哈希冲突函数得到一个新的空闲的哈希地址的方法。有以下几种方式：  线性探查法：从发生冲突的地址开始，依次探查下一个地址，直到找到一个空闲单元。 平方探查法：设冲突地址为d0，则探查序列为：d0+1^2,d0-1^2,d0+2^2\u0026hellip;   拉链法：把所有的同义词用单链表链接起来。在这种方法下，哈希表每个单元中存放的不再是元素本身，而是相应同义词单链表的头指针。HashMap就是使用这种方法解决冲突的。  "});index.add({'id':96,'href':'/interview/docs/offer/LastRemaining/','title':"圆圈中最后剩下的数",'content':"题目 牛客网\n每年六一儿童节,牛客都会准备一些小礼物去看望孤儿院的小朋友,今年亦是如此。HF 作为牛客的资深元老,自然也准备了一些小游戏。其中,有个游戏是这样的:首先,让小朋友们围成一个大圈。然后,他随机指定一个数m,让编号为0的小朋友开始报数。每次喊到m-1的那个小朋友要出列唱首歌,然后可以在礼品箱中任意的挑选礼物,并且不再回到圈中,从他的下一个小朋友开始,继续0\u0026hellip;m-1报数\u0026hellip;.这样下去\u0026hellip;.直到剩下最后一个小朋友,可以不用表演,并且拿到牛客名贵的“名侦探柯南”典藏版。请你试着想下,哪个小朋友会得到这份礼品呢？(注：小朋友的编号是从 0 到 n-1 )\n解题思路 模拟 最简单直接的解法，但是时间效率不够\npublic int LastRemaining_Solution(int n, int m) { if (n == 1) return 1; LinkedList\u0026lt;Integer\u0026gt; data = new LinkedList\u0026lt;\u0026gt;(); for (int i = 0; i \u0026lt; n; i++) { data.addLast(i); } while (data.size() != 1) { for (int i = 0; i \u0026lt; m; i++) { Integer first = data.pollFirst(); if (i != m - 1) { data.addLast(first); } } } return data.pollFirst(); } 通过数学推导的解法 时间效率和空间效率都很高，但是。。。没看懂\n$$ f(n,m)= \\begin{cases} 0\u0026amp;n=1 \\\n[f(n-1,m)+m]%n \u0026amp; n\u0026gt;1 \\end{cases} $$\npublic int LastRemaining_Solution(int n, int m) { if (n == 0) return -1; if (n == 1) return 0; int last = 0; for (int i = 2; i \u0026lt;= n; i++) { last = (last + m) % i; } return last; } "});index.add({'id':97,'href':'/interview/docs/offer/O1DeleteNode/','title':"在",'content':"题目 给定单向链表的头指针以及待删除的指针，定义一个函数在 O(1) 的时间复杂度下删除\n解题思路  待删除节点非尾节点，将后一个节点的值复制到当前节点，然后删除后一个节点 待删除节点为尾节点，从头节点开始，找到待删除节点的前一个节点进行删除  public void O1DeleteNode(ListNode head, ListNode needDelete) { if (needDelete.next != null) { ListNode next = needDelete.next.next; needDelete.val = needDelete.next.val; needDelete.next = next; } else { ListNode cursor = head; while (cursor != null) { if (cursor.next == needDelete) break; cursor = cursor.next; } if (cursor == null) return; cursor.next = needDelete.next; } } "});index.add({'id':98,'href':'/interview/docs/offer/CountOfSortedArray/','title':"在排序数组中查找数字",'content':"题目 统计一个数字在排序数组中出现的次数。\n解题思路  通过二分查找分别找到 n 的第一个位置和最后一个位置 再进行计算就可以得出结果  public int countOfSortedArray2(int[] nums, int n) { if (nums == null || nums.length == 0) return 0; int firstN = getFirstN(nums, n); int lastN = getLastN(nums, n); return lastN - firstN + 1; } private int getFirstN(int[] nums, int n) { int s = 0, e = nums.length - 1; int mid = -1; while (s \u0026lt;= e) { mid = (s + e) / 2; if (mid \u0026gt; 0 \u0026amp;\u0026amp; nums[mid - 1] == n) { e = mid - 1; continue; } if (nums[mid] \u0026gt; n) { e = mid - 1; continue; } if (nums[mid] \u0026lt; n) { s = mid + 1; continue; } break; } return mid; } private int getLastN(int[] nums, int n) { int s = 0, e = nums.length - 1; int mid = -1; while (s \u0026lt;= e) { mid = (s + e) / 2; if (mid \u0026lt; nums.length - 1 \u0026amp;\u0026amp; nums[mid + 1] == n) { s = mid + 1; continue; } if (nums[mid] \u0026gt; n) { e = mid - 1; continue; } if (nums[mid] \u0026lt; n) { s = mid + 1; continue; } break; } return mid; } "});index.add({'id':99,'href':'/interview/docs/leetcode/restoreIpAddresses/','title':"复原",'content':"头条重点\n题目 给定一个只包含数字的字符串，复原它并返回所有可能的 IP 地址格式。\n示例: 输入: \u0026quot;25525511135\u0026quot; 输出: [\u0026quot;255.255.11.135\u0026quot;, \u0026quot;255.255.111.35\u0026quot;] 解题思路  利用回溯法，遍历所有可能的 IP  public static List\u0026lt;String\u0026gt; restoreIpAddresses(String s) { if (s.length() \u0026gt; 12 || s.length() \u0026lt; 4) { return Collections.emptyList(); } List\u0026lt;String\u0026gt; res = new ArrayList\u0026lt;\u0026gt;(); ArrayList\u0026lt;String\u0026gt; ip = new ArrayList\u0026lt;\u0026gt;(); for (int i = 0; i \u0026lt; 4; i++) { ip.add(\u0026quot;\u0026quot;); } p(res, s.toCharArray(), 0, ip, 0); return res; } private static void p(List\u0026lt;String\u0026gt; res, char[] chars, int startIndex, List\u0026lt;String\u0026gt; ip, int segmentIndex) { StringBuilder builder = new StringBuilder(); for (int i = startIndex; i \u0026lt; chars.length; i++) { builder.append(chars[i]); String ipStr = builder.toString(); int parseInt = Integer.parseInt(ipStr); if (parseInt \u0026gt; 255) { return; } if (ipStr.length() \u0026gt; 1 \u0026amp;\u0026amp; ipStr.startsWith(\u0026quot;0\u0026quot;)) { return; } ip.set(segmentIndex, ipStr); if (segmentIndex == 3 \u0026amp;\u0026amp; i == chars.length - 1) { res.add(String.join(\u0026quot;.\u0026quot;, ip)); } if (segmentIndex \u0026lt; 3) { p(res, chars, i + 1, ip, segmentIndex + 1); } } "});index.add({'id':100,'href':'/interview/docs/offer/CloneLink/','title':"复杂链表的复制",'content':"题目 牛客网\n输入一个复杂链表（每个节点中有节点值，以及两个指针，一个指向下一个节点，另一个特殊指针指向任意一个节点），返回结果为复制后复杂链表的 head 。（注意，输出结果中请不要返回参数中的节点引用，否则判题程序会直接返回空）\n解题思路  复制每个节点，如：复制节点 A 得到 A1 ，将 A1 插入节点 A 后面 遍历链表，并将 A1-\u0026gt;random = A-\u0026gt;random-\u0026gt;next; 将链表拆分成原链表和复制后的链表  public RandomListNode Clone(RandomListNode pHead) { if (pHead == null) { return null; } RandomListNode cursor = pHead; while (cursor != null) { RandomListNode copyNode = new RandomListNode(cursor.label); RandomListNode nextNode = cursor.next; cursor.next = copyNode; copyNode.next = nextNode; cursor = nextNode; } cursor = pHead; while (cursor != null) { RandomListNode copyNode = cursor.next; if (cursor.random == null) { cursor = copyNode.next; continue; } copyNode.random = cursor.random.next; cursor = copyNode.next; } RandomListNode copyHead = pHead.next; cursor = pHead; while (cursor.next != null) { RandomListNode copyNode = cursor.next; cursor.next = copyNode.next; cursor = copyNode; } return copyHead; } "});index.add({'id':101,'href':'/interview/docs/leetcode/checkInclusion/','title':"字符串的排列",'content':"字符串的排列 题目 给定两个字符串 s1 和 s2，写一个函数来判断 s2 是否包含 s1 的排列。\n换句话说，第一个字符串的排列之一是第二个字符串的子串。\n示例1: 输入: s1 = \u0026quot;ab\u0026quot; s2 = \u0026quot;eidbaooo\u0026quot; 输出: True 解释: s2 包含 s1 的排列之一 (\u0026quot;ba\u0026quot;). 解题思路  这道题，我们用到的算法是 滑动窗口 首先字符串s1的排列的可能性应该是它的长度的阶乘，因为字符串长度可能为10000，所以找出所有排列情况是不太可能。 我们可以转换思路，不要关注排列的形式，而是关注排列中元素的数量关系 比如 aab，那么，转换为数量关系就是{a:2,b:1}，因为 S1 长度为 3，所以我们的窗口长度也为3 如果我们在 S2 的找到了这样一个窗口符合出现 a 的次数是两个， b 是一个，那么 S2 就是包含 S1 的排列的  public boolean checkInclusion(String s1, String s2) { int len1 = s1.length(); int len2 = s2.length(); int[] c1 = new int[26]; int[] c2 = new int[26]; for (char c : s1.toCharArray()) { c1[c - 'a']++; } for (int i = 0; i \u0026lt; len2; i++) { if (i \u0026gt;= len1) --c2[s2.charAt(i - len1) - 'a'];//先把坐标查过的减掉 c2[s2.charAt(i) - 'a']++; if (Arrays.equals(c1, c2)) return true; } return false; } "});index.add({'id':102,'href':'/interview/docs/offer/Permutation/','title':"字符串的排列",'content':"题目 牛客网\n输入一个字符串,按字典序打印出该字符串中字符的所有排列。例如输入字符串abc,则打印出由字符a,b,c所能排列出来的所有字符串abc,acb,bac,bca,cab和cba。\n输入描述:输入一个字符串,长度不超过9(可能有字符重复),字符只包括大小写字母。\n解题思路  将字符串划分为两个部分，第一个字符以及后面的其他字符 将第一个字符和后面所有字符进行交换  对于 abc 这个字符串，计算出的排列顺序为：\nabc acb bac bca cba cab 代码：\npublic ArrayList\u0026lt;String\u0026gt; Permutation(String str) { Set\u0026lt;String\u0026gt; res = new HashSet\u0026lt;\u0026gt;(); if (str == null || str.length() == 0) { return new ArrayList\u0026lt;\u0026gt;(); } Permutation(res, str.toCharArray(), 0); ArrayList\u0026lt;String\u0026gt; list = new ArrayList\u0026lt;\u0026gt;(res); list.sort(String::compareTo); return list; } private void Permutation(Set\u0026lt;String\u0026gt; res, char[] chars, int start) { if (start == chars.length) { res.add(new String(chars)); return; } for (int i = start; i \u0026lt; chars.length; i++) { swap(chars, start, i); Permutation(res, chars, start + 1); swap(chars, start, i); } } private void swap(char[] chars, int i, int j) { char temp = chars[i]; chars[i] = chars[j]; chars[j] = temp; } "});index.add({'id':103,'href':'/interview/docs/leetcode/StringMultiply/','title':"字符串相乘",'content':"题目 给定两个以字符串形式表示的非负整数 num1 和 num2，返回 num1 和 num2 的乘积，它们的乘积也表示为字符串形式。\n示例 1: 输入: num1 = \u0026quot;2\u0026quot;, num2 = \u0026quot;3\u0026quot; 输出: \u0026quot;6\u0026quot;  num1 和 num2 的长度小于110。 num1 和 num2 只包含数字 0-9。 num1 和 num2 均不以零开头，除非是数字 0 本身。 不能使用任何标准库的大数类型（比如 BigInteger）或直接将输入转换为整数来处理。  解题思路   对于字符串 num2 中的每一位数与字符串 num1 相乘所得的结果，不再分开计算最后相加，而是先全部累加，最后再考虑进位的影响。\n  对于最终结果的第i + j位数，可以由 num1 数组的第 i 位数和 num2 数组的第 j 位数组成。\n  public String multiply(String num1, String num2) { if (num1.length() == 0 || num2.length() == 0) { return ZEO; } if (num1.equals(ZEO) || num2.equals(ZEO)) { return ZEO; } if (num1.equals(ONE)) { return num2; } if (num2.equals(ONE)) { return num1; } int[] num = new int[num1.length() + num2.length() - 1]; Arrays.fill(num, 0); for (int i = 0; i \u0026lt; num1.length(); i++) { for (int j = 0; j \u0026lt; num2.length(); j++) { num[i + j] += (num1.charAt(i) - '0') * (num2.charAt(j) - '0'); } } StringBuilder res = new StringBuilder(); int addIn = 0; for (int i = num.length - 1; i \u0026gt;= 0; i--) { int t = num[i] + addIn; addIn = t / 10; res.append(t % 10); } if (addIn \u0026gt; 0) { res.append(addIn); } return res.reverse().toString(); } "});index.add({'id':104,'href':'/interview/docs/basic/cryptology/','title':"密码学",'content':"密码学 对称加密 对称加密算法的加密和解密使用的密匙是相同的，也就是说如果通讯两方如果使用对称加密算法来加密通讯数据，那么通讯双方就需要都知道这个密匙，收到通讯数据后用这个密匙来解密数据。\n这类算法在加密和解密时使用相同的密钥，或是使用两个可以简单地相互推算的密钥。事实上，这组密钥成为在两个或多个成员间的共同秘密，以便维持专属的通信联系。与非对称加密相比，要求双方获取相同的密钥是对称密钥加密的主要缺点之一。常见的对称加密算法有 DES、3DES、AES、Blowfish、IDEA、RC5、RC6。\n对称加密的速度比公钥加密快很多，在很多场合都需要对称加密。\n非对称加密 它需要两个密钥，一个是公开密钥，另一个是私有密钥；一个用作加密的时候，另一个则用作解密。使用其中一个密钥把明文加密后所得的密文，只能用相对应的另一个密钥才能解密得到原本的明文；甚至连最初用来加密的密钥也不能用作解密。由于加密和解密需要两个不同的密钥，故被称为非对称加密；\n虽然两个密钥在数学上相关，但如果知道了其中一个，并不能凭此计算出另外一个；因此其中一个可以公开，称为 公钥，任意向外发布；不公开的密钥为 私钥 ，必须由用户自行严格秘密保管，绝不透过任何途径向任何人提供，也不会透露给要通信的另一方，即使他被信任。\n 公钥 \u0026amp; 私钥 均可以作为加密密钥\n 数字签名 数字签名是一种类似写在纸上的签名，但是使用了 公钥加密领域的技术实现 ，用于鉴别数字信息的方法。在网络上，我们可以使用“数字签名”来进行身份确认。数字签名是一个独一无二的数值，若公钥能通过验证，那我们就能确定对应的公钥的正确性，数字签名兼具这两种双重属性：\u0026ldquo;可确认性\u0026quot;及\u0026quot;不可否认性（不需要笔迹专家验证）\u0026quot;。\n数字签名就是将公钥密码反过来使用。签名者将讯息用私钥加密（这是一种反用，因为通常非对称加密中私钥用于解密），然后公布公钥;验证者使用公钥将加密讯息解密并比对消息（一般签名对象为消息的散列值）。\n密码散列函数 密码散列函数（英语：Cryptographic hash function），又译为加密散列函数、密码散列函数、加密散列函数，是散列函数的一种。它被认为是一种 单向函数，也就是说极其难以由散列函数输出的结果，回推输入的数据是什么。这种散列函数的输入数据，通常被称为消息（ message ），而它的输出结果，经常被称为消息摘要（ message digest ）或摘要（ digest ）。\n"});index.add({'id':105,'href':'/interview/docs/offer/IsSymmetrical/','title':"对称的二叉树",'content':"题目 请实现一个函数，用来判断一颗二叉树是不是对称的。注意，如果一个二叉树同此二叉树的镜像是同样的，定义其为对称的。\n解题思路  定义一个对称的前序遍历，即root -\u0026gt; right -\u0026gt; left 与普通的前序遍历进行对比 相同则认为树是对称的  boolean isSymmetrical(TreeNode pRoot) { LinkedList\u0026lt;Integer\u0026gt; scanner = new LinkedList\u0026lt;\u0026gt;(); LinkedList\u0026lt;Integer\u0026gt; symmetricalScanner = new LinkedList\u0026lt;\u0026gt;(); preScanner(scanner, pRoot); symmetricalPreScanner(symmetricalScanner, pRoot); return scanner.equals(symmetricalScanner); } /** * 普通的前序遍历 * @param res * @param root */ private void preScanner(LinkedList\u0026lt;Integer\u0026gt; res, TreeNode root) { if (root == null) { res.addLast(null); return; } res.addLast(root.val); preScanner(res, root.left); preScanner(res, root.right); } /** * 先右再左的前序遍历 * @param res * @param root */ private void symmetricalPreScanner(LinkedList\u0026lt;Integer\u0026gt; res, TreeNode root) { if (root == null) { res.addLast(null); return; } res.addLast(root.val); symmetricalPreScanner(res, root.right); symmetricalPreScanner(res, root.left); } "});index.add({'id':106,'href':'/interview/docs/java/gc/jvm-object-lifecycle/','title':"对象的生命周期",'content':"对象的生命周期 一旦一个类被装载、连接和初始化，它就随时可以被使用。程序可以访问它的静态字段，调用它的静态方法，或者创建它的实例。作为Java程序员有必要了解Java对象的生命周期。\n类实例化 在Java程序中，类可以被明确或隐含地实例化。明确的实例化类有四种途径：\n 明确调用new。 调用Class或者java.lang.reflect.Constructor对象的newInstance方法。 调用任何现有对象的clone。 通过java.io.ObjectInputStream.getObject()反序列化。  隐含的实例化：\n 可能是保存命令行参数的String对象。 对于Java虚拟机装载的每个类，都会暗中实例化一个Class对象来代表这个类型 当Java虚拟机装载了在常量池中包含CONSTANT_String_info入口的类的时候，它会创建新的String对象来表示这些常量字符串。 执行包含字符串连接操作符的表达式会产生新的对象。  Java编译器为它编译的每个类至少生成一个实例初始化方法。在Java class文件中，这个方法被称为\u0026lt;init\u0026gt;。针对源代码中每个类的构造方法，Java编译器都会产生一个\u0026lt;init\u0026gt;()方法。如果类没有明确的声明任何构造方法，编译器会默认产生一个无参数的构造方法，它仅仅调用父类的无参构造方法。\n一个\u0026lt;init\u0026gt;()中可能包含三种代码：调用另一个\u0026lt;init\u0026gt;()、实现对任何实例变量的初始化、构造方法体的代码。\n如果构造方法明确的调用了同一个类中的另一个构造方法(this())，那么它对应的\u0026lt;init\u0026gt;()由两部分组成：\n 一个同类的\u0026lt;init\u0026gt;()的调用。 实现了对应构造方法的方法体的字节码。   在它对应的\u0026lt;init\u0026gt;()方法中不会有父类的\u0026lt;init\u0026gt;()，但不代表不会调用父类的\u0026lt;init\u0026gt;()，因为this()中也会调用父类\u0026lt;init\u0026gt;()\n 如果构造方法不是通过一个this()调用开始的，而且这个对象不是Object，\u0026lt;init\u0026gt;()则有三部分组成：\n 一个父类的\u0026lt;init\u0026gt;()调用。如果这个类是Object,则没有这个部分 任意实例变量初始化方法的字节码。 实现了对应构造方法的方法体的字节码。  如果构造方法明确的调用父类的构造方法super()开始，它的\u0026lt;init\u0026gt;()会调用对应父类的\u0026lt;init\u0026gt;()。比如，如果一个构造方法明确的调用super(int,String)开始，对应的\u0026lt;init\u0026gt;()会从调用父类的\u0026lt;init\u0026gt;(int,String)方法开始。如果构造方法没有明确地从this()或super()开始，对应的\u0026lt;init\u0026gt;()默认会调用父类的无参\u0026lt;init\u0026gt;()。\n垃圾收集和对象的终结 程序可以明确或隐含的为对象分配内存，但不能明确的释放内存。一个对象不再为程序引用，虚拟机必须回收那部分内存。\n卸载类 在很多方面，Java虚拟机中类的生命周期和对象的生命周期很相似。当程序不再使用某个类的时候，可以选择卸载它们。\n 类的垃圾收集和卸载值所以在Java虚拟机中很重要，是因为Java程序可以在运行时通过用户自定义的类装载器装载类型来动态的扩展程序。所有被装载的类型都在方法区占据内存空间。\n Java虚拟机通过判断类是否在被引用来进行垃圾收集。判断动态装载的类的Class实例在正常的垃圾收集过程中是否可触及有两种方式：\n 如果程序保持非Class实例的明确引用。 如果在堆中还存在一个可触及的对象，在方法区中它的类型数据指向一个Class实例。  "});index.add({'id':107,'href':'/interview/docs/leetcode/maxAreaOfIsland/','title':"岛屿的最大面积",'content':"头条重点\n题目 给定一个包含了一些 0 和 1的非空二维数组 grid , 一个 岛屿 是由四个方向 (水平或垂直) 的 1 (代表土地) 构成的组合。你可以假设二维矩阵的四个边缘都被水包围着。\n找到给定的二维数组中最大的岛屿面积。(如果没有岛屿，则返回面积为0。)\n示例 1: [[0,0,1,0,0,0,0,1,0,0,0,0,0], [0,0,0,0,0,0,0,1,1,1,0,0,0], [0,1,1,0,1,0,0,0,0,0,0,0,0], [0,1,0,0,1,1,0,0,1,0,1,0,0], [0,1,0,0,1,1,0,0,1,1,1,0,0], [0,0,0,0,0,0,0,0,0,0,1,0,0], [0,0,0,0,0,0,0,1,1,1,0,0,0], [0,0,0,0,0,0,0,1,1,0,0,0,0]] 对于上面这个给定矩阵应返回 6。注意答案不应该是11，因为岛屿只能包含水平或垂直的四个方向的‘1’。 示例 2: [[0,0,0,0,0,0,0,0]] 对于上面这个给定的矩阵, 返回 0。 注意: 给定的矩阵 grid 的长度和宽度都不超过 50。 解题思路  通过循环遍历，找到 1 再通过递归遍历该 1 临近的所有 1，并计算总面积  private static int[][] steps = new int[][]{{1, 0}, {0, 1}, {-1, 0}, {0, -1}}; /** * 上学时做过，属于图的 DFS * @param grid * @return */ public static int maxAreaOfIsland(int[][] grid) { if (grid.length == 0) { return 0; } int[][] marks = new int[grid.length][grid[0].length]; for (int[] mark : marks) { Arrays.fill(mark, 0); } Wrapper\u0026lt;Integer\u0026gt; maxArea = new Wrapper\u0026lt;\u0026gt;(0); for (int i = 0; i \u0026lt; grid.length; i++) { for (int j = 0; j \u0026lt; grid[i].length; j++) { p(grid, marks, i, j, new Wrapper\u0026lt;\u0026gt;(0), maxArea); } } return maxArea.v; } private static void p(int[][] grid, int[][] mark, int i, int j, Wrapper\u0026lt;Integer\u0026gt; curArea, Wrapper\u0026lt;Integer\u0026gt; maxArea) { if (i \u0026lt; 0 || j \u0026lt; 0 || i \u0026gt;= grid.length || j \u0026gt;= grid[0].length) { return; } if (grid[i][j] == 1 \u0026amp;\u0026amp; mark[i][j] == 0) { curArea.v++; maxArea.v = Math.max(maxArea.v, curArea.v); } else { return; } for (int[] step : steps) { mark[i][j] = 1; p(grid, mark, i + step[0], j + step[1], curArea, maxArea); // mark[i][j] = 0; } } private static final class Wrapper\u0026lt;V\u0026gt; { V v; public Wrapper(V v) { this.v = v; } } "});index.add({'id':108,'href':'/interview/docs/offer/LeftRotateString/','title':"左旋转字符串",'content':"题目 牛客网\n汇编语言中有一种移位指令叫做循环左移（ROL），现在有个简单的任务，就是用字符串模拟这个指令的运算结果。对于一个给定的字符序列S，请你把其循环左移K位后的序列输出。例如，字符序列S=”abcXYZdef”,要求输出循环左移3位后的结果，即“XYZdefabc”。是不是很简单？OK，搞定它！\n解题思路  对于 abcXYZdef 左移 3位，可以将字符串分为两个部分：abc \u0026amp; XYZdef 分别将两个部分进行反转得到：cba \u0026amp; fedZYX 将两部分和在一起再进行反转：XYZdefabc  public String LeftRotateString(String str, int n) { if (str == null || str.trim().equals(\u0026quot;\u0026quot;)) return str; String res = revert(str, 0, n - 1); res = revert(res, n, str.length() - 1); res = revert(res, 0, str.length() - 1); return res; } private String revert(String str, int start, int end) { char[] chars = str.toCharArray(); while (start \u0026lt; end) { char t = chars[start]; chars[start] = chars[end]; chars[end] = t; start++; end--; } return new String(chars); } "});index.add({'id':109,'href':'/interview/docs/basic/os/concurrency/','title':"并发",'content':"并发 进程 进程是一个具有独立功能的程序关于某个数据集合的一次运行活动。它可以申请和拥有系统资源，是一个动态的概念，是一个活动的实体。它不只是程序的代码，还包括当前的活动，通过程序计数器的值和处理寄存器的内容来表示。\n进程的概念主要有两点：\n 进程是一个实体，每一个进程都有它自己的地址空间，一般情况下，包括文本区域（text region）、数据区域（data region）和堆栈（stack region）。文本区域存储处理器执行的代码；数据区域存储变量和进程执行期间使用的动态分配的内存；堆栈区域存储着活动过程调用的指令和本地变量。 进程是一个“执行中的程序”，程序是一个没有生命的实体，只有处理器赋予程序生命时，它才能成为一个活动的实体，我们称其为进程。  进程的基本状态  阻塞态：等待某个事件的完成； 就绪态：等待系统分配处理器以便运行； 执行态：占有处理器正在运行。   执行态 -\u0026gt; 阻塞态：往往是由于等待外设，等待主存等资源分配或等待人工干预而引起的。\n  阻塞态 -\u0026gt; 就绪态：则是等待的条件已满足，只需分配到处理器后就能运行。\n  执行态 -\u0026gt; 就绪态：不是由于自身原因，而是由外界原因使运行状态的进程让出处理器，这时候就变成就绪态。例如时间片用完，或有更高优先级的进程来抢占处理器等。\n  就绪态 -\u0026gt; 执行态：系统按某种策略选中就绪队列中的一个进程占用处理器，此时就变成了运行态\n 进程调度 调度种类 高级、中级和低级调度作业从提交开始直到完成，往往要经历下述三级调度：\n 高级调度：又称为作业调度，它决定把后备作业调入内存运行； 中级调度：又称为在虚拟存储器中引入，在内、外存对换区进行进程对换。 低级调度：又称为进程调度，它决定把就绪队列的某进程获得CPU；  非抢占式调度与抢占式调度   非抢占式：分派程序一旦把处理机分配给某进程后便让它一直运行下去，直到进程完成或发生进程调度进程调度某事件而阻塞时，才把处理机分配给另一个进程。\n  抢占式：操作系统将正在运行的进程强行暂停，由调度程序将CPU分配给其他就绪进程的调度方式。\n  调度策略的设计  响应时间：从用户输入到产生反应的时间 周转时间：从任务开始到任务结束的时间 平均周转时间：周转总时间除以作业个数  CPU任务可以分为交互式任务和批处理任务，调度最终的目标是合理的使用CPU，使得交互式任务的响应时间尽可能短，用户不至于感到延迟，同时使得批处理任务的周转时间尽可能短，减少用户等待的时间。\n调度算法  FCFS：调度的顺序就是任务到达就绪队列的顺序。对短作业不公平。   公平、简单(FIFO队列)、非抢占、不适合交互式。未考虑任务特性，平均等待时间可以缩短\n SJF：最短的作业(CPU区间长度最小)最先调度。   可以证明，SJF可以保证最小的平均等待时间。\n  SRJF：SJF的可抢占版本，比SJF更有优势。  SJF(SRJF): 如何知道下一CPU区间大小？根据历史进行预测: 指数平均法。\n HRN：最高响应比优先法，是FCFS和SJF的综合平衡，响应比R定义如下： R =(W+T)/T 。\n  优先权调度：每个任务关联一个优先权，调度优先权最高的任务。\n   注意：优先权太低的任务一直就绪，得不到运行，出现“饥饿”现象。\n FCFS是RR的特例，SJF是优先权调度的特例。这些调度算法都不适合于交互式系统。\nRound-Robin(RR)：设置一个时间片，按时间片来轮转调度  优点: 定时有响应，等待时间较短；缺点: 上下文切换次数较多；\n 时间片太大，响应时间太长；吞吐量变小，周转时间变长；当时间片过长时，退化为FCFS。\n 多级队列调度   按照一定的规则建立多个进程队列 不同的队列有固定的优先级（高优先级有抢占权） 不同的队列可以给不同的时间片和采用不同的调度方法   存在问题1：没法区分I/O bound和CPU bound；\n  存在问题2：也存在一定程度的“饥饿”现象；\n 多级反馈队列：在多级队列的基础上，任务可以在队列之间移动，更细致的区分任务。可以根据“享用”CPU时间多少来移动队列，阻止“饥饿”。   最通用的调度算法，多数OS都使用该方法或其变形，如UNIX、Windows等。\n 进程同步 临界资源与临界区 在操作系统中，进程是占有资源的最小单位（线程可以访问其所在进程内的所有资源，但线程本身并不占有资源或仅仅占有一点必须资源）。但对于某些资源来说，其在同一时间只能被一个进程所占用。这些一次只能被一个进程所占用的资源就是所谓的临界资源。典型的临界资源比如物理上的打印机，或是存在硬盘或内存中被多个进程所共享的一些变量和数据等(如果这类资源不被看成临界资源加以保护，那么很有可能造成丢数据的问题)。\n对于临界资源的访问，必须是互斥进行。也就是当临界资源被占用时，另一个申请临界资源的进程会被阻塞，直到其所申请的临界资源被释放。而进程内访问临界资源的代码被成为临界区。\n对于临界区的访问过程分为四个部分：\n 进入区:查看临界区是否可访问，如果可以访问，则转到步骤二，否则进程会被阻塞 临界区:在临界区做操作 退出区:清除临界区被占用的标志 剩余区：进程与临界区不相关部分的代码  解决临界区问题可能的方法：\n 一般软件方法 关中断方法 硬件原子指令方法 信号量方法  信号量 信号量是一个确定的二元组（s，q），其中s是一个具有非负初值的整形变量，q是一个初始状态为空的队列，整形变量s表示系统中某类资源的数目：\n 当其值 \u0026gt;= 0 时，表示系统中当前可用资源的数目 当其值 \u0026lt; 0 时，其绝对值表示系统中因请求该类资源而被阻塞的进程数目  除信号量的初值外，信号量的值仅能由P操作和V操作更改，操作系统利用它的状态对进程和资源进行管理\nP操作 P操作记为P(s)，其中s为一信号量，它执行时主要完成以下动作：\ns.value = s.value - 1； /*可理解为占用1个资源，若原来就没有则记帐“欠”1个*/ 若s.value ≥ 0，则进程继续执行，否则（即s.value \u0026lt; 0），则进程被阻塞，并将该进程插入到信号量s的等待队列s.queue中\n 实际上，P操作可以理解为分配资源的计数器，或是使进程处于等待状态的控制指令\n V操作 V操作记为V(s)，其中s为一信号量，它执行时，主要完成以下动作：\ns.value = s.value + 1；/*可理解为归还1个资源，若原来就没有则意义是用此资源还1个欠帐*/ 若s.value \u0026gt; 0，则进程继续执行，否则（即s.value ≤ 0），则从信号量s的等待队s.queue中移出第一个进程，使其变为就绪状态，然后返回原进程继续执行\n 实际上，V操作可以理解为归还资源的计数器，或是唤醒进程使其处于就绪状态的控制指令\n 锁  互斥锁：同一时间只能有一个线程访问加锁的数据。 自旋锁：互斥锁的一种实现，如果自旋锁已经被别的执行单元保持，调用者就一直 循环等待 是否该自旋锁的保持者已经释放了锁。 读写锁：一种特殊的自旋锁，它把对共享资源的访问者划分成读者和写者，读者只对共享资源进行读访问，写者则需要对共享资源进行写操作。写者是排他性的，一个读写锁同时只能有一个写者或多个读者（与CPU数相关），但不能同时既有读者又有写者。 阻塞锁：与自旋锁不同，改变了线程的运行状态。让线程进入阻塞状态进行等待，当获得相应的信号（唤醒，时间） 时，才可以进入线程的准备就绪状态，准备就绪状态的所有线程，通过竞争，进入运行状态。   在Java中synchronized,ReentrantLock,Object.wait() / notify()都属于阻塞锁。\n  可重入锁：也叫做递归锁，指的是同一线程上该锁是可重入的，对于不同线程则相当于普通的互斥锁。 公平锁：加锁前检查是否有排队等待的线程，优先排队等待的线程，先来先得。 非公平锁：加锁时不考虑排队等待问题，直接尝试获取锁，获取不到自动到队尾等待。ReentrantLock中的lock()默认就是非公平锁。 悲观锁：假定会发生并发冲突，屏蔽一切可能违反数据完整性的操作。加锁的时间可能会很长，也就是说悲观锁的并发访问性不好。 乐观锁：假设不会发生并发冲突，只在提交操作时检查是否违反数据完整性。乐观锁不能解决脏读的问题，可以通过添加时间戳和版本来来解决。  CAS 比较并交换(compare and swap, CAS)，是原子操作的一种，可用于在多线程编程中实现不被打断的数据交换操作。该操作通过将内存中的值与指定数据进行比较，当数值一样时将内存中的数据替换为新的值。\n在使用上，通常会记录下某块内存中的旧值，通过对旧值进行一系列的操作后得到新值，然后通过CAS操作将新值与旧值进行交换。如果这块内存的值在这期间内没被修改过，则旧值会与内存中的数据相同，这时CAS操作将会成功执行使内存中的数据变为新值。如果内存中的值在这期间内被修改过，则一般来说旧值会与内存中的数据不同，这时CAS操作将会失败，新值将不会被写入内存。\n死锁 死锁是指多个进程因循环等待资源而造成无法执行的现象。死锁会造成进程无法执行，同时会造成系统资源的极大浪费(资源无法释放)。\n死锁产生的四个必要条件   互斥使用：指进程对所分配到的资源进行排它性使用，即在一段时间内某资源只由一个进程占用。如果此时还有其它进程请求资源，则请求者只能等待，直至占有资源的进程用毕释放。\n  不可抢占：指进程已获得的资源，在未使用完之前，不能被剥夺，只能在使用完时由自己释放。\n  请求和保持：指进程已经保持至少一个资源，但又提出了新的资源请求，而该资源已被其它进程占有，此时请求进程阻塞，但又对自己已获得的其它资源保持不放。\n  循环等待：指在发生死锁时，必然存在一个进程——资源的环形链，即进程集合{P0，P1，P2，···，Pn}中的P0正在等待一个P1占用的资源；P1正在等待P2占用的资源，……，Pn正在等待已被P0占用的资源。\n  死锁避免 银行家算法：判断此次请求是否造成死锁若会造成死锁，则拒绝该请求。\n进程间通信 本地进程间通信的方式有很多，可以总结为下面四类：\n 消息传递（管道、FIFO、消息队列） 同步（互斥量、条件变量、读写锁、文件和写记录锁、信号量） 共享内存（匿名的和具名的） 远程过程调用（Solaris门和Sun RPC）  线程 线程是 操作系统能够进行运算调度的最小单位。它被包含在进程之中，是进程中的实际运作单位。一条线程指的是进程中一个单一顺序的控制流，一个进程中可以并发多个线程，每条线程并行执行不同的任务。在Unix System V及SunOS中也被称为轻量进程(lightweight processes)，但轻量进程更多指内核线程(kernel thread)，而把用户线程(user thread)称为线程。\n线程是独立调度和分派的基本单位。线程可以操作系统内核调度的内核线程，如Win32线程；由用户进程自行调度的用户线程，如Linux平台的POSIX Thread；或者由内核与用户进程，如Windows 7的线程，进行混合调度。\n同一进程中的多条线程将共享该进程中的全部系统资源，如虚拟地址空间，文件描述符和信号处理等等。但同一进程中的多个线程有各自的调用栈，自己的寄存器环境，自己的线程本地存储。\n线程的属性：   轻型实体：线程中的实体基本上不拥有系统资源，只是有一点必不可少的、能保证独立运行的资源。线程的实体包括程序、数据和TCB。线程是动态概念，它的动态特性由线程控制块TCB（Thread Control Block）描述。TCB包括以下信息：\n 线程状态。 当线程不运行时，被保存的现场资源。 一组执行堆栈。 存放每个线程的局部变量主存区。 访问同一个进程中的主存和其它资源。  用于指示被执行指令序列的程序计数器、保留局部变量、少数状态参数和返回地址等的一组寄存器和堆栈。\n  独立调度和分派的基本单位：在多线程OS中，线程是能独立运行的基本单位，因而也是独立调度和分派的基本单位。由于线程很“轻”，故线程的切换非常迅速且开销小（在同一进程中的）。\n  可并发执行：在一个进程中的多个线程之间，可以并发执行，甚至允许在一个进程中所有线程都能并发执行；同样，不同进程中的线程也能并发执行，充分利用和发挥了处理机与外围设备并行工作的能力。\n  共享进程资源：在同一进程中的各个线程，都可以共享该进程所拥有的资源，这首先表现在：所有线程都具有相同的地址空间（进程的地址空间），这意味着，线程可以访问该地址空间的每一个虚地址；此外，还可以访问进程所拥有的已打开文件、定时器、信号量机构等。由于同一个进程内的线程共享内存和文件，所以线程之间互相通信不必调用内核。\n   线程共享的环境包括：进程代码段、进程的公有数据(利用这些共享的数据，线程很容易的实现相互之间的通讯)、进程打开的文件描述符、信号的处理器、进程的当前目录和进程用户ID与进程组ID。\n 线程是程序执行的一条路径，在多线程的OS中，线程是调度和分配的基本单位，而进程是拥有资源的基本单位。\n"});index.add({'id':110,'href':'/interview/docs/basic/database/concurrent/','title':"并发控制",'content':"并发控制 如果数据库中的所有事务都是串行执行的，那么它非常容易成为整个应用的性能瓶颈，虽然说没法水平扩展的节点在最后都会成为瓶颈，但是串行执行事务的数据库会加速这一过程；而并发（Concurrency）使一切事情的发生都有了可能，它能够解决一定的性能问题，但是它会带来更多诡异的错误。\n引入了并发事务之后，如果不对事务的执行进行控制就会出现各种各样的问题，你可能没有享受到并发带来的性能提升就已经被各种奇怪的问题折磨的欲仙欲死了。\n如何控制并发是数据库领域中非常重要的问题之一，不过到今天为止事务并发的控制已经有了很多成熟的解决方案，而这些方案的原理就是这篇文章想要介绍的内容，最为常见的三种并发控制机制：\n 悲观并发控制：悲观并发控制其实是最常见的并发控制机制，也就是锁 乐观并发控制：即乐观锁，乐观锁其实并不是一种真实存在的锁 多版本并发控制（MVCC）：与前两者对立的命名不同，MVCC 可以与前两者中的任意一种机制结合使用，以提高数据库的读性能  悲观并发控制 控制不同的事务对同一份数据的获取是保证数据库的一致性的最根本方法，如果我们能够让事务在同一时间对同一资源有着独占的能力，那么就可以保证操作同一资源的不同事务不会相互影响。\n最简单的、应用最广的方法就是使用锁来解决，当事务需要对资源进行操作时需要先获得资源对应的锁，保证其他事务不会访问该资源后，再对资源进行各种操作；在悲观并发控制中，数据库程序对于数据被修改持悲观的态度，在数据处理的过程中都会被锁定，以此来解决竞争的问题。\n读写锁 为了最大化数据库事务的并发能力，数据库中的锁被设计为两种模式，分别是 共享锁和互斥锁。当一个事务获得共享锁之后，它只可以进行读操作，所以共享锁也叫 读锁 ；而当一个事务获得一行数据的互斥锁时，就可以对该行数据进行读和写操作，所以互斥锁也叫 写锁 。\n 共享锁和互斥锁除了限制事务能够执行的读写操作之外，它们之间还有『共享』和『互斥』的关系，也就是多个事务可以同时获得某一行数据的共享锁，但是互斥锁与共享锁和其他的互斥锁并不兼容\n 如果当前事务没有办法获取该行数据对应的锁时就会陷入等待的状态，直到其他事务将当前数据对应的锁释放才可以获得锁并执行相应的操作。\n两阶段锁协议 两阶段锁协议（2PL）是一种能够保证事务可串行化的协议，它将事务的获取锁和释放锁划分成了增长（Growing）和缩减（Shrinking）两个不同的阶段。\n在增长阶段，一个事务可以获得锁但是不能释放锁；而在缩减阶段事务只可以释放锁，并不能获得新的锁，如果只看 2PL 的定义，那么到这里就已经介绍完了，但是它还有两个变种：\n Strict 2PL：事务持有的 互斥锁 必须在提交后再释放； Rigorous 2PL：事务持有的 所有锁 必须在提交后释放；  虽然 锁的使用能够为我们解决不同事务之间由于并发执行造成的问题，但是两阶段锁的使用却引入了另一个严重的问题，死锁；不同的事务等待对方已经锁定的资源就会造成死锁，我们在这里举一个简单的例子：\n两个事务在刚开始时分别获取了 draven 和 beacon 资源上面的锁，然后再请求对方已经获得的锁时就会发生死锁，双方都没有办法等到锁的释放，如果没有死锁的处理机制就会无限等待下去，两个事务都没有办法完成。\n预防死锁 有两种方式可以帮助我们预防死锁的出现，一种是保证事务之间的等待不会出现环，也就是事务之间的等待图应该是一张有向无环图，没有循环等待的情况或者保证一个事务中想要获得的所有资源都在事务开始时以原子的方式被锁定，所有的资源要么被锁定要么都不被锁定。\n但是这种方式有两个问题，在事务一开始时很难判断哪些资源是需要锁定的，同时因为一些很晚才会用到的数据被提前锁定，数据的利用率与事务的并发率也非常的低。一种解决的办法就是按照一定的顺序为所有的数据行加锁，同时与 2PL 协议结合，在加锁阶段保证所有的数据行都是从小到大依次进行加锁的，不过这种方式依然需要事务提前知道将要加锁的数据集。\n另一种预防死锁的方法就是使用抢占加事务回滚的方式预防死锁，当事务开始执行时会先获得一个时间戳，数据库程序会根据事务的时间戳决定事务应该等待还是回滚。\n锁的粒度 到目前为止我们都没有对不同粒度的锁进行讨论，一直以来我们都讨论的都是数据行锁，但是在有些时候我们希望将多个节点看做一个数据单元，使用锁直接将这个数据单元、表甚至数据库锁定起来。这个目标的实现需要我们在数据库中定义不同粒度的锁：\n当我们拥有了不同粒度的锁之后，如果某个事务想要锁定整个数据库或者整张表时只需要简单的锁住对应的节点就会在当前节点加上显示（explicit）锁，在所有的子节点上加隐式（implicit）锁；虽然这种不同粒度的锁能够解决父节点被加锁时，子节点不能被加锁的问题，但是我们没有办法在子节点被加锁时，立刻确定父节点不能被加锁。\n在这时我们就需要引入 意向锁 来解决这个问题了，当需要给子节点加锁时，先给所有的父节点加对应的意向锁，意向锁之间是完全不会互斥的，只是用来帮助父节点快速判断是否可以对该节点进行加锁：\n这里是一张引入了两种意向锁，意向共享锁 和 意向互斥锁 之后所有的锁之间的兼容关系；到这里，我们通过不同粒度的锁和意向锁加快了数据库的吞吐量。\n乐观并发控制 除了悲观并发控制机制 - 锁之外，我们其实还有其他的并发控制机制，乐观并发控制（Optimistic Concurrency Control）。乐观并发控制也叫乐观锁，但是它并不是真正的锁，很多人都会误以为乐观锁是一种真正的锁，然而它只是一种并发控制的思想。\n基于时间戳的协议 锁协议按照不同事务对同一数据项请求的时间依次执行，因为后面执行的事务想要获取的数据已将被前面的事务加锁，只能等待锁的释放，所以基于锁的协议执行事务的顺序与获得锁的顺序有关。在这里想要介绍的 基于时间戳的协议能够在事务执行之前先决定事务的执行顺序。\n每一个事务都会具有一个全局唯一的时间戳，它即可以使用系统的时钟时间，也可以使用计数器，只要能够保证所有的时间戳都是唯一并且是随时间递增的就可以。\n基于时间戳的协议能够保证事务并行执行的顺序与事务按照时间戳串行执行的效果完全相同；每一个数据项都有两个时间戳，读时间戳和写时间戳，分别代表了当前成功执行对应操作的事务的时间戳。\n该协议能够保证所有冲突的读写操作都能按照时间戳的大小串行执行，在执行对应的操作时不需要关注其他的事务只需要关心数据项对应时间戳的值就可以了：\n无论是读操作还是写操作都会从左到右依次比较读写时间戳的值，如果小于当前值就会直接被拒绝然后回滚，数据库系统会给回滚的事务添加一个新的时间戳并重新执行这个事务。\n基于验证的协议 乐观并发控制其实本质上就是基于验证的协议，因为在多数的应用中只读的事务占了绝大多数，事务之间因为写操作造成冲突的可能非常小，也就是说大多数的事务在不需要并发控制机制也能运行的非常好，也可以保证数据库的一致性；而 并发控制机制其实向整个数据库系统添加了很多的开销，我们其实可以通过别的策略降低这部分开销。\n而验证协议就是我们找到的解决办法，它根据事务的只读或者更新将所有事务的执行分为两到三个阶段：\n在读阶段，数据库会执行事务中的 全部读操作和写操作，并将所有写后的值存入临时变量中，并不会真正更新数据库中的内容；在这时候会进入下一个阶段，数据库程序会检查当前的改动是否合法，也就是是否有其他事务在 RAED PHASE 期间更新了数据，如果通过测试那么直接就进入 WRITE PHASE 将所有存在临时变量中的改动全部写入数据库，没有通过测试的事务会直接被终止。\n为了保证乐观并发控制能够正常运行，我们需要知道一个事务不同阶段的发生时间，包括事务开始时间、验证阶段的开始时间以及写阶段的结束时间；通过这三个时间戳，我们可以保证任意冲突的事务不会同时写入数据库，一旦由一个事务完成了验证阶段就会立即写入，其他读取了相同数据的事务就会回滚重新执行。\n作为乐观的并发控制机制，它会假定所有的事务在最终都会通过验证阶段并且执行成功，而锁机制和基于时间戳排序的协议是悲观的，因为它们会在发生冲突时强制事务进行等待或者回滚，哪怕有不需要锁也能够保证事务之间不会冲突的可能。\n多版本并发控制 \u0026ndash; MVCC 到目前为止我们介绍的 并发控制机制其实都是通过延迟或者终止相应的事务来解决事务之间的竞争条件（Race condition）来保证事务的可串行化；虽然前面的两种并发控制机制确实能够从根本上解决并发事务的可串行化的问题，但是在实际环境中数据库的事务大都是只读的，读请求是写请求的很多倍，如果写请求和读请求之前没有并发控制机制，那么最坏的情况也是读请求读到了已经写入的数据，这对很多应用完全是可以接受的。\n在这种大前提下，数据库系统引入了另一种并发控制机制 - 多版本并发控制（Multiversion Concurrency Control），每一个写操作都会创建一个新版本的数据，读操作会从有限多个版本的数据中挑选一个最合适的结果直接返回；在这时，读写操作之间的冲突就不再需要被关注，而 管理和快速挑选数据的版本就成了 MVCC 需要解决的主要问题。\nMVCC 并不是一个与乐观和悲观并发控制对立的东西，它能够与两者很好的结合以增加事务的并发量，在目前最流行的 SQL 数据库 MySQL 和 PostgreSQL 中都对 MVCC 进行了实现；但是由于它们分别实现了悲观锁和乐观锁，所以 MVCC 实现的方式也不同。\nMVCC vs 乐观锁 MVCC 可以保证不阻塞地读到一致的数据。但是，MVCC 并没有对实现细节做约束，为此不同的数据库的语义有所不同，比如：\n  postgres 对写操作也是乐观并发控制；在表中保存同一行数据记录的多个不同版本，每次写操作，都是创建，而回避更新；在事务提交时，按版本号检查当前事务提交的数据是否存在写冲突，则抛异常告知用户，回滚事务；\n  innodb 则只对读无锁，写操作仍是上锁的悲观并发控制，这也意味着，innodb 中只能见到因死锁和不变性约束而回滚，而见不到因为写冲突而回滚，不像 postgres 那样对数据修改在表中创建新纪录，而是每行数据只在表中保留一份，在更新数据时上行锁，同时将旧版数据写入 undo log。表和 undo log 中行数据都记录着事务ID，在检索时，只读取来自当前已提交的事务的行数据。\n  可见 MVCC 中的写操作仍可以按悲观并发控制实现，而 CAS 的写操作只能是乐观并发控制。还有一个不同在于，MVCC 在语境中倾向于 “对多行数据打快照造平行宇宙”，然而 CAS 一般只是保护单行数据而已。比如 mongodb 有 CAS 的支持，但不能说这是 MVCC。\nMySQL 与 MVCC MySQL 中实现的多版本两阶段锁协议（Multiversion 2PL）将 MVCC 和 2PL 的优点结合了起来，每一个版本的数据行都具有一个唯一的时间戳，当有读事务请求时，数据库程序会直接从多个版本的数据项中具有最大时间戳的返回。\n更新操作就稍微有些复杂了，事务会先读取最新版本的数据计算出数据更新后的结果，然后创建一个新版本的数据，新数据的时间戳是目前数据行的最大版本 ＋1：\n数据版本的删除也是根据时间戳来选择的， MySQL 会将版本最低的数据定时从数据库中清除以保证不会出现大量的遗留内容。\n"});index.add({'id':111,'href':'/interview/docs/java/serilaser/','title':"序列化",'content':"序列化 ProtoBuffer Protocol Buffers 是一种轻便高效的结构化数据存储格式，可以用于结构化数据串行化，或者说序列化。它很适合做数据存储或 RPC 数据交换格式。可用于通讯协议、数据存储等领域的语言无关、平台无关、可扩展的序列化结构数据格式。\nProtobuf 的优点  Protobuf 更小、更快、也更简单。你可以定义自己的数据结构，然后使用代码生成器生成的代码来读写这个数据结构。你甚至可以在无需重新部署程序的情况下更新数据结构。只需使用 Protobuf 对数据结构进行一次描述，即可利用各种不同语言或从各种不同数据流中对你的结构化数据轻松读写。 “向后”兼容性好，人们不必破坏已部署的、依靠“老”数据格式的程序就可以对数据结构进行升级。这样您的程序就可以不必担心因为消息结构的改变而造成的大规模的代码重构或者迁移的问题。因为添加新的消息中的 field 并不会引起已经发布的程序的任何改变。 Protobuf 语义更清晰，无需类似 XML 解析器的东西（因为 Protobuf 编译器会将 .proto 文件编译生成对应的数据访问类以对 Protobuf 数据进行序列化、反序列化操作）。 Protobuf 的编程模式比较友好，简单易学，同时它拥有良好的文档和示例，对于喜欢简单事物的人们而言，Protobuf 比其他的技术更加有吸引力。  Protobuf 的不足 由于文本并不适合用来描述数据结构，所以 Protobuf 也不适合用来对基于文本的标记文档（如 HTML）建模。另外，由于 XML 具有某种程度上的自解释性，它可以被人直接读取编辑，在这一点上 Protobuf 不行，它以二进制的方式存储，除非你有 .proto 定义，否则你没法直接读出 Protobuf 的任何内容。\n"});index.add({'id':112,'href':'/interview/docs/offer/SerializeTree/','title':"序列化二叉树",'content':"题目 请实现两个函数，分别用来序列化和反序列化二叉树\n解题思路  通过前序遍历，进行序列化和反序列化 对于空节点用 $ 来代替  String Serialize(TreeNode root) { if (root==null) return \u0026quot;\u0026quot;; LinkedList\u0026lt;String\u0026gt; res = new LinkedList\u0026lt;\u0026gt;(); serialize(res, root); StringBuilder builder = new StringBuilder(); res.forEach(v-\u0026gt; builder.append(v).append(\u0026quot;,\u0026quot;)); return builder.toString(); } private void serialize(LinkedList\u0026lt;String\u0026gt; res, TreeNode root) { if (root == null) { res.addLast(\u0026quot;$\u0026quot;); return; } res.addLast(String.valueOf(root.val)); serialize(res, root.left); serialize(res, root.right); } TreeNode Deserialize(String str) { if (str == null || str.length() == 0) return null; return deserialize(str.split(\u0026quot;,\u0026quot;), new int[]{0}); } private TreeNode deserialize(String[] str, int[] index) { if (index[0] \u0026gt;= str.length) return null; String c = str[index[0]++]; if (c.equals(\u0026quot;$\u0026quot;)) return null; TreeNode node = new TreeNode(Integer.valueOf(c)); node.left = deserialize(str, index); node.right = deserialize(str, index); return node; } "});index.add({'id':113,'href':'/interview/docs/offer/isContinuous/','title':"扑克牌顺子",'content':"题目 牛客网\nLL今天心情特别好,因为他去买了一副扑克牌,发现里面居然有 2 个大王, 2 个小王(一副牌原本是 54 张)\u0026hellip;他随机从中抽出了 5 张牌,想测测自己的手气,看看能不能抽到顺子,如果抽到的话,他决定去买体育彩票,嘿嘿！！“红心A,黑桃3,小王,大王,方片5”,“Oh My God!”不是顺子\u0026hellip;..LL不高兴了,他想了想,决定大\\小 王可以看成任何数字,并且A看作1,J为11,Q为12,K为13。上面的5张牌就可以变成“1,2,3,4,5”(大小王分别看作2和4),“So Lucky!”。LL决定去买体育彩票啦。 现在,要求你使用这幅牌模拟上面的过程,然后告诉我们 LL 的运气如何， 如果牌能组成顺子就输出 true，否则就输出 false。为了方便起见,你可以认为大小王是0。\n解题思路  对数组进行排序 计算非0元素之间的间隔总和 如果有相同元素则直接认为失败 如果间隔大于0，那么间隔的总个数等于0的总个数，即为成功  public boolean isContinuous(int[] numbers) { if (numbers == null || numbers.length \u0026lt; 5) return false; Arrays.sort(numbers); int count = 0; int zeroCount = 0; int pre = -1; for (int number : numbers) { if (number == 0) { zeroCount++; continue; } if (pre == -1) pre = number; else { int t = number - pre - 1; if (t \u0026gt; 0) { count += t; } else if (t \u0026lt; 0) return false; pre = number; } } if (count == 0) return true; else return count == zeroCount; } "});index.add({'id':114,'href':'/interview/docs/offer/printn/','title':"打印最大的",'content':"输入n，打印出 1 到最大的 n 位十进制数。比如输入3，则打印出1、2、3 直到最大的 3 位数 999。\n解题思路  n 可能很大，导致输出的数字超过 int 或者 long  public void PrintN(int n) { if (n \u0026lt;= 0) { return; } String res = \u0026quot;0\u0026quot;; while (true) { boolean all9 = true; res = Plus(res, 1); System.out.println(res); for (int i = 0; i \u0026lt; res.length(); i++) { if (res.charAt(i) != '9') { all9 = false; break; } } if (all9 \u0026amp;\u0026amp; res.length() == n) { break; } } } private String Plus(String t, int i) { char[] chars = t.toCharArray(); StringBuilder res = new StringBuilder(); chars[chars.length - 1] += i; boolean flag = false; for (int j = chars.length - 1; j \u0026gt;= 0; j--) { int a = chars[j]; if (flag) { a++; flag = false; } if (a \u0026gt; '9') { flag = true; a = a - '9' + '0' - 1; } res.append((char) a); } if (flag) { res.append('1'); } return res.reverse().toString(); } "});index.add({'id':115,'href':'/interview/docs/offer/TranslateNumToStr/','title':"把数字翻译成字符串",'content':"题目 给定一个数字，按照如下规则翻译成字符串：0 翻译成“a”，1 翻译成“b”… 25翻译成“z”。一个数字有多种翻译可能，例如12258一共有5种，分别是bccfi，bwfi，bczi，mcfi，mzi。实现一个函数，用来计算一个数字有多少种不同的翻译方法。\n解题思路  定义 $$f(i)$$ 表示第 i 位有多少种翻译的方法，动态规划方程：$$f(i)=f(i+1)+g(i,i+1) \\times f(i+2)$$ 其中 $$g(i,i+1)$$ 表示 i,i+1 是否能组成 10 ~ 25  public int translateNumToStr(int num) { char[] str = String.valueOf(num).toCharArray(); int[] res = new int[str.length]; for (int i = str.length - 1; i \u0026gt;= 0; i--) { if (i + 1 \u0026gt;= str.length) { res[i] = 1; continue; } res[i] = res[i + 1]; if (i + 2 \u0026lt; str.length \u0026amp;\u0026amp; str[i] \u0026lt;= '2' \u0026amp;\u0026amp; str[i] \u0026gt;= '1' \u0026amp;\u0026amp; str[i + 1] \u0026lt;= '5') { res[i] += res[i + 2]; } } return res[0]; } "});index.add({'id':116,'href':'/interview/docs/offer/PrintMinNumber/','title':"把数组排成最小的数",'content':"题目 把数组排成最小的数\n输入一个正整数数组，把数组里所有数字拼接起来排成一个数，打印能拼接出的所有数字中最小的一个。例如输入数组{3，32，321}，则打印出这三个数字能排成的最小数字为321323。\n解题思路  最直接的办法就是，找到数组中数字的所有排列组合，找到最小的 对于 $$m, n$$，可以组成 $$mn , nm$$ 这两个数，如果 $$mn \u0026lt; nm$$ 那么，$$m$$ 应该在 $$n$$ 之前 对于一组数，可以通过上述规则进行排序，依次打印出来就是最小的数 由于组合之后的数可能超出 int 的表示范围，注意使用字符串来处理大数问题  public String PrintMinNumber(int[] numbers) { List\u0026lt;String\u0026gt; nums = new ArrayList\u0026lt;\u0026gt;(); for (int number : numbers) { nums.add(String.valueOf(number)); } nums.sort(Comparator.comparing(s -\u0026gt; s, (o1, o2) -\u0026gt; (o1 + o2).compareTo(o2 + o1))); StringJoiner joiner = new StringJoiner(\u0026quot;\u0026quot;); nums.forEach(joiner::add); return joiner.toString(); } "});index.add({'id':117,'href':'/interview/docs/basic/algo/sort/','title':"排序算法",'content':"排序算法 常见排序算法 稳定排序：  冒泡排序 — O(n²) 插入排序 — O(n²) 桶排序 — O(n); 需要 O(k) 额外空间 归并排序 — O(nlogn); 需要 O(n) 额外空间 二叉排序树排序 — O(n log n) 期望时间; O(n²)最坏时间; 需要 O(n) 额外空间 基数排序 — O(n·k); 需要 O(n) 额外空间  不稳定排序  选择排序 — O(n²) 希尔排序 — O(nlogn) 堆排序 — O(nlogn) 快速排序 — O(nlogn) 期望时间, O(n²) 最坏情况; 对于大的、乱数串行一般相信是最快的已知排序  交换排序 冒泡排序 它重复地走访过要排序的数列，一次比较两个元素，如果他们的顺序错误就把他们交换过来。走访数列的工作是重复地进行直到没有再需要交换，也就是说该数列已经排序完成。冒泡排序总的平均时间复杂度为O(n^2)。冒泡排序是一种稳定排序算法。\n 比较相邻的元素。如果第一个比第二个大，就交换他们两个。 对每一对相邻元素作同样的工作，从开始第一对到结尾的最后一对。在这一点，最后的元素应该会是最大的数。 针对所有的元素重复以上的步骤，除了最后一个。 持续每次对越来越少的元素重复上面的步骤，直到没有任何一对数字需要比较。  void bubble_sort(int a[], int n) { int i, j, temp; for (j = 0; j \u0026lt; n - 1; j++) for (i = 0; i \u0026lt; n - 1 - j; i++) { if(a[i] \u0026gt; a[i + 1]) { temp = a[i]; a[i] = a[i + 1]; a[i + 1] = temp; } } } 快速排序 快速排序-百度百科 快速排序是一种 不稳定 的排序算法，平均时间复杂度为 O(nlogn)。快速排序使用分治法（Divide and conquer）策略来把一个序列（list）分为两个子序列（sub-lists）。 步骤为：\n 从数列中挑出一个元素，称为\u0026quot;基准\u0026rdquo;（pivot）， 重新排序数列，所有元素比基准值小的摆放在基准前面，所有元素比基准值大的摆在基准的后面（相同的数可以到任一边）。在这个分区结束之后，该基准就处于数列的中间位置。这个称为分区（partition）操作。 递归地（recursive）把小于基准值元素的子数列和大于基准值元素的子数列排序。   快排的时间花费主要在划分上，所以\n 最坏情况：时间复杂度为O(n^2)。因为最坏情况发生在每次划分过程产生的两个区间分别包含n-1个元素和1个元素的时候。 最好情况：每次划分选取的基准都是当前无序区的中值。如果每次划分过程产生的区间大小都为n/2，则快速排序法运行就快得多了。    public void sort(int[] arr, int low, int high) { int l = low; int h = high; int povit = arr[low]; while (l \u0026lt; h) { while (l \u0026lt; h \u0026amp;\u0026amp; arr[h] \u0026gt;= povit) h--; if (l \u0026lt; h) { arr[l] = arr[h]; l++; } while (l \u0026lt; h \u0026amp;\u0026amp; arr[l] \u0026lt;= povit) l++; if (l \u0026lt; h) { arr[h] = arr[l]; h--; } } arr[l] = povit; System.out.print(\u0026quot;l=\u0026quot; + (l + 1) + \u0026quot;;h=\u0026quot; + (h + 1) + \u0026quot;;povit=\u0026quot; + povit + \u0026quot;\\n\u0026quot;); System.out.println(Arrays.toString(arr)); if (l - 1 \u0026gt; low) sort(arr, low, l - 1); if (h + 1 \u0026lt; high) sort(arr, h + 1, high); } 快排的优化  当待排序序列的长度分割到一定大小后，使用插入排序。 快排函数在函数尾部有两次递归操作，我们可以对其使用尾递归优化。优化后，可以缩减堆栈深度，由原来的O(n)缩减为O(logn)，将会提高性能。 从左、中、右三个数中取中间值。  插入排序 直接插入排序 插入排序的基本操作就是将一个数据插入到已经排好序的有序数据中，从而得到一个新的、个数加一的有序数据，算法适用于少量数据的排序，时间复杂度为O(n^2)。是稳定的排序方法。 插入算法把要排序的数组分成两部分：第一部分包含了这个数组的所有元素，但将最后一个元素除外（让数组多一个空间才有插入的位置），而第二部分就只包含这一个元素（即待插入元素）。在第一部分排序完成后，再将这个最后元素插入到已排好序的第一部分中。\nvoid insert_sort(int* a, int len) { for (int i = 1; i \u0026lt; len; ++i) { int j = i - 1; int temp = a[i]; while (j \u0026gt;= 0 \u0026amp;\u0026amp; temp \u0026lt; a[j]) { a[j + 1] = a[j]; j--; } a[j + 1] = temp; } } 希尔排序 也称缩小增量排序，是直接插入排序算法的一种更高效的改进版本。希尔排序是非稳定排序算法。\n希尔排序是把记录按下标的一定增量分组，对每组使用直接插入排序算法排序；随着增量逐渐减少，每组包含的关键词越来越多，当增量减至1时，整个文件恰被分成一组，算法便终止。\nvoid shell_sort(int* a, int len) { int step = len / 2; int temp; while (step \u0026gt; 0) { for (int i = step; i \u0026lt; len; ++i) { temp = a[i]; int j = i - step; while (j \u0026gt;= 0 \u0026amp;\u0026amp; temp \u0026lt; a[j]) { a[j + step] = a[j]; j -= step; } a[j + step] = temp; } step /= 2; } } 选择排序 直接选择排序 首先在未排序序列中找到最小（大）元素，存放到排序序列的起始位置，然后，再从剩余未排序元素中继续寻找最小（大）元素，然后放到已排序序列的末尾。实际适用的场合非常罕见。\nvoid selection_sort(int arr[], int len) { int i, j, min, temp; for (i = 0; i \u0026lt; len - 1; i++) { min = i; for (j = i + 1; j \u0026lt; len; j++) if (arr[min] \u0026gt; arr[j]) min = j; temp = arr[min]; arr[min] = arr[i]; arr[i] = temp; } } 堆排序 堆排序利用了大根堆（或小根堆）堆顶记录的关键字最大（或最小）这一特征，使得在当前无序区中选取最大（或最小）关键字的记录变得简单。\n 将数组分为有序区和无序区，在无序区中建立最大堆 将堆顶的数据与无序区末尾的数据交换 从后往前，直到所有数据排序完成  public void heapSort(int[] nums) { for (int i = nums.length - 1; i \u0026gt;= 0; i--) { maxHeap(nums, 0, i); swap(nums, 0, i); } } public void maxHeap(int[] heap, int start, int end) { if (start == end) { return; } int parent = start; int childLeft = start * 2 + 1; int childRight = childLeft + 1; if (childLeft \u0026lt;= end) { maxHeap(heap, childLeft, end); if (heap[childLeft] \u0026gt; heap[parent]) { swap(heap, parent, childLeft); } } if (childRight \u0026lt;= end) { maxHeap(heap, childRight, end); if (heap[childRight] \u0026gt; heap[parent]) { swap(heap, parent, childRight); } } } private void swap(int[] nums, int a, int b) { int t = nums[a]; nums[a] = nums[b]; nums[b] = t; } 归并排序 归并排序采用分治的思想：\n Divide：将n个元素平均划分为各含n/2个元素的子序列； Conquer：递归的解决俩个规模为n/2的子问题； Combine：合并俩个已排序的子序列。  性能：时间复杂度总是为O(NlogN)，空间复杂度也总为为O(N)，算法与初始序列无关，排序是稳定的。\npublic void mergeSort(int[] array, int start, int end, int[] temp) { if (start \u0026gt;= end) { return; } int mid = (start + end) / 2; mergeSort(array, start, mid, temp); mergeSort(array, mid + 1, end, temp); int f = start, s = mid + 1; int t = 0; while (f \u0026lt;= mid \u0026amp;\u0026amp; s \u0026lt;= end) { if (array[f] \u0026lt; array[s]) { temp[t++] = array[f++]; } else { temp[t++] = array[s++]; } } while (f \u0026lt;= mid) { temp[t++] = array[f++]; } while (s \u0026lt;= end) { temp[t++] = array[s++]; } for (int i = 0, j = start; i \u0026lt; t; i++) { array[j++] = temp[i]; } } 基数排序 对于有d个关键字时，可以分别按关键字进行排序。有俩种方法：\n MSD：先从高位开始进行排序，在每个关键字上，可采用基数排序 LSD：先从低位开始进行排序，在每个关键字上，可采用桶排序   即通过每个数的每位数字的大小来比较\n //找出最大数字的位数 int maxNum(int arr[], int len) { int _max = 0; for (int i = 0; i \u0026lt; len; ++i) { int d = 0; int a = arr[i]; while (a) { a /= 10; d++; } if (_max \u0026lt; d) { _max = d; } } return _max; } void radixSort(int *arr, int len) { int d = maxNum(arr, len); int *temp = new int[len]; int count[10]; int radix = 1; for (int i = 0; i \u0026lt; d; ++i) { for (int j = 0; j \u0026lt; 10; ++j) { count[j] = 0; } for (int k = 0; k \u0026lt; len; ++k) { count[(arr[k] / radix) % 10]++; } for (int l = 1; l \u0026lt; 10; ++l) { count[l] += count[l - 1]; } for (int m = 0; m \u0026lt; len; ++m) { int index = (arr[m] / radix) % 10; temp[count[index] - 1] = arr[m]; count[index]--; } for (int n = 0; n \u0026lt; len; ++n) { arr[n] = temp[n]; } radix *= 10; } delete (temp); } 拓扑排序 在有向图中找拓扑序列的过程，就是拓扑排序。拓扑序列常常用于判定图是否有环。\n 从有向图中选择一个入度为0的结点，输出它。 将这个结点以及该结点出发的所有边从图中删除。 重复前两步，直到没有入度为0的点。   如果所有点都被输出，即存在一个拓扑序列，则图没有环。\n "});index.add({'id':118,'href':'/interview/docs/leetcode/sortList/','title':"排序链表",'content':"头条重点\n题目 在 O(n log n) 时间复杂度和常数级空间复杂度下，对链表进行排序。\n示例 1: 输入: 4-\u0026gt;2-\u0026gt;1-\u0026gt;3 输出: 1-\u0026gt;2-\u0026gt;3-\u0026gt;4 示例 2: 输入: -1-\u0026gt;5-\u0026gt;3-\u0026gt;4-\u0026gt;0 输出: -1-\u0026gt;0-\u0026gt;3-\u0026gt;4-\u0026gt;5 解题思路  通过快慢指针将链表拆分 递归进行拆分，再通过合并两个排序链表的方式进行合并 类似于归并排序  public ListNode sortList(ListNode head) { if (head == null || head.next == null) { return head; } ListNode slow = head, fast = head; while (fast.next != null \u0026amp;\u0026amp; fast.next.next != null) { fast = fast.next.next; slow = slow.next; } ListNode mid = slow.next; slow.next = null; ListNode l1 = sortList(head); ListNode l2 = sortList(mid); return merge(l1, l2); } private ListNode merge(ListNode l1, ListNode l2) { if (l1 == null) { return l2; } if (l2 == null) { return l1; } ListNode head,res; if (l1.val \u0026gt; l2.val) { head = l2; l2 = l2.next; } else { head = l1; l1 = l1.next; } res = head; // head.next = null; while (l1 != null || l2 != null) { if (l1 == null) { head.next = l2; l2 = l2.next; } else if (l2 == null) { head.next = l1; l1 = l1.next; } else { if (l1.val \u0026gt; l2.val) { head.next = l2; l2 = l2.next; } else { head.next = l1; l1 = l1.next; } } head = head.next; } return res; } "});index.add({'id':119,'href':'/interview/docs/leetcode/trap/','title':"接雨水",'content':"头条重点\n题目 给定 n 个非负整数表示每个宽度为 1 的柱子的高度图，计算按此排列的柱子，下雨之后能接多少雨水。\n上面是由数组 [0,1,0,2,1,0,1,3,2,1,2,1] 表示的高度图，在这种情况下，可以接 6 个单位的雨水（蓝色部分表示雨水）。 感谢 Marcos 贡献此图。\n示例: 输入: [0,1,0,2,1,0,1,3,2,1,2,1] 输出: 6 解题思路  首先找到最高点，然后从首尾向中间遍历，找到局部高点，然后就可以计算总量  public int trap(int[] height) { if (height.length \u0026lt;= 2) { return 0; } int max = 0, maxIndex = 0; for (int i = 0; i \u0026lt; height.length; i++) { if (height[i] \u0026gt; max) { max = height[i]; maxIndex = i; } } int total = 0; int topIndex = 0;//局部最高点 for (int i = 0; i \u0026lt; maxIndex; i++) { if (height[topIndex] \u0026lt; height[i]) { topIndex = i; } else { total += height[topIndex] - height[i]; } } topIndex = height.length - 1; for (int i = height.length - 1; i \u0026gt; maxIndex; i--) { if (height[topIndex] \u0026lt; height[i]) { topIndex = i; } else { total += height[topIndex] - height[i]; } } return total; } "});index.add({'id':120,'href':'/interview/docs/offer/search-a-2d-matrix/','title':"搜索二维矩阵",'content':"题目 Leetcode\n编写一个高效的算法来搜索 m x n 矩阵 matrix 中的一个目标值 target。该矩阵具有以下特性：\n 每行的元素从左到右升序排列。 每列的元素从上到下升序排列。  示例:\n现有矩阵 matrix 如下：\n[ [1, 4, 7, 11, 15], [2, 5, 8, 12, 19], [3, 6, 9, 16, 22], [10, 13, 14, 17, 24], [18, 21, 23, 26, 30] ]  给定 target = 5，返回 true。 给定 target = 20，返回 false。  解题思路 二维数组是有规律的：右上角的数字是一列中最小的、一行中最大的，通过这个数字和 target 进行对比，可以将一行或者一列作为候选区域排出，那么 target 可能存在的范围缩小，最终得出结果。\npublic boolean searchMatrix(int[][] matrix, int target) { if (matrix.length == 0) { return false; } for (int i = 0, j = matrix[0].length - 1; i \u0026lt; matrix.length \u0026amp;\u0026amp; j \u0026gt;= 0; ) { if (matrix[i][j] \u0026gt; target) { j--; } else if (matrix[i][j] \u0026lt; target) { i++; } else { return true; } } return false; } "});index.add({'id':121,'href':'/interview/docs/leetcode/searchRote/','title':"搜索旋转排序数组",'content':"头条重点\n题目 假设按照升序排序的数组在预先未知的某个点上进行了旋转。( 例如，数组 [0,1,2,4,5,6,7] 可能变为 [4,5,6,7,0,1,2] )。\n搜索一个给定的目标值，如果数组中存在这个目标值，则返回它的索引，否则返回 -1 。\n你可以假设数组中不存在重复的元素。\n你的算法时间复杂度必须是 O(log n) 级别。\n示例 1: 输入: nums = [4,5,6,7,0,1,2], target = 0 输出: 4 示例 2: 输入: nums = [4,5,6,7,0,1,2], target = 3 输出: -1 解题思路  旋转数组是分为两段有序，主要得注意 mid 落在哪个段上  public static int search(int[] nums, int target) { int start = 0, end = nums.length - 1; while (start \u0026lt;= end) { int mid = (start + end) / 2; if (nums[mid]==target) return mid; if (nums[mid] \u0026gt;= nums[start]) { if (target \u0026lt; nums[mid] \u0026amp;\u0026amp; target \u0026gt;= nums[start]) { end = mid - 1; } else { start = mid + 1; } } if (nums[mid] \u0026lt;= nums[end]) { if (target \u0026gt; nums[mid] \u0026amp;\u0026amp; target \u0026lt;= nums[end]) { start = mid + 1; } else { end = mid - 1; } } } return -1; } "});index.add({'id':122,'href':'/interview/docs/basic/os/','title':"操作系统基础",'content':"操作系统基础 操作系统提供的服务 操作系统的五大功能，分别为：作业管理、文件管理、存储管理、输入输出设备管理、进程及处理机管理\n中断 所谓的中断就是在计算机执行程序的过程中，由于出现了某些特殊事情，使得CPU暂停对程序的执行，转而去执行处理这一事件的程序。等这些特殊事情处理完之后再回去执行之前的程序。中断一般分为三类：\n 内部异常中断：由计算机硬件异常或故障引起的中断； 软中断：由程序中执行了引起中断的指令而造成的中断（这也是和我们将要说明的系统调用相关的中断）； 外部中断：由外部设备请求引起的中断，比如I/O请求。   简单来说，对中断的理解就是对一些特殊事情的处理。\n 与中断紧密相连的一个概念就是中断处理程序了。当中断发生的时候，系统需要去对中断进行处理，对这些中断的处理是由操作系统内核中的特定函数进行的，这些处理中断的特定的函数就是我们所说的中断处理程序了。\n另一个与中断紧密相连的概念就是中断的优先级。中断的优先级说明的是当一个中断正在被处理的时候，处理器能接受的中断的级别。中断的优先级也表明了中断需要被处理的紧急程度。每个中断都有一个对应的优先级，当处理器在处理某一中断的时候，只有比这个中断优先级高的中断可以被处理器接受并且被处理。优先级比这个当前正在被处理的中断优先级要低的中断将会被忽略。\n典型的中断优先级如下所示：\n机器错误 \u0026gt; 时钟 \u0026gt; 磁盘 \u0026gt; 网络设备 \u0026gt; 终端 \u0026gt; 软件中断 当发生软件中断时，其他所有的中断都可能发生并被处理；但当发生磁盘中断时，就只有时钟中断和机器错误中断能被处理了。\n系统调用 在讲系统调用之前，先说下进程的执行在系统上的两个级别：用户级和核心级，也称为用户态和系统态(user mode and kernel mode)。\n程序的执行一般是在用户态下执行的，但当程序需要使用操作系统提供的服务时，比如说打开某一设备、创建文件、读写文件等，就需要向操作系统发出调用服务的请求，这就是系统调用。\nLinux系统有专门的函数库来提供这些请求操作系统服务的入口，这个函数库中包含了操作系统所提供的对外服务的接口。当进程发出系统调用之后，它所处的运行状态就会由用户态变成核心态。但这个时候，进程本身其实并没有做什么事情，这个时候是由内核在做相应的操作，去完成进程所提出的这些请求。\n系统调用和中断的关系就在于，当进程发出系统调用申请的时候，会产生一个软件中断。产生这个软件中断以后，系统会去对这个软中断进行处理，这个时候进程就处于核心态了。\n那么用户态和核心态之间的区别是什么呢？\n 用户态的进程能存取它们自己的指令和数据，但不能存取内核指令和数据（或其他进程的指令和数据）。然而，核心态下的进程能够存取内核和用户地址 某些机器指令是特权指令，在用户态下执行特权指令会引起错误  对此要理解的一个是，在系统中内核并不是作为一个与用户进程平行的估计的进程的集合，内核是为用户进程运行的。\n"});index.add({'id':123,'href':'/interview/docs/offer/GetNumberOfK/','title':"数字在排序数组中出现的次数",'content':"题目 牛客网\n统计一个数字在排序数组中出现的次数。\n解题思路  利用二分查找，找到任意一个 k 由于 k 有多个，并且当前找到的 k 可能在任意位置。所以，在当前 k 的前后进行遍历查找  public int GetNumberOfK(int[] array, int k) { if (array == null || array.length == 0) { return 0; } //二分查找 int start = 0, end = array.length - 1; int t = -1; while (start \u0026lt; end) { int mid = (start + end) / 2; if (array[mid] == k) { t = mid; break; } else if (array[mid] \u0026gt; k) { end = mid - 1; } else { start = mid + 1; } } if (array[start] == k) { t = start; } if (t == -1) { return 0; } //左侧 int sum = 0; int a = t; while (a \u0026gt;= 0 \u0026amp;\u0026amp; array[a] == k) { sum++; a--; } //右侧 a = t + 1; while (a \u0026lt; array.length \u0026amp;\u0026amp; array[a] == k) { sum++; a++; } return sum; } "});index.add({'id':124,'href':'/interview/docs/offer/NOfNumberSerialize/','title':"数字序列中的某一位的数字",'content':"题目 数字以0123456789101112131415…的格式序列化到一个字符序列中。在这个序列中，第5位（从0开始计数，即从第0位开始）是5，第13位是1，第19位是4，等等。请写一个函数，求任意第n位对应的数字。\n解题思路  可以将 n 进行拆分，1位数一共10个数字、10位，2位数一共90个数字、180位，依此类推 当确定 n 所在位数范围时，对位数取商，计算出 n 位对应的数字 a，再取余，计算出结果位于 a 的第几位  public int nOfNumberSerialize(int n) { int i = 1; int count = 0; int nLeft = n; while (true) { nLeft -= count; count = countOfIntegers(i) * i; if (nLeft \u0026lt; count) { break; } i++; } int a = nLeft / i; String s = String.valueOf(a); return s.charAt(nLeft % i) - '0'; } private int countOfIntegers(int n) { int sum = 0; if (n == 1) { sum = 10; } else { sum = (int) (9 * Math.pow(10, n - 1)); } return sum; } "});index.add({'id':125,'href':'/interview/docs/offer/StreamMid/','title':"数据流中的中位数",'content':"题目 牛客网\n如何得到一个数据流中的中位数？如果从数据流中读出奇数个数值，那么中位数就是所有数值排序之后位于中间的数值。如果从数据流中读出偶数个数值，那么中位数就是所有数值排序之后中间两个数的平均值。我们使用 Insert() 方法读取数据流，使用 GetMedian() 方法获取当前读取数据的中位数。\n解题思路  同两个堆来表示中位数的左右两部分，左边是大根堆，右边是小根堆 在插入元素时，两边元素个数最多只能相差1，并且要保证左边的元素均小于右边的元素 当插入大堆的元素大于部分小堆元素时，需要将大堆的 top 元素移动到小堆，反之亦然  private PriorityQueue\u0026lt;Integer\u0026gt; maxHeap = new PriorityQueue\u0026lt;\u0026gt;((o1, o2) -\u0026gt; -o1.compareTo(o2)); private PriorityQueue\u0026lt;Integer\u0026gt; minHeap = new PriorityQueue\u0026lt;\u0026gt;(); private int size = 0; public void Insert(Integer num) { if (size % 2 == 0) { maxHeap.add(num); if (minHeap.isEmpty() || num \u0026gt; minHeap.peek()) { minHeap.add(maxHeap.poll()); } } else { minHeap.add(num); if (maxHeap.isEmpty() || num \u0026lt; maxHeap.peek()) { maxHeap.add(minHeap.poll()); } } size++; } public Double GetMedian() { if (maxHeap.isEmpty() \u0026amp;\u0026amp; minHeap.isEmpty()) return 0.0; if (maxHeap.isEmpty()) return minHeap.peek() * 1.0; if (minHeap.isEmpty()) return maxHeap.peek() * 1.0; if (maxHeap.size() == minHeap.size()) { return (maxHeap.peek() + minHeap.peek()) / 2.0; } return maxHeap.size() \u0026gt; minHeap.size() ? maxHeap.peek() * 1.0 : minHeap.peek() * 1.0; } "});index.add({'id':126,'href':'/interview/docs/offer/MoreThanHalfNum/','title':"数组中出现次数超过一半的数字",'content':"题目 牛客网\n数组中有一个数字出现的次数超过数组长度的一半，请找出这个数字。例如输入一个长度为9的数组{1,2,3,2,2,2,5,4,2}。由于数字2在数组中出现了5次，超过数组长度的一半，因此输出 2 。如果不存在则输出 0 。\n解题思路  由于数组的特性，在排序数组中，超过半数的数字一定包含中位数 通过 partition 方法，借用快排的思想，随机选取一个 key，将数组中小于 key 的移动到 key 的左侧，数组中大于 key 的移动到 key 的右侧 最终找到中位数的下标，还需要检查中位数是否超过半数  public int MoreThanHalfNum_Solution(int[] array) { int start = 0, end = array.length - 1; int mid = array.length / 2; int index = partition(array, start, end); if (index == mid) { return array[index]; } while (index != mid \u0026amp;\u0026amp; start \u0026lt;= end) { if (index \u0026gt; mid) { end = index - 1; index = partition(array, start, end); } else { start = index + 1; index = partition(array, start, end); } } if (checkIsHalf(array, index)) return array[index]; return 0; } private boolean checkIsHalf(int[] array, int index) { if (index \u0026lt; 0) { return false; } int count = 0; for (int i : array) { if (array[index] == i) { count++; } } return count \u0026gt; array.length / 2; } private int partition(int[] array, int start, int end) { if (start \u0026gt;= array.length || start \u0026lt; 0 || end \u0026gt;= array.length || end \u0026lt; 0) { return -1; } int key = array[start]; int left = start, right = end; while (left \u0026lt; right) { while (left \u0026lt; right \u0026amp;\u0026amp; array[right] \u0026gt;= key) { right--; } if (left \u0026lt; right) { array[left] = array[right]; left++; } while (left \u0026lt; right \u0026amp;\u0026amp; array[left] \u0026lt;= key) { left++; } if (left \u0026lt; right) { array[right] = array[left]; right--; } } array[left] = key; return left; } "});index.add({'id':127,'href':'/interview/docs/offer/FindNumsAppearOnce/','title':"数组中只出现一次的数字",'content':"题目 牛客网\n一个整型数组里除了两个数字之外，其他的数字都出现了两次。请写程序找出这两个只出现一次的数字。\n解题思路  两个相等的数字进行异或的结果为0 在这个特殊的数组中，重复出现的数字只能为2次，那么如果将所有数字异或 就等价与将两个不同的数字进行异或 异或的结果肯定有一位为1，那么这两个不同的数字，在这一位上不同。 找到第一个为1的位，并将第一位为1的位是否为1作为分组条件，相同的数字一定在同一个分组里，整个数组分组异或 得到两个结果，即为两个不同的数  /** * num1,num2分别为长度为1的数组。传出参数。将num1[0],num2[0]设置为返回结果 * @param array * @param num1 * @param num2 */ public void FindNumsAppearOnce(int[] array, int num1[], int num2[]) { if (array == null || array.length \u0026lt; 3) { return; } int result = array[0]; for (int i = 1; i \u0026lt; array.length; i++) { result ^= array[i]; } //找到第一个为1的位 int indexOfFirstBit1 = 0; int temp = result; while (temp != 0) { indexOfFirstBit1++; temp \u0026gt;\u0026gt;\u0026gt;= 1; } int mask = 1; for (int i = 1; i \u0026lt; indexOfFirstBit1; i++) { mask \u0026lt;\u0026lt;= 1; } //将第一位为1的位是否为1作为分组条件，分组异或 int n1 = -1, n2 = -1; for (int i : array) { if ((i \u0026amp; mask) == mask) { if (n1 == -1) n1 = i; else n1 ^= i; } else { if (n2 == -1) n2 = i; else n2 ^= i; } } num1[0] = n1; num2[0] = n2; } "});index.add({'id':128,'href':'/interview/docs/leetcode/findKthLargest/','title':"数组中的第K个最大元素",'content':"数组中的第K个最大元素 题目 在未排序的数组中找到第 k 个最大的元素。请注意，你需要找的是数组排序后的第 k 个最大的元素，而不是第 k 个不同的元素。\n示例 1: 输入: [3,2,1,5,6,4] 和 k = 2 输出: 5 示例 2: 输入: [3,2,3,1,2,4,5,5,6] 和 k = 4 输出: 4 解题思路  利用快排的思想，当排序到 k 后，停止排序，输出结果  public static int findKthLargest(int[] nums, int k) { fastSort(nums, 0, nums.length - 1); return nums[nums.length - k]; } public static void fastSort(int[] nums, int start, int end) { if (nums.length \u0026lt;= 1) { return; } if (start \u0026gt; end) { return; } if (end \u0026lt; 0 || start \u0026lt; 0 || end \u0026gt; nums.length - 1 || start \u0026gt; nums.length - 1) { return; } int left = start, right = end; int keyIndex = (left + right) / 2; while (left \u0026lt; right) { while (right \u0026gt; keyIndex \u0026amp;\u0026amp; nums[right] \u0026gt; nums[keyIndex]) { right--; } if (right \u0026gt; keyIndex) { swap(nums, keyIndex, right); keyIndex = right; } while (left \u0026lt; keyIndex \u0026amp;\u0026amp; nums[left] \u0026lt; nums[keyIndex]) { left++; } if (left \u0026lt; keyIndex) { swap(nums, left, keyIndex); keyIndex = left; } left++; } fastSort(nums, keyIndex + 1, end); fastSort(nums, start, keyIndex - 1); } "});index.add({'id':129,'href':'/interview/docs/offer/InversePairs/','title':"数组中的逆序对",'content':"题目 牛客网\n在数组中的两个数字，如果前面一个数字大于后面的数字，则这两个数字组成一个逆序对。输入一个数组,求出这个数组中的逆序对的总数P。并将P对1000000007取模的结果输出。 即输出P%1000000007\n输入描述: 题目保证输入的数组中没有的相同的数字\n数据范围：\n\t对于%50的数据,size\u0026lt;=10^4 对于%75的数据,size\u0026lt;=10^5 对于%100的数据,size\u0026lt;=2*10^5 解题思路 1. 使用归并排序的方式，划分子数组 2. 两个子数组进行对比，有两个分别指向两个数组末尾的指针 `f,s`，数组分割下标为 `mid`，如果 `array[f] \u0026gt; array[s]`那么，就有`s - mid`个 `array[f]` 的逆序 3. 依此类推，最终将数组排序，并且获得结果  public int InversePairs(int[] array) { long[] sum = {0}; if (array == null || array.length == 0) { return (int) sum[0]; } int[] temp = new int[array.length]; mergeSort(array, 0, array.length - 1, temp, sum); return (int) (sum[0] % 1000000007); } private void mergeSort(int[] array, int start, int end, int[] temp, long[] sum) { if (start == end) { return; } int mid = (start + end) / 2; mergeSort(array, start, mid, temp, sum); mergeSort(array, mid + 1, end, temp, sum); int f = mid, s = end; int t = end; while (f \u0026gt;= start \u0026amp;\u0026amp; s \u0026gt;= mid + 1) { if (array[f] \u0026gt; array[s]) { temp[t--] = array[f--]; sum[0] += s - mid; } else { temp[t--] = array[s--]; } } while (f \u0026gt;= start) { temp[t--] = array[f--]; } while (s \u0026gt;= mid + 1) { temp[t--] = array[s--]; } for (int i = end, j = end; i \u0026gt;= start; ) { array[j--] = temp[i--]; } } "});index.add({'id':130,'href':'/interview/docs/offer/Duplicate/','title':"数组中重复的数字",'content':"题目 在一个长度为n的数组里的所有数字都在0到 n-1 的范围内。 数组中某些数字是重复的，但不知道有几个数字是重复的。也不知道每个数字重复几次。请找出数组中任意一个重复的数字。 例如，如果输入长度为7的数组{2,3,1,0,2,5,3}，那么对应的输出是第一个重复的数字2。\n解题思路 解法一  由于数组内数字在 0 ~ n-1 的范围内，可以将数组按 数字做下标 进行重排序 将 n 放置到 num[n] 上，交换之前再判定在 num[n] 上是否为相同数字  public boolean duplicate(int numbers[], int length, int[] duplication) { if (numbers == null || numbers.length == 0) return false; for (int i = 0; i \u0026lt; numbers.length; i++) { while (numbers[i] != i) { int number = numbers[i]; int wrongNum = numbers[number]; if (number == wrongNum) { duplication[0] = number; return true; } swap(numbers, i, number); } } return false; } private static void swap(int[] nums, int i, int j) { int temp = nums[i]; nums[i] = nums[j]; nums[j] = temp; } 解法二  把数字 1 ~ n 划分为 1 ~ m、m+1 ~ n，统计两个子数组中每个数字在 1~n 出现的次数 如果出现的次数大于 m，那么重复数字一定在 1 ~ m 中 继续这样进行划分，可以找到重复数组  "});index.add({'id':131,'href':'/interview/docs/offer/NumberOfOneBetweenOneAndN/','title':"整数中",'content':"题目 牛客网\n求出1~13的整数中 1 出现的次数,并算出 100~1300 的整数中1出现的次数？为此他特别数了一下 1~13 中包含1的数字有 1、10、11、12、13 因此共出现 6 次,但是对于后面问题他就没辙了。ACMer 希望你们帮帮他,并把问题更加普遍化,可以很快的求出任意非负整数区间中1出现的次数（从1 到 n 中1出现的次数）。\n解题思路  假定 $$n=21345$$ 将数字分为首位和非首位两个部分 对于首位为 1 的情况，如果首位 $$\u0026gt;1$$ 那么$$sum=sum+10^{len(n)-1}$$，如果首位 $$=1$$ 那么 $$sum=sum+1$$ 对于非首位 1，指定其中一位为 1，根据排列组合有 $$10^{len(n)-2}\\times(len(n)-1)$$ 个。那么非首位 1 总共有 $$2\\times10^{len(n)-2}\\times(len(n)-1)$$  public int NumberOf1Between1AndN_Solution(int n) { int[] res = {0}; NumberOf1Between1AndN(res, n); return res[0]; } private void NumberOf1Between1AndN(int[] res, int n) { //假设 num=21345 String num = String.valueOf(n); int firstNum = num.charAt(0) - '0'; if (num.length() == 1) { if (firstNum \u0026gt; 0) res[0]++; return; } String nextNum = num.substring(1); int nextN = Integer.valueOf(nextNum); //数字 10000 ～ 19999 的第一位中的个数 if (firstNum \u0026gt; 1) { res[0] += Math.pow(10, num.length() - 1); } else if (firstNum == 1) { res[0] += nextN + 1; } //1346 ～ 21345 除第一位之外的数的个数 res[0] += firstNum * (num.length() - 1) * Math.pow(10, num.length() - 2); NumberOf1Between1AndN(res, nextN); } "});index.add({'id':132,'href':'/interview/docs/offer/fibonacci/','title':"斐波纳切数列",'content':"题目 牛客网\n大家都知道斐波那契数列，现在要求输入一个整数n，请你输出斐波那契数列的第n项（从0开始，第0项为0）。n\u0026lt;=39\n解题思路  递归计算很慢，是最简单的算法  public int Fibonacci(int n) { if (n == 0) { return 0; } if (n == 1) { return 1; } int l = 1, ll = 0; for (int i = 2; i \u0026lt;= n; i++) { int t = ll + l; ll = l; l = t; } return l; } "});index.add({'id':133,'href':'/interview/docs/offer/find-minimum-in-rotated-sorted-array/','title':"旋转数组的最小数字",'content':"题目 牛客网\n把一个数组最开始的若干个元素搬到数组的末尾，我们称之为数组的旋转。 输入一个非减排序的数组的一个旋转，输出旋转数组的最小元素。 例如数组{3,4,5,1,2}为{1,2,3,4,5}的一个旋转，该数组的最小值为1。 NOTE：给出的所有元素都大于0，若数组大小为0，请返回0。\n解题思路  旋转之后的数组存在两个上升序列，最小元素在两个上升序列的中间 用两个指针在两个序列中找到最大和最小的值，这样 end 指向的数则为最小  public int minNumberInRotateArray(int[] array) { if (array.length == 0) { return 0; } int start = 0, end = array.length - 1; while (end - start != 1) { int mid = (start + end) / 2; if (array[mid] \u0026gt;= array[start]) { start = mid; } if (array[mid] \u0026lt;= array[end]) { end = mid; } } return array[end]; } "});index.add({'id':134,'href':'/interview/docs/leetcode/lengthOfLongestSubstring/','title':"无重复字符的最长子串",'content':"头条重点\n题目 给定一个字符串，请你找出其中不含有重复字符的 最长子串 的长度。\n输入: \u0026quot;abcabcbb\u0026quot; 输出: 3 解释: 因为无重复字符的最长子串是 \u0026quot;abc\u0026quot;，所以其长度为 3。 解题思路  用 Map 记录字符所在位置，当遇到重复字符时，移动 start 指针 替换 Map 中下标，并计算子串长度  public int lengthOfLongestSubstring(String str) { if (str == null || str.length() == 0) return 0; HashMap\u0026lt;Character, Integer\u0026gt; temp = new HashMap\u0026lt;\u0026gt;(); char[] chars = str.toCharArray(); int res = 0, start = 0; for (int i = 0; i \u0026lt; chars.length; i++) { if (temp.containsKey(chars[i])) { start = Math.max(temp.put(chars[i], i) + 1, start); } temp.put(chars[i], i); res = Math.max(res, i - start + 1); } return res; } "});index.add({'id':135,'href':'/interview/docs/offer/replay-space/','title':"替换空格",'content':"题目 牛客网\n请实现一个函数，将一个字符串中的每个空格替换成“%20”。例如，当字符串为We Are Happy.则经过替换之后的字符串为 We%20Are%20Happy。\n解题思路  通过字符串中空格的个数，计算新字符串长度 两个指针进行字符串拷贝，当遇到‘ ’时替换为 %20  public String replaceSpace(StringBuffer str) { char[] chars = str.toString().toCharArray(); StringBuilder res = new StringBuilder(); for (char c : chars) { if (c == ' ') res.append(\u0026quot;%20\u0026quot;); else res.append(c); } return res.toString(); } "});index.add({'id':136,'href':'/interview/docs/leetcode/maxSubArray/','title':"最大子序和",'content':"头条重点\n题目 给定一个整数数组 nums ，找到一个具有最大和的连续子数组（子数组最少包含一个元素），返回其最大和。\n示例: 输入: [-2,1,-3,4,-1,2,1,-5,4], 输出: 6 解释: 连续子数组 [4,-1,2,1] 的和最大，为 6。 进阶: 如果你已经实现复杂度为 O(n) 的解法，尝试使用更为精妙的分治法求解。\n解题思路   动态规划：$$f(i)=\\begin{cases}num[i]\u0026amp;f(i-1)+num[i]\u0026lt;num[i]\\f(i-1)+num[i] \u0026amp;f(i-1)+num[i]\u0026gt;num[i]\\end{cases}$$\n  用result[i]保存以数字nums[i]结尾的最大子序和，然后不断更新result数组的最大值即可。总的时间复杂度O(n)\n  public int maxSubArray(int[] nums) { if (nums.length == 0) { return 0; } if (nums.length == 1) { return nums[0]; } int[] res = new int[nums.length]; res[0] = nums[0]; int max = res[0]; for (int i = 1; i \u0026lt; nums.length; i++) { int curMax = nums[i] + res[i - 1]; if (curMax \u0026gt; nums[i]) { res[i] = curMax; } else { res[i] = nums[i]; } max = Math.max(max, res[i]); } return max; } "});index.add({'id':137,'href':'/interview/docs/leetcode/MinStack/','title':"最小栈",'content':"头条重点\n题目 设计一个支持 push，pop，top 操作，并能在常数时间内检索到最小元素的栈。\npush(x) \u0026ndash; 将元素 x 推入栈中。 pop() \u0026ndash; 删除栈顶的元素。 top() \u0026ndash; 获取栈顶元素。 getMin() \u0026ndash; 检索栈中的最小元素。\n示例: MinStack minStack = new MinStack(); minStack.push(-2); minStack.push(0); minStack.push(-3); minStack.getMin(); --\u0026gt; 返回 -3. minStack.pop(); minStack.top(); --\u0026gt; 返回 0. minStack.getMin(); --\u0026gt; 返回 -2. 解题思路 class MinStack { /** initialize your data structure here. */ public MinStack() { } private LinkedList\u0026lt;Integer\u0026gt; stack = new LinkedList\u0026lt;\u0026gt;(); private Queue\u0026lt;Integer\u0026gt; minStack = new PriorityQueue\u0026lt;\u0026gt;(); public void push(int x) { stack.offerFirst(x); minStack.offer(x); } public void pop() { Integer poll = stack.pollFirst(); if (poll != null) { minStack.remove(poll); } } public int top() { Integer first = stack.peekFirst(); return first == null ? 0 : first; } public int getMin() { Integer first = minStack.peek(); return first == null ? 0 : first; } } "});index.add({'id':138,'href':'/interview/docs/basic/algo/mst/','title':"最小生成树算法",'content':"最小生成树算法   连通图：在无向图G中，若从顶点i到顶点j有路径，则称顶点i和顶点j是连通的。若图G中任意两个顶点都连通，则称G为连通图。\n  生成树：一个连通图的生成树是该连通图的一个极小连通子图，它含有全部顶点，但只有构成一个数的(n-1)条边。\n  最小生成树：对于一个带权连通无向图G中的不同生成树，各树的边上的 权值之和最小。构造最小生成树的准则有三条：\n 必须只使用该图中的边来构造最小生成树。 必须使用且仅使用(n-1)条边来连接图中的n个顶点。 不能使用产生回路的边。    Prim算法 假设G=(V,E)是一个具有n个顶点的带权连通无向图，T(U,TE)是G的最小生成树，其中U是T的顶点集，TE是T的边集，则由G构造从起始顶点v出发的最小生成树T的步骤为：\n  初始化U={v}，以v到其他顶点的所有边为候选边(U中所有点到其他顶点的边)。\n  重复以下步骤(n-1)次，使得其他(n-1)个顶点被加入到U中。\n  从候选边中挑选权值最小的边加入TE，设该边在V-U(这里是集合减)中的顶点是k，将k加入U中。\n  考察当前V-U中的所有顶点j，修改候选边，若边(k,j)的权值小于原来和顶点j关联的候选边，则用(k,j)取代后者作为候选边。\n    Kruskal算法 假设G=(V,E)是一个具有n个顶点的带权连通无向图，T(U,TE)是G的最小生成树，其中U是T的顶点集，TE是T的边集，则由G构造从起始顶点v出发的最小生成树T的步骤为：\n  置U的初始值等于V(即包含G中的全部顶点)，TE的初始值为空\n  将图G中的边按权值从小到大的顺序依次选取，若选取的边未使生成树T形成回路，则加入TE，否则放弃，知道TE中包含(n-1)条边为止。\n  "});index.add({'id':139,'href':'/interview/docs/offer/GetLeastNumbers/','title':"最小的",'content':"题目 牛客网\n输入n个整数，找出其中最小的K个数。例如输入4,5,1,6,2,7,3,8这8个数字，则最小的4个数字是1,2,3,4,。\n解题思路 Partition 该算法基于 Partition\npublic ArrayList\u0026lt;Integer\u0026gt; GetLeastNumbers_Solution_Partition(int[] input, int k) { ArrayList\u0026lt;Integer\u0026gt; res = new ArrayList\u0026lt;\u0026gt;(); if (k \u0026gt; input.length || k \u0026lt; 1) { return res; } int start = 0, end = input.length - 1; int index = partition(input, start, end); while (index != k - 1) { if (index \u0026gt; k - 1) { end = index - 1; index = partition(input, start, end); } else { start = index + 1; index = partition(input, start, end); } } for (int i = 0; i \u0026lt; input.length \u0026amp;\u0026amp; i \u0026lt; k; i++) { res.add(input[i]); } return res; } private int partition(int[] nums, int start, int end) { int left = start, right = end; int key = nums[left]; while (left \u0026lt; right) { while (left \u0026lt; right \u0026amp;\u0026amp; nums[right] \u0026gt; key) { right--; } if (left \u0026lt; right) { nums[left] = nums[right]; left++; } while (left \u0026lt; right \u0026amp;\u0026amp; nums[left] \u0026lt;= key) { left++; } if (left \u0026lt; right) { nums[right] = nums[left]; right++; } } nums[left] = key; return left; } 小根堆算法 该算法基于小根堆，适合海量数据，时间复杂度为：n*logk\npublic ArrayList\u0026lt;Integer\u0026gt; GetLeastNumbers_Solution(int[] input, int k) { ArrayList\u0026lt;Integer\u0026gt; res = new ArrayList\u0026lt;\u0026gt;(); if (k \u0026gt; input.length||k==0) { return res; } for (int i = input.length - 1; i \u0026gt;= 0; i--) { minHeap(input, 0, i); swap(input, 0, i); res.add(input[i]); if (res.size() == k) break; } return res; } private void minHeap(int[] heap, int start, int end) { if (start == end) { return; } int childLeft = start * 2 + 1; int childRight = childLeft + 1; if (childLeft \u0026lt;= end) { minHeap(heap, childLeft, end); if (heap[childLeft] \u0026lt; heap[start]) { swap(heap, start, childLeft); } } if (childRight \u0026lt;= end) { minHeap(heap, childRight, end); if (heap[childRight] \u0026lt; heap[start]) { swap(heap, start, childRight); } } } private void swap(int[] nums, int a, int b) { int t = nums[a]; nums[a] = nums[b]; nums[b] = t; } "});index.add({'id':140,'href':'/interview/docs/offer/GetLeastNumbersSolution/','title':"最小的",'content':"题目 牛客网\n输入n个整数，找出其中最小的K个数。例如输入4,5,1,6,2,7,3,8这8个数字，则最小的4个数字是1,2,3,4,。\n解题思路  利用堆排序原理，计算出最小的 k 个数  public ArrayList\u0026lt;Integer\u0026gt; GetLeastNumbers_Solution(int[] input, int k) { ArrayList\u0026lt;Integer\u0026gt; res = new ArrayList\u0026lt;\u0026gt;(); if (k \u0026gt; input.length || k == 0) { return res; } for (int i = input.length - 1; i \u0026gt;= 0; i--) { minHeap(input, 0, i); swap(input, 0, i); res.add(input[i]); if (res.size() == k) break; } return res; } private void minHeap(int[] heap, int start, int end) { if (start == end) { return; } int childLeft = start * 2 + 1; int childRight = childLeft + 1; if (childLeft \u0026lt;= end) { minHeap(heap, childLeft, end); if (heap[childLeft] \u0026lt; heap[start]) { swap(heap, start, childLeft); } } if (childRight \u0026lt;= end) { minHeap(heap, childRight, end); if (heap[childRight] \u0026lt; heap[start]) { swap(heap, start, childRight); } } } private void swap(int[] nums, int a, int b) { int t = nums[a]; nums[a] = nums[b]; nums[b] = t; } "});index.add({'id':141,'href':'/interview/docs/basic/algo/path/','title':"最短路径算法",'content':"最短路径算法 Dijkstra —— 贪心算法  从一个顶点到其余顶点的最短路径\n 设G=(V,E)是一个带权有向图，把图中顶点集合V分成两组，第1组为已求出最短路径的顶点（用S表示，初始时S只有一个源点，以后每求得一条最短路径v,...k，就将k加到集合S中，直到全部顶点都加入S）。第2组为其余未确定最短路径的顶点集合（用U表示），按最短路径长度的递增次序把第2组的顶点加入S中。\n步骤： 1. 初始时，S只包含源点，即`S={v}`，顶点v到自己的距离为0。U包含除v外的其他顶点，v到U中顶点i的距离为边上的权。 2. 从U中选取一个顶点u，顶点v到u的距离最小，然后把顶点u加入S中。 3. 以顶点u为新考虑的中间点，修改v到U中各个点的距离。 4. 重复以上步骤知道S包含所有顶点。 Floyd —— 动态规划 Floyd 算法是解决任意两点间的最短路径的一种算法，可以正确处理有向图或负权（但不可存在负权回路）的最短路径问题。该算法的时间复杂度为 $$O(N^{3})$$，空间复杂度为 $$O(N^{2})$$\n设 $$D_{i,j,k}$$ 为从 $$i$$ 到 $$j$$ 的只以 $$(1..k)$$ 集合中的节点为中间节点的最短路径的长度。\n$$ D_{i,j,k}=\\begin{cases} D_{i,j,k-1} \u0026amp; 最短路径不经过 k\\\nD_{i,k,k-1}+D_{k,j,k-1} \u0026amp; 最短路径经过 k \\end{cases} $$\n因此， $$D_{i,j,k}=min(D_{i,k,k-1}+D_{k,j,k-1},D_{i,j,k-1})$$。伪代码描述如下：\n// let dist be a |V| × |V| array of minimum distances initialized to ∞ (infinity) for each vertex v dist[v][v] ← 0 for each edge (u,v) dist[u][v] ← w(u,v) // the weight of the edge (u,v) for k from 1 to |V| for i from 1 to |V| for j from 1 to |V| if dist[i][j] \u0026gt; dist[i][k] + dist[k][j] dist[i][j] ← dist[i][k] + dist[k][j] end if "});index.add({'id':142,'href':'/interview/docs/offer/LongestNoRepeatSubString/','title':"最长不含重复字符的子字符串",'content':"题目 LeetCode\n给定一个字符串，请你找出其中不含有重复字符的 最长子串 的长度。\n输入: \u0026quot;abcabcbb\u0026quot; 输出: 3 解释: 因为无重复字符的最长子串是 \u0026quot;abc\u0026quot;，所以其长度为 3。 解题思路  用 Map 记录字符所在位置，当遇到重复字符时，移动 start 指针 替换 Map 中下标，并计算子串长度  public int longestNoRepeatSubString(String str) { if (str == null || str.length() == 0) return 0; HashMap\u0026lt;Character, Integer\u0026gt; temp = new HashMap\u0026lt;\u0026gt;(); char[] chars = str.toCharArray(); int res = 0, start = 0; for (int i = 0; i \u0026lt; chars.length; i++) { if (temp.containsKey(chars[i])) { start = Math.max(temp.put(chars[i], i) + 1, start); } temp.put(chars[i], i); res = Math.max(res, i - start + 1); } return res; } "});index.add({'id':143,'href':'/interview/docs/leetcode/longestCommonPrefix/','title':"最长公共前缀",'content':"题目 编写一个函数来查找字符串数组中的最长公共前缀。\n如果不存在公共前缀，返回空字符串 \u0026ldquo;\u0026quot;。\n示例 1: 输入: [\u0026quot;flower\u0026quot;,\u0026quot;flow\u0026quot;,\u0026quot;flight\u0026quot;] 输出: \u0026quot;fl\u0026quot; 解题思路  找到最短字符串 多个字符串逐个字符比较  public String longestCommonPrefix(String[] strs) { if (strs.length == 0) { return \u0026quot;\u0026quot;; } int minLen = strs[0].length(); for (String str : strs) { minLen = Math.min(minLen, str.length()); } char[][] data = new char[strs.length][minLen]; for (int i = 0; i \u0026lt; strs.length; i++) { char[] chars = strs[i].toCharArray(); System.arraycopy(chars, 0, data[i], 0, minLen); } StringBuilder res = new StringBuilder(); for (int i = 0; i \u0026lt; minLen; i++) { for (int j = 1; j \u0026lt; data.length; j++) { if (data[j - 1][i] != data[j][i]) { return res.toString(); } } res.append(data[0][i]); } return res.toString(); } "});index.add({'id':144,'href':'/interview/docs/leetcode/longestConsecutive/','title':"最长连续序列",'content':"题目 给定一个未排序的整数数组，找出最长连续序列的长度。\n要求算法的时间复杂度为 O(n)。\n示例: 输入: [100, 4, 200, 1, 3, 2] 输出: 4 解释: 最长连续序列是 [1, 2, 3, 4]。它的长度为 4。 解题思路  用 Set 保存所有数字 遍历数组，查找当前数字之前、之后的数，并计算个数  public static int longestConsecutive(int[] nums) { if (nums.length \u0026lt;= 1) { return nums.length; } Set\u0026lt;Integer\u0026gt; set = new HashSet\u0026lt;\u0026gt;(); for (int num : nums) { set.add(num); } int pre, after, max = 0; for (int num : nums) { int temp = 1; set.remove(num); pre = num; after = num; while (set.contains(--pre)) { temp++; set.remove(pre); } while (set.contains(++after)) { temp++; set.remove(after); } max = Math.max(max, temp); if (max \u0026gt; nums.length / 2) { return max; } } return max; } "});index.add({'id':145,'href':'/interview/docs/leetcode/findLengthOfLCIS/','title':"最长连续递增序列",'content':"题目 给定一个未经排序的整数数组，找到最长且连续的的递增序列。\n示例 1: 输入: [1,3,5,4,7] 输出: 3 解释: 最长连续递增序列是 [1,3,5], 长度为3。 尽管 [1,3,5,7] 也是升序的子序列, 但它不是连续的，因为5和7在原数组里被4隔开。 示例 2: 输入: [2,2,2,2,2] 输出: 1 解释: 最长连续递增序列是 [2], 长度为1。 解题思路  用两个变量记录序列开始和结束的下标 从左到右遍历，如果下一个节点小当前节点则移动 start，否则移动end，并更新 max  public static int findLengthOfLCIS(int[] nums) { if (nums.length == 0) { return 0; } if (nums.length == 1) { return 1; } int start = 0, end = 0; int max = 1; for (int i = 1; i \u0026lt; nums.length; i++) { if (nums[i] \u0026gt; nums[i - 1]) { end = i; max = Math.max(max, end - start + 1); } else { start = i; } } return max; } "});index.add({'id':146,'href':'/interview/docs/leetcode/findCircleNum/','title':"朋友圈",'content':"朋友圈 头条重点\n题目 班上有 N 名学生。其中有些人是朋友，有些则不是。他们的友谊具有是传递性。如果已知 A 是 B 的朋友，B 是 C 的朋友，那么我们可以认为 A 也是 C 的朋友。所谓的朋友圈，是指所有朋友的集合。\n给定一个 N * N 的矩阵 M，表示班级中学生之间的朋友关系。如果M[i][j] = 1，表示已知第 i 个和 j 个学生互为朋友关系，否则为不知道。你必须输出所有学生中的已知的朋友圈总数。\n示例 1: 输入: [[1,1,0], [1,1,0], [0,0,1]] 输出: 2 说明：已知学生0和学生1互为朋友，他们在一个朋友圈。 第2个学生自己在一个朋友圈。所以返回2。 示例 2: 输入: [[1,1,0], [1,1,1], [0,1,1]] 输出: 1 说明：已知学生0和学生1互为朋友，学生1和学生2互为朋友，所以学生0和学生2也是朋友，所以他们三个在一个朋友圈，返回1。 注意：\n N 在[1,200]的范围内。 对于所有学生，有M[i][i] = 1。 如果有M[i][j] = 1，则有M[j][i] = 1。  解题思路  逐个遍历所有学生，将他所有朋友标记  public int findCircleNum(int[][] M) { if (M.length == 0) { return 0; } int[] marks = new int[M.length]; int total = 0; for (int i = 0; i \u0026lt; M.length; i++) { if (marks[i] != 1) { total++; dfs(M, marks, i); } } return total; } private void dfs(int[][] M, int[] marks, int i) { marks[i] = 1; for (int j = 0; j \u0026lt; M[i].length; j++) { if (M[i][j] == 1 \u0026amp;\u0026amp; marks[j] != 1) { dfs(M, marks, j); } } } "});index.add({'id':147,'href':'/interview/docs/offer/MovingCount/','title':"机器人的运动范围",'content':"题目 地上有一个m行和n列的方格。一个机器人从坐标 0,0 的格子开始移动，每一次只能向左，右，上，下四个方向移动一格，但是不能进入行坐标和列坐标的数位之和大于k的格子。 例如，当k为 18 时，机器人能够进入方格（35,37），因为3+5+3+7 = 18。但是，它不能进入方格（35,38），因为3+5+3+8 = 19。请问该机器人能够达到多少个格子？\n解题思路  "});index.add({'id':148,'href':'/interview/docs/basic/algo/search/','title':"查找算法",'content':"查找算法 ASL 由于查找算法的主要运算是关键字的比较，所以通常把查找过程中对关键字的平均比较次数（平均查找长度）作为衡量一个查找算法效率的标准。ASL= ∑(n,i=1) Pi*Ci，其中n为元素个数，Pi是查找第i个元素的概率，一般为Pi=1/n，Ci是找到第i个元素所需比较的次数。\n顺序查找 原理是让关键字与队列中的数从最后一个开始逐个比较，直到找出与给定关键字相同的数为止，它的缺点是效率低下。时间复杂度o(n)。\n折半查找 折半查找要求线性表是有序表。搜索过程从数组的中间元素开始，如果中间元素正好是要查找的元素，则搜索过程结束；如果某一特定元素大于或者小于中间元素，则在数组大于或小于中间元素的那一半中查找，而且跟开始一样从中间元素开始比较。如果在某一步骤数组为空，则代表找不到。这种搜索算法每一次比较都使搜索范围缩小一半。折半搜索每次把搜索区域减少一半，时间复杂度为O(log n)。\n 可以借助二叉判定树求得折半查找的平均查找长度：log2(n+1)-1。 折半查找在失败时所需比较的关键字个数不超过判定树的深度，n个元素的判定树的深度和n个元素的完全二叉树的深度相同log2(n)+1。  public int binarySearchStandard(int[] num, int target){ int start = 0; int end = num.length - 1; while(start \u0026lt;= end){ //注意1 int mid = start + ((end - start) \u0026gt;\u0026gt; 1); if(num[mid] == target) return mid; else if(num[mid] \u0026gt; target){ end = mid - 1; //注意2 } else{ start = mid + 1; //注意3 } } return -1; }   如果是start \u0026lt; end，那么当target等于num[num.length-1]时，会找不到该值。\n  因为num[mid] \u0026gt; target, 所以如果有num[index] == target, index一定小于mid，能不能写成end = mid呢？举例来说：num = {1, 2, 5, 7, 9}; 如果写成end = mid，当循环到start = 0, end = 0时（即num[start] = 1, num[end] = 1时），mid将永远等于0，此时end也将永远等于0，陷入死循环。也就是说寻找target = -2时，程序将死循环。\n  因为num[mid] \u0026lt; target, 所以如果有num[index] == target, index一定大于mid，能不能写成start = mid呢？举例来说：num = {1, 2, 5, 7, 9}; 如果写成start = mid，当循环到start = 3, end = 4时（即num[start] = 7, num[end] = 9时），mid将永远等于3，此时start也将永远等于3，陷入死循环。也就是说寻找target = 9时，程序将死循环。\n  分块查找 分块查找又称索引顺序查找，它是一种性能介于顺序查找和折半查找之间的查找方法。分块查找由于只要求索引表是有序的，对块内节点没有排序要求，因此特别适合于节点动态变化的情况。\n"});index.add({'id':149,'href':'/interview/docs/offer/IsPopOrder/','title':"栈的压入",'content':"题目 牛客网\n输入两个整数序列，第一个序列表示栈的压入顺序，请判断第二个序列是否可能为该栈的弹出顺序。假设压入栈的所有数字均不相等。例如序列1,2,3,4,5是某栈的压入顺序，序列4,5,3,2,1是该压栈序列对应的一个弹出序列，但4,3,5,1,2就不可能是该压栈序列的弹出序列。（注意：这两个序列的长度是相等的）\n解题思路  通过 Stack 进行模拟 push，当 pop 的节点等于 Stack 的 top 节点时，pop Stack 最后如果 Stack 剩余数据，则判定为 false  public boolean IsPopOrder(int[] pushA, int[] popA) { if (pushA.length != popA.length) { return false; } if (pushA.length == 0) { return false; } LinkedList\u0026lt;Integer\u0026gt; stack = new LinkedList\u0026lt;\u0026gt;(); int j = 0; for (int value : pushA) { stack.addLast(value); while (stack.peekLast() != null \u0026amp;\u0026amp; popA[j] == stack.getLast()) { j++; stack.removeLast(); } } return stack.isEmpty(); } "});index.add({'id':150,'href':'/interview/docs/basic/algo/tree/','title':"树",'content':"树 二叉树 L、D、R分别表示遍历左子树、访问根结点和遍历右子树\n 先序遍历：DLR 中序遍历：LDR 后序遍历：LRD   仅有前序和后序遍历，不能确定一个二叉树，必须有中序遍历的结果\n 二叉树的性质  性质1：在二叉树中第 i 层的结点数最多为2^(i-1)（i ≥ 1） 性质2：高度为k的二叉树其结点总数最多为2^k－1（ k ≥ 1） 性质3：对任意的非空二叉树 T ，如果叶结点的个数为 n0，而其度为 2 的结点数为 n2，则：n0 = n2 + 1  满二叉树 深度为k，且有2^k-1个节点称之为满二叉树；\n 性质4：第i层上的节点数为2^(i-1)；  完全二叉树 深度为k，有n个节点的二叉树，当且仅当其每一个节点都与深度为k的满二叉树中，序号为1至n的节点对应时，称之为完全二叉树。\n 性质5：对于具有n个结点的完全二叉树的高度为log2(n)+1  求完全二叉树的叶子结点个数：\n二叉树的构造 //n 表示当前结点字符 Node* tree(vector\u0026lt;char\u0026gt; data, int n) { Node* node; if (n \u0026gt;= data.size()) return NULL; if (data[n] == \u0026#39;#\u0026#39;) return NULL; node = new Node; node-\u0026gt;data = data[n]; node-\u0026gt;left = tree(data, n + 1); node-\u0026gt;right = tree(data, n + 2); return node; } 堆 堆通常是一个可以被看做一棵树的数组对象。堆的实现通过构造二叉堆（binary heap），实为二叉树的一种；\n 任意节点小于（或大于）它的所有后裔，最小元（或最大元）在堆的根上（堆序性）。 堆总是一棵完全树。即除了最底层，其他层的节点都被元素填满，且最底层尽可能地从左到右填入。  将根节点最大的堆叫做最大堆或大根堆，根节点最小的堆叫做最小堆或小根堆。常见的堆有二叉堆、斐波那契堆等。\n通常堆是通过一维数组来实现的。在数组起始位置为1的情形中：\n 父节点i的左子节点在位置(2*i); 父节点i的右子节点在位置(2*i+1); 子节点i的父节点在位置(i/2);  霍夫曼树 霍夫曼树又称最优二叉树，是一种带权路径长度最短的二叉树。所谓树的带权路径长度，就是树中所有的叶结点的权值乘上其到根结点的路径长度（若根结点为0层，叶结点到根结点的路径长度为叶结点的层数）。树的路径长度是从树根到每一结点的路径长度之和，记为WPL=（W1L1+W2L2+W3L3+\u0026hellip;+WnLn），N个权值Wi（i=1,2,\u0026hellip;n）构成一棵有N个叶结点的二叉树，相应的叶结点的路径长度为Li（i=1,2,\u0026hellip;n）。可以证明霍夫曼树的WPL是最小的。\n霍夫曼树构造  根据给定的n个权值(W1,W2...Wn)，使对应节点构成n个二叉树的森林T=(T1,T2...Tn)，其中每个二叉树Ti(1 \u0026lt;= i \u0026lt;= n)中都有一个带权值为Wi的根节点，其左、右子树均为空。 在森林T中选取两个节点权值最小的子树，分别作为左、右子树构造一个新的二叉树，且置新的二叉树的根节点的权值为其左右子树上根节点权值之和。 在森林T中，用新得到的二叉树替代选取的两个二叉树。 重复2和3，直到T只包含一个树为止。这个数就是霍夫曼树。   定理：对于具有n个叶子节点的霍夫曼树，共有2n-1个节点。这是由于霍夫曼树只有度为0和度为2的结点，根据二叉树的性质 n0 = n2 + 1，因此度为2的结点个数为n-1个，总共有2n-1个节点。\n 霍夫曼编码 对于一个霍夫曼树，所有左链接取'0\u0026rsquo;、右链接取'1\u0026rsquo;。从树根至树叶依序记录所有字母的编码。\n带权路径  结点的权：若将树中结点赋给一个有着某种含义的数值，则这个数值称为该结点的权。 结点的带权路径：从根结点到该结点之间的路径长度与该结点的权的乘积。 树的带权路径：所有叶子结点的带权路径长度之和，记为WPL。  二叉排序树 二叉查找树，也称二叉搜索树、有序二叉树，排序二叉树，是指一棵空树或者具有下列性质的二叉树：\n 任意节点的左子树不空，则左子树上所有结点的值均小于它的根结点的值； 任意节点的右子树不空，则右子树上所有结点的值均大于它的根结点的值； 任意节点的左、右子树也分别为二叉查找树； 没有键值相等的节点。  二分查找的时间复杂度是O(log(n))，最坏情况下的时间复杂度是O(n)（相当于顺序查找）\n平衡二叉树 平衡树是计算机科学中的一类改进的二叉查找树。一般的二叉查找树的查询复杂度是跟目标结点到树根的距离（即深度）有关，因此当结点的深度普遍较大时，查询的均摊复杂度会上升，为了更高效的查询，平衡树应运而生了。平衡指所有叶子的深度趋于平衡，更广义的是指在树上所有可能查找的均摊复杂度偏低。\nAVL树 AVL树是最先发明的 自平衡二叉查找树。在AVL树中任何节点的两个子树的高度最大差别为一，所以它也被称为高度平衡树。\n 它的左子树和右子树都是平衡二叉树。 左子树和右子树的深度之差的绝对值不超过1。  增加和删除可能需要通过一次或多次树旋转来重新平衡这个树。\n 右旋：左结点转到根节点位置。 左旋：右节点转到根节点位置。   高度为k的AVL树，节点数N最多2^k -1，即满二叉树；\n 红黑树 红黑树是一种自平衡二叉查找树，每个节点都带有颜色属性的二叉查找树，颜色为红色或黑色。在二叉查找树强制一般要求以外，对于任何有效的红黑树我们增加了如下的额外要求：\n 节点是红色或黑色。 根是黑色。 所有叶子都是黑色（叶子是NIL节点）。 每个红色节点必须有两个黑色的子节点。（从每个叶子到根的所有路径上不能有两个连续的红色节点。） 从任一节点到其每个叶子的所有简单路径都包含相同数目的黑色节点。   如果一条路径上的顶点除了起点和终点可以相同外，其它顶点均不相同，则称此路径为一条简单路径；起点和终点相同的简单路径称为回路（或环）。\n  红黑树相对于AVL树来说，牺牲了部分平衡性以换取插入/删除操作时少量的旋转操作，整体来说性能要优于AVL树。\n 这些约束确保了红黑树的关键特性：从根到叶子的最长的可能路径不多于最短的可能路径的两倍长。结果是这个树大致上是平衡的。因为操作比如插入、删除和查找某个值的最坏情况时间都要求与树的高度成比例，这个在高度上的理论上限 允许红黑树在最坏情况下都是高效的，而不同于普通的二叉查找树。\n在很多树数据结构的表示中，一个节点有可能只有一个子节点，而叶子节点包含数据。用这种范例表示红黑树是可能的，但是这会改变一些性质并使算法复杂。为此，本文中我们使用\u0026quot;nil叶子\u0026quot;或\u0026quot;空（null）叶子\u0026rdquo;，如上图所示，它不包含数据而只充当树在此结束的指示。这些节点在绘图中经常被省略，导致了这些树好像同上述原则相矛盾，而实际上不是这样。与此有关的结论是所有节点都有两个子节点，尽管其中的一个或两个可能是空叶子。\n因为每一个红黑树也是一个特化的二叉查找树，因此红黑树上的只读操作与普通二叉查找树上的只读操作相同。然而，在红黑树上进行插入操作和删除操作会导致不再符合红黑树的性质。恢复红黑树的性质需要少量（O(log n)）的颜色变更（实际是非常快速的）和不超过三次树旋转（对于插入操作是两次）。虽然插入和删除很复杂，但操作时间仍可以保持为O(log n)次。\nB-树 是一种多路搜索树（并不是二叉的）：\n 所有叶子结点位于同一层，并且 不带信息。 树中每个节点最多有m个子树(即至多含有m-1个关键字)。 若根节点不是终端节点，则根节点子树[2,m]. 除根节点外其他非叶子节点至少有[m/2]个子树(即至少含有[m/2]-1个关键字)。 每个非叶子节点的结构为：  | n | p0 | k1 | p1 | k2 | p2 | \u0026hellip; | kn | pn |\n n为该节点中的关键字个数，除根节点外，其他所有非叶子节点的关键字个数n：[m/2]-1 \u0026lt;= n \u0026lt;= m-1;\n  ki(i \u0026lt;= i \u0026lt;=n)为该节点的关键字且满足ki \u0026lt; ki+1\n  pi(0 \u0026lt;= i \u0026lt;=n)为该节点的孩子节点指针pi(0 \u0026lt;= i \u0026lt;=n-1)所指节点上的关键字大于等于ki且小于ki+1，pn所指节点上的关键字大于kn.\n  B-树的阶：所有节点的孩子节点数的最大值\n B-树的查找 在B-树中的查找给定关键字的方法 类似于二叉排序树上的查找，不同的是在每个节点上确定向下查找的路径不一定是二路的，而是n+1路的。因为节点内的关键字序列key[1..n]有序，故既可以使用顺序查找，也可以使用二分查找。在一棵B-树上查找关键字为k的方法为：将k与根节点中的key[i]进行比较：\n 若k=key[i]，则查找成功； 若k\u0026lt;key[1]，则沿指针ptr[0]所指的子树继续查找； 若key[i]\u0026lt;k\u0026lt;key[i+1]，则沿着指针ptr[i]所指的子树继续查找； 若k\u0026gt;key[n]，则沿着指针ptr[n]所指的子树继续查找。  B-树的插入 将关键字k插入到B-树的过程分两步完成：\n  利用B-树的查找算法查找出该关键字的插入节点(注意B-树的插入节点一定属于最低非叶子节点层)。\n  判断该节点是否还有空位，即判断该节点是否满足n \u0026lt; m-1，若满足：直接把关键字k插入到该节点合适位置上；若不满足：分裂节点，取一新节点，把原节点上的关键字和k按升序排列后，从中间位置(m/2)处把关键字(不包括中间位置的关键字)分成两部分，左部分所含关键字放在旧节点中，右部分关键字放在新节点中，中间位置的关键字连同新节点的存储位置插入到双亲节点。如果双亲节点的关键字个数也超出max则再分裂。\n  B-树的删除 首先查找B树中需删除的元素，如果该元素在B树中存在，则将该元素在其结点中进行删除；如果删除该元素后，首先判断该元素是否有左右孩子结点，如果有，则上移孩子结点中的某相近元素到父节点中，然后是移动之后的情况；如果没有，直接删除后，然后是移动之后的情况。\n删除元素，移动相应元素之后，如果某结点中元素数目（即关键字数）小于Min(m/2)-1，则需要看其某相邻兄弟结点是否丰满，如果丰满，则向父节点借一个元素来满足条件；如果其相邻兄弟都刚脱贫，即借了之后其结点数目小于Min(m/2)-1，则该结点与其相邻的某一兄弟结点进行“合并”成一个结点，\nB+树 是一种自平衡二叉树，通常用于数据库和操作系统的文件系统中。B+树的特点是能够保持数据稳定有序，其插入与修改拥有较稳定的对数时间复杂度。B+树元素自底向上插入，这与二叉树恰好相反。B+树不需要象其他自平衡二叉查找树那样经常的重新平衡。\nB+树是B-树的变体，也是一种多路搜索树：\n 每个分支节点最多m个子树 根节点没有子树或至少两个子树 除根节点外，其他每个分支节点至少[m/2]个子树 有n个子树的节点有n个关键字 所有叶子节点包含全部关键字及指向相应记录的指针，而且叶子节点按关键字大小顺序链接(可以把每个叶子及诶单看成一个基本索引块，它的指针不再指向另一级索引块，而是直接指向数据文件中的记录) 所有分支节点中仅仅包含它的哥哥子节点(即下级索引块)中最大关键字及指向子节点的指针。  m阶的B+树和B-树的主要差异如下：\n 在B+树中，具有n个关键字的节点含有n个子树，即每个关键字对应一个子树，而在B-树中，具有n个关键字的节点含有(n+1)个子树。 在B+树中，每个节点(除根节点外)中的关键字个数n的取值范围是[m/2] \u0026lt;= n \u0026lt;= m，根节点n的取值范围2 \u0026lt;=n \u0026lt;=m；而在B-树中，除根节点外，其他所有非叶子节点的关键字个数：[m/2]-1 \u0026lt;= n \u0026lt;= m-1，根节点关键字个数为1 \u0026lt;= n \u0026lt;= m-1 B+树中所有叶子节点包含了全部关键字，即其他非叶子节点中的关键字包含在叶子节点中，而在B-树中，关键字是不重复的。 B+树中所有非叶子节点仅起到索引的作用，即节点中每个索引项值含有对应子树的最大关键字和指向该子树的指针，不含有该关键字对应记录的存储地址。而在B-树中，每个关键字对应一个记录的存储地址。 通常B+树上有两个头指针，一个指向根节点，另一个指向关键字最小的叶子节点，所有叶子节点链接成一个不定长的线性表。  B+树的查找 在B+树中可以采用两种查找方式：\n 直接从最小关键字开始顺序查找。 从B+树的根节点开始随机查找。这种查找方式与B-树的查找方式类似，只是在分支节点上的关键字与查找值相等时，查找并不会结束，要继续查到叶子节点为止，此时若查找成功，则按所给指针取出对应元素。  在B+树中，不管查找是否成功，每次查找都是经历一条树从根节点到叶子节点的路径。\nB+树的插入  首先，查找要插入其中的节点的位置。接着把值插入这个节点中。 如果没有节点处于违规状态则处理结束。 如果某个节点有过多元素，则把它分裂为两个节点，每个都有最小数目的元素。在树上递归向上继续这个处理直到到达根节点，如果根节点被分裂，则创建一个新根节点。为了使它工作，元素的最小和最大数目典型的必须选择为使最小数不小于最大数的一半。  B+树的删除  首先，查找要删除的值。接着从包含它的节点中删除这个值。 如果没有节点处于违规状态则处理结束。 如果节点处于违规状态则有两种可能情况： - 它的兄弟节点，就是同一个父节点的子节点，可以把一个或多个它的子节点转移到当前节点，而把它返回为合法状态。如果是这样，在更改父节点和两个兄弟节点的分离值之后处理结束。 - 它的兄弟节点由于处在低边界上而没有额外的子节点。在这种情况下把两个兄弟节点合并到一个单一的节点中，而且我们递归到父节点上，因为它被删除了一个子节点。持续这个处理直到当前节点是合法状态或者到达根节点，在其上根节点的子节点被合并而且合并后的节点成为新的根节点。   B-树和B+树 主要用于外部查找，即数据在外存中。\n B+树的优势所在 为什么说B+树比B-树更适合实际应用中操作系统的文件索引和数据库索引？\n B+树的磁盘读写代价更低  我们都知道磁盘时可以块存储的，也就是同一个磁道上同一盘块中的所有数据都可以一次全部读取。而B+树的内部结点并没有指向关键字具体信息的指针(比如文件内容的具体地址 ） 。因此其内部结点相对B-树更小。如果把所有同一内部结点的关键字存放在同一盘块中，那么盘块所能容纳的关键字数量也越多。这样，一次性读入内存中的需要查找的关键字也就越多。**相对来说IO读写次数也就降低了**。 举个例子，假设磁盘中的一个盘块容纳`16bytes`，而一个关键字`2bytes`，一个关键字具体信息指针`2bytes`。一棵9阶B-树(一个结点最多8个关键字)的内部结点需要2个盘块。而B+树内部结点只需要1个盘块。**当需要把内部结点读入内存中的时候，B-树就比B+数多一次盘块查找时间（在磁盘中就是盘片旋转的时间）**。  B+树的查询效率更加稳定。  由于非终结点并不是最终指向文件内容的结点，而只是叶子结点中关键字的索引。所以任何关键字的查找必须走一条从根结点到叶子结点的路。**所有关键字查询的路径长度相同，导致每一个数据的查询效率相当**。  Trie树 Trie树，又称前缀树，字典树， 是一种有序树，用于保存关联数组，其中的键通常是字符串。与二叉查找树不同，键不是直接保存在节点中，而是由节点在树中的位置决定。一个节点的所有子孙都有相同的前缀，也就是这个节点对应的字符串，而根节点对应空字符串。一般情况下，不是所有的节点都有对应的值，只有叶子节点和部分内部节点所对应的键才有相关的值。\nTrie树查询和插入时间复杂度都是 O(n)，是一种以空间换时间的方法。当节点树较多的时候，Trie 树占用的内存会很大。\nTrie树常用于搜索提示。如当输入一个网址，可以自动搜索出可能的选择。当没有完全匹配的搜索结果，可以返回前缀最相似的可能。\n"});index.add({'id':151,'href':'/interview/docs/offer/HasSubtree/','title':"树的子结构",'content':"题目 牛客网\n输入两棵二叉树A，B，判断B是不是A的子结构。（ps：我们约定空树不是任意一个树的子结构）\n解题思路  遍历查找相等根节点 通过递归查找当前根节点下是否包含子树 root2  public boolean HasSubtree(TreeNode root1, TreeNode root2) { if (root2 == null) { return false; } LinkedList\u0026lt;TreeNode\u0026gt; pipeline = new LinkedList\u0026lt;\u0026gt;(); pipeline.addLast(root1); while (!pipeline.isEmpty()) { TreeNode node = pipeline.pop(); if (node == null) { continue; } pipeline.addLast(node.left); pipeline.addLast(node.right); if (node.val == root2.val \u0026amp;\u0026amp; isSub(node, root2)) { return true; } } return false; } private boolean isSub(TreeNode root1, TreeNode root2) { if (root1 == null \u0026amp;\u0026amp; root2 == null) { return true; } if (root1 == null) { return false; } if (root2 == null) { return true; } if (root1.val == root2.val) { return isSub(root1.left, root2.left) \u0026amp;\u0026amp; isSub(root1.right, root2.right); } else { return false; } } "});index.add({'id':152,'href':'/interview/docs/offer/PatternMatch/','title':"正则表达式匹配",'content':"请实现一个函数用来匹配包括'.'和'*'的正则表达式。模式中的字符'.'表示任意一个字符，而'*'表示它前面的字符可以出现任意次（包含0次）。 在本题中，匹配是指字符串的所有字符匹配整个模式。例如，字符串\u0026quot;aaa\u0026quot;与模式\u0026quot;a.a\u0026quot;和\u0026quot;ab*ac*a\u0026quot;匹配，但是与\u0026quot;aa.a\u0026quot;和\u0026quot;ab*a\u0026quot;均不匹配\n解题思路  对于 * 有三种匹配模式：匹配0次，1次以及多次 对于 . 只有一种匹配模式  public boolean match(char[] str, char[] pattern) { if (str.length == 0 \u0026amp;\u0026amp; new String(pattern).replaceAll(\u0026quot;.\\\\*\u0026quot;, \u0026quot;\u0026quot;).length() == 0) { return true; } return match(str, 0, pattern, 0); } private boolean match(char[] str, int i, char[] pattern, int j) { if (i == str.length \u0026amp;\u0026amp; j == pattern.length) { return true; } if (j \u0026gt;= pattern.length) return false; if (j + 1 \u0026lt; pattern.length \u0026amp;\u0026amp; pattern[j + 1] == '*') { if ((i \u0026lt; str.length \u0026amp;\u0026amp; pattern[j] == str[i]) || (pattern[j] == '.' \u0026amp;\u0026amp; i != str.length)) { return match(str, i + 1, pattern, j + 2) || match(str, i + 1, pattern, j) || match(str, i, pattern, j + 2); } else { return match(str, i, pattern, j + 2); } } if ((i \u0026lt; str.length \u0026amp;\u0026amp; pattern[j] == str[i]) || (pattern[j] == '.' \u0026amp;\u0026amp; i != str.length)) { return match(str, i + 1, pattern, j + 1); } return false; } "});index.add({'id':153,'href':'/interview/docs/offer/sum/','title':"求",'content':"题目 牛客网\n求1+2+3+\u0026hellip;+n，要求不能使用乘除法、for、while、if、else、switch、case等关键字及条件判断语句（A?B:C）。\n解题思路  利用递归代替循环  public int Sum_Solution(int n) { int ans = n; boolean t = ((ans != 0) \u0026amp;\u0026amp; ((ans += Sum_Solution(n - 1)) != 0)); return ans; } "});index.add({'id':154,'href':'/interview/docs/java/annotation/','title':"注解",'content':"注解 注解(Annotation)是 Java1.5 中引入的一个重大修改之一，为我们在代码中添加信息提供了一种形式化的方法，使我们可以在稍后某个时刻非常方便的使用这些数据。注解在一定程度上是把元数据与源代码结合在一起，而不是保存在外部文档中。注解的含义可以理解为 java 中的元数据。元数据是描述数据的数据。\n注解是一个继承自java.lang.annotation.Annotation的接口\n可见性 根据注解在程序不同时期的可见性，可以把注解区分为：\n source：注解会在编译期间被丢弃，不会编译到 class 文件 class：注解会被编译到 class 文件中，但是在运行时不能获取 runtime：注解会被编译到 class 文件中，并且能够在运行时通过反射获取  继承     有@Inherited 没有@Inherited     子类的类上能否继承到父类的类上的注解？ 否 能   子类实现了父类上的抽象方法 否 否   子类继承了父类上的方法 能 能   子类覆盖了父类上的方法 否 否    @Inherited 只是可控制对类名上注解是否可以被继承。不能控制方法上的注解是否可以被继承。\n注解的实现机制  注解是继承自：java.lang.annotation.Annotation 的接口  ... Compiled from \u0026quot;TestAnnotation.java\u0026quot; public interface TestAnnotation extends java.lang.annotation.Annotation ... 注解内部的属性是在编译期间确定的  ... SourceFile: \u0026quot;SimpleTest.java\u0026quot; RuntimeVisibleAnnotations: 0: #43(#44=s#45) ... 注解在运行时会生成 Proxy 代理类，并使用 AnnotationInvocationHandler.memberValues 来进行数据读取  ... default: //从 Map 中获取数据 Object var6 = this.memberValues.get(var4); if (var6 == null) { throw new IncompleteAnnotationException(this.type, var4); } else if (var6 instanceof ExceptionProxy) { throw ((ExceptionProxy)var6).generateException(); } else { if (var6.getClass().isArray() \u0026amp;\u0026amp; Array.getLength(var6) != 0) { var6 = this.cloneArray(var6); } return var6; } } ... 参考链接  java注解是怎么实现的？  "});index.add({'id':155,'href':'/interview/docs/offer/MaxInWindows/','title':"滑动窗口的最大值",'content':"题目 牛客网\n给定一个数组和滑动窗口的大小，找出所有滑动窗口里数值的最大值。例如，如果输入数组{2,3,4,2,6,2,5,1}及滑动窗口的大小3，那么一共存在6个滑动窗口，他们的最大值分别为{4,4,6,6,6,5}； 针对数组{2,3,4,2,6,2,5,1}的滑动窗口有以下6个： {[2,3,4],2,6,2,5,1}， {2,[3,4,2],6,2,5,1}， {2,3,[4,2,6],2,5,1}， {2,3,4,[2,6,2],5,1}， {2,3,4,2,[6,2,5],1}， {2,3,4,2,6,[2,5,1]}。\n解题思路  使用一个队列来保存最大值和次大的值  public ArrayList\u0026lt;Integer\u0026gt; maxInWindows(int[] num, int size) { ArrayList\u0026lt;Integer\u0026gt; res = new ArrayList\u0026lt;\u0026gt;(); if (size == 0) return res; LinkedList\u0026lt;Integer\u0026gt; queue = new LinkedList\u0026lt;\u0026gt;(); for (int i = 0; i \u0026lt; num.length; i++) { while (queue.peekFirst() != null \u0026amp;\u0026amp; i - queue.peekFirst() \u0026gt;= size) { queue.removeFirst(); } while (queue.peekLast() != null \u0026amp;\u0026amp; i - queue.peekLast() \u0026gt;= size) { queue.removeLast(); } if (queue.isEmpty()) { queue.addFirst(i); } else { if (num[i] \u0026gt; num[queue.peekFirst()]) { queue.clear(); queue.addFirst(i); } else { while (num[i] \u0026gt; num[queue.peekLast()]) { queue.removeLast(); } queue.addLast(i); } } if (i \u0026gt;= size - 1) res.add(num[queue.peekFirst()]); } return res; } "});index.add({'id':156,'href':'/interview/docs/leetcode/detectCycle/','title':"环形链表 II",'content':"环形链表 II 题目 给定一个链表，返回链表开始入环的第一个节点。 如果链表无环，则返回 null。\n为了表示给定链表中的环，我们使用整数 pos 来表示链表尾连接到链表中的位置（索引从 0 开始）。 如果 pos 是 -1，则在该链表中没有环。\n解题思路  首先通过快慢指针确定链表是否有环 再使用一个指针从头节点与快慢指针相遇节点同步长前进，最终找到环的入口  public ListNode detectCycle(ListNode head) { ListNode fast = head, slow = head; ListNode meetNode = null; while (fast != null \u0026amp;\u0026amp; fast.next != null) { fast = fast.next.next; slow = slow.next; if (fast == slow) { meetNode = fast; break; } } if (meetNode == null) { return meetNode; } while (head != meetNode) { head = head.next; if (head == meetNode) { break; } meetNode = meetNode.next; } return meetNode; } "});index.add({'id':157,'href':'/interview/docs/offer/two-stack-fifo/','title':"用两个栈实现一个队列",'content':"题目 牛客网\n用两个栈来实现一个队列，完成队列的 Push 和 Pop 操作。 队列中的元素为int类型。\n解题思路  用 stack1 作为 push 队列，将元素 push 到 stack1 用 stack2 作为 pop 队列，当 stack2 为空时则将 stack1 的数据 push 到 stack2，否则直接 pop stack2  相当于将两个 stack 拼接：-\u0026gt; stack1 \u0026lt;::\u0026gt; stack2 -\u0026gt;\nStack\u0026lt;Integer\u0026gt; pushStack = new Stack\u0026lt;\u0026gt;(); Stack\u0026lt;Integer\u0026gt; popStack = new Stack\u0026lt;\u0026gt;(); public void push(int node) { pushStack.push(node); } public int pop() { if (popStack.isEmpty()) { while (!pushStack.isEmpty()) { popStack.push(pushStack.pop()); } } if (popStack.isEmpty()) return -1; else return popStack.pop(); } "});index.add({'id':158,'href':'/interview/docs/leetcode/mySqrt/','title':"的平方根",'content':"头条重点\n题目 实现 int sqrt(int x) 函数。\n计算并返回 x 的平方根，其中 x 是非负整数。\n由于返回类型是整数，结果只保留整数的部分，小数部分将被舍去。\n示例 1: 输入: 4 输出: 2 示例 2: 输入: 8 输出: 2 说明: 8 的平方根是 2.82842...,由于返回类型是整数，小数部分将被舍去。 解题思路  牛顿迭代法：$$a_{i}=(x/a_{i-1}+a_{i-1})/2$$  public int mySqrt(int x) { double a = 1, diff = 0; do { a = (x / a + a) / 2.0; diff = Math.abs(a * a - x); } while (diff \u0026gt; 0.1); return (int) a; } "});index.add({'id':159,'href':'/interview/docs/leetcode/getIntersectionNode/','title':"相交链表",'content':"题目 编写一个程序，找到两个单链表相交的起始节点。\n解题思路  首先将两个链表中长的一个向前遍历，直到两个链表长度一致 两个链表同时向前遍历，便可找到交点  public ListNode getIntersectionNode(ListNode headA, ListNode headB) { if (headA == null || headB == null) { return null; } if (headA == headB) { return headA; } int lenA = 1; int lenB = 1; ListNode temp = headA; while (temp.next != null) { temp = temp.next; lenA++; } ListNode tailA = temp; temp = headB; while (temp.next != null) { temp = temp.next; lenB++; } ListNode tailB = temp; if (tailB != tailA) { return null; } if (lenA \u0026gt; lenB) { for (int i = 0; i \u0026lt; lenA - lenB \u0026amp;\u0026amp; headA != null; i++) { headA = headA.next; } } else if (lenA \u0026lt; lenB) { for (int i = 0; i \u0026lt; lenB - lenA \u0026amp;\u0026amp; headB != null; i++) { headB = headB.next; } } while (!headA.equals(headB)) { headA = headA.next; headB = headB.next; } return headA; } "});index.add({'id':160,'href':'/interview/docs/offer/hasPath/','title':"矩阵中的路径",'content':"题目 请设计一个函数，用来判断在一个矩阵中是否存在一条包含某字符串所有字符的路径。路径可以从矩阵中的任意一个格子开始，每一步可以在矩阵中向左，向右，向上，向下移动一个格子。如果一条路径经过了矩阵中的某一个格子，则之后不能再次进入这个格子。 例如 a b c e s f c s a d e e 这样的 3 X 4 矩阵中包含一条字符串\u0026quot;bcced\u0026quot;的路径，但是矩阵中不包含\u0026quot;abcb\u0026quot;路径，因为字符串的第一个字符b占据了矩阵中的第一行第二个格子之后，路径不能再次进入该格子。\n解题思路  简单的回溯查找  static int[][] steps = {{0, 1}, {1, 0}, {0, -1}, {-1, 0}}; public boolean hasPath(char[] matrix, int rows, int cols, char[] str) { char[][] _matrix = new char[rows][cols]; int k = 0; for (int i = 0; i \u0026lt; _matrix.length; i++) { for (int j = 0; j \u0026lt; _matrix[i].length; j++) { _matrix[i][j] = matrix[k++]; } } int[][] flag = new int[rows][cols]; for (int i = 0; i \u0026lt; _matrix.length; i++) { for (int j = 0; j \u0026lt; _matrix[i].length; j++) { if (_matrix[i][j] == str[0]) { if (hasPath(_matrix, flag, i, j, str, 0)) { return true; } } } } return false; } private boolean hasPath(char[][] matrix, int[][] flag, int x, int y, char[] str, int index) { if (x \u0026lt; 0 || y \u0026lt; 0) return false; if (x \u0026gt;= matrix.length || y \u0026gt;= matrix[0].length) return false; if (flag[x][y] == 1) return false; boolean subRes = false; if (matrix[x][y] == str[index]) { if (index == str.length - 1) return true; flag[x][y] = 1; for (int[] step : steps) { subRes |= hasPath(matrix, flag, x + step[0], y + step[1], str, index + 1); } flag[x][y] = 0; } return subRes; } "});index.add({'id':161,'href':'/interview/docs/architecture/design/tinyURL/','title':"短链接",'content':"短链接 使用场景(Scenario) 微博和Twitter都有140字数的限制，如果分享一个长网址，很容易就超出限制，发布出去。短网址服务可以把一个长网址变成短网址，方便在社交网络上传播。\n需求(Needs) 很显然，要尽可能的短。长度设计为多少才合适呢？\n短网址的长度 当前互联网上的网页总数大概是 45亿(参考 短网址_短网址资讯mrw.so)，45亿 超过了 2^{32}=4294967296232=4294967296，但远远小于64位整数的上限值，那么用一个64位整数足够了。微博的短网址服务用的是长度为 7 的字符串，这个字符串可以看做是62进制的数，那么最大能表示{62}^7=3521614606208627=3521614606208个网址，远远大于 45亿。所以长度为7就足够了。一个64位整数如何转化为字符串呢？，假设我们只是用大小写字母加数字，那么可以看做是62进制数，log_{62{(2^{64}-1)=10.7log62(264−1)=10.7，即字符串最长11就足够了。实际生产中，还可以再短一点，比如新浪微博采用的长度就是7，因为 62^7=3521614606208627=3521614606208，这个量级远远超过互联网上的URL总数了，绝对够用了。现代的web服务器（例如Apache, Nginx）大部分都区分URL里的大小写了，所以用大小写字母来区分不同的URL是没问题的。因此，正确答案：长度不超过7的字符串，由大小写字母加数字共62个字母组成。\n一对一还是一对多映射？ 一个长网址，对应一个短网址，还是可以对应多个短网址？ 这也是个重大选择问题。一般而言，一个长网址，在不同的地点，不同的用户等情况下，生成的短网址应该不一样，这样，在后端数据库中，可以更好的进行数据分析。如果一个长网址与一个短网址一一对应，那么在数据库中，仅有一行数据，无法区分不同的来源，就无法做数据分析了。\n以这个7位长度的短网址作为唯一ID，这个ID下可以挂各种信息，比如生成该网址的用户名，所在网站，HTTP头部的 User Agent等信息，收集了这些信息，才有可能在后面做大数据分析，挖掘数据的价值。短网址服务商的一大盈利来源就是这些数据。\n正确答案：一对多\n如何计算短网址 现在我们设定了短网址是一个长度为7的字符串，如何计算得到这个短网址呢？\n最容易想到的办法是哈希，先hash得到一个64位整数，将它转化为62进制整，截取低7位即可。但是哈希算法会有冲突，如何处理冲突呢，又是一个麻烦。这个方法只是转移了矛盾，没有解决矛盾，抛弃。\n正确答案：分布式发号器(Distributed ID Generator)\n如何存储 如果存储短网址和长网址的对应关系？以短网址为 primary key, 长网址为value, 可以用传统的关系数据库存起来，例如MySQL,PostgreSQL，也可以用任意一个分布式 KV 数据库，例如Redis, LevelDB。\n301还是302重定向 这也是一个有意思的问题。这个问题主要是考察你对301和302的理解，以及浏览器缓存机制的理解。\n301是永久重定向，302是临时重定向。短地址一经生成就不会变化，所以用301是符合http语义的。但是如果用了301， Google，百度等搜索引擎，搜索的时候会直接展示真实地址，那我们就无法统计到短地址被点击的次数了，也无法收集用户的Cookie, User Agent 等信息，这些信息可以用来做很多有意思的大数据分析，也是短网址服务商的主要盈利来源。\n所以，正确答案是302重定向。\n可以抓包看看mrw.so的短网址是怎么做的，使用 Chrome 浏览器，访问这个URL http://mrw.so/4UD39p，是我事先发微博自动生成的短网址。来抓包看看返回的结果是啥，可见新浪微博用的就是302临时重定向。\n"});index.add({'id':162,'href':'/interview/docs/basic/os/disk/','title':"磁盘与文件",'content':"磁盘与文件 磁盘调度 磁盘访问延迟 = 队列时间 + 控制器时间 + 寻道时间 + 旋转时间 + 传输时间\n磁盘调度的目的是减小延迟，其中前两项可以忽略，寻道时间是主要矛盾。\n磁盘调度算法   FCFS：先进先出的调度策略，这个策略具有公平的优点，因为每个请求都会得到处理，并且是按照接收到的顺序进行处理。\n  SSTF(Shortest-seek-time First 最短寻道时间优先)：选择使磁头从当前位置开始移动最少的磁盘I/O请求，所以 SSTF 总是选择导致最小寻道时间的请求。总是选择最小寻找时间并不能保证平均寻找时间最小，但是能提供比 FCFS 算法更好的性能，会存在饥饿现象（会导致较远的I/O请求不能满足）。\n  SCAN：SSTF+中途不回折，每个请求都有处理机会。SCAN 要求磁头仅仅沿一个方向移动，并在途中满足所有未完成的请求，直到它到达这个方向上的最后一个磁道，或者在这个方向上没有其他请求为止。由于磁头移动规律与电梯运行相似，SCAN 也被称为电梯算法。\n   SCAN 算法对最近扫描过的区域不公平，因此，它在访问局部性方面不如 FCFS 算法和 SSTF 算法好。\n   C-SCAN：SCAN+直接移到另一端，两端请求都能很快处理。把扫描限定在一个方向，当访问到某个方向的最后一个磁道时，磁道返回磁盘相反方向磁道的末端，并再次开始扫描。其中“C”是Circular（环）的意思。\n  LOOK(C-LOOK)：釆用SCAN算法和C-SCAN算法时磁头总是严格地遵循从盘面的一端到另一端，显然，在实际使用时还可以改进，即磁头移动只需要到达最远端的一个请求即可返回，不需要到达磁盘端点。这种形式的SCAN算法和C-SCAN算法称为LOOK和C-LOOK调度。这是因为它们在朝一个给定方向移动前会查看是否有请求。\n  文件系统 分区表  MBR：支持最大卷为2 TB（Terabytes）并且每个磁盘最多有4个主分区（或3个主分区，1个扩展分区和无限制的逻辑驱动器） GPT：支持最大卷为18EB（Exabytes）并且每磁盘的分区数没有上限，只受到操作系统限制（由于分区表本身需要占用一定空间，最初规划硬盘分区时，留给分区表的空间决定了最多可以有多少个分区，IA-64版Windows限制最多有128个分区，这也是EFI标准规定的分区表的最小尺寸。另外，GPT分区磁盘有备份分区表来提高分区数据结构的完整性。  RAID 技术 磁盘阵列（Redundant Arrays of Independent Disks，RAID），独立冗余磁盘阵列之。原理是利用数组方式来作磁盘组，配合数据分散排列的设计，提升数据的安全性。\n常见文件系统  Windows: FAT, FAT16, FAT32, NTFS Linux: ext2/3/4, btrfs, ZFS Mac OS X: HFS+  Linux文件权限 Linux文件采用10个标志位来表示文件权限，如下所示：\n-rw-r--r-- 1 skyline staff 20B 1 27 10:34 1.txt drwxr-xr-x 5 skyline staff 170B 12 23 19:01 ABTableViewCell 第一个字符一般用来区分文件和目录，其中：\n d：表示是一个目录，事实上在ext2fs中，目录是一个特殊的文件。 －：表示这是一个普通的文件。 l: 表示这是一个符号链接文件，实际上它指向另一个文件。 b、c：分别表示区块设备和其他的外围设备，是特殊类型的文件。 s、p：这些文件关系到系统的数据结构和管道，通常很少见到。  第2～10个字符当中的每3个为一组，左边三个字符表示所有者权限，中间3个字符表示与所有者同一组的用户的权限，右边3个字符是其他用户的权限。\n这三个一组共9个字符，代表的意义如下：\n r(Read，读取)：对文件而言，具有读取文件内容的权限；对目录来说，具有浏览目录的权限 w(Write,写入)：对文件而言，具有新增、修改文件内容的权限；对目录来说，具有删除、移动目录内文件的权限。 x(eXecute，执行)：对文件而言，具有执行文件的权限；对目录来说该用户具有进入目录的权限。  权限的掩码可以使用十进制数字表示：\n 如果可读，权限是二进制的100，十进制是4； 如果可写，权限是二进制的010，十进制是2； 如果可运行，权限是二进制的001，十进制是1；  chmod命令 chmod命令非常重要，用于改变文件或目录的访问权限。用户用它控制文件或目录的访问权限。\n该命令有两种用法。一种是包含字母和操作符表达式的文字设定法；另一种是包含数字的数字设定法。\n 文字设定法  chmod ［who］ ［+ | - | =］ ［mode］ 文件名\n命令中各选项的含义为：\n操作对象who可是下述字母中的任一个或者它们的组合：\n u 表示“用户（user）”，即文件或目录的所有者。 g 表示“同组（group）用户”，即与文件属主有相同组ID的所有用户。 o 表示“其他（others）用户”。 a 表示“所有（all）用户”。它是系统默认值。  操作符号可以是：\n   添加某个权限。     取消某个权限。   = 赋予给定权限并取消其他所有权限（如果有的话）。  设置mode所表示的权限可用下述字母的任意组合：\n r 可读。 w 可写。 x 可执行。 X 只有目标文件对某些用户是可执行的或该目标文件是目录时才追加x 属性。 s 在文件执行时把进程的属主或组ID置为该文件的文件属主。方式“u＋s”设置文件的用户ID位，“g＋s”设置组ID位。 t 保存程序的文本到交换设备上。 u 与文件属主拥有一样的权限。 g 与和文件属主同组的用户拥有一样的权限。 o 与其他用户拥有一样的权限。  文件名：以空格分开的要改变权限的文件列表，支持通配符。\n在一个命令行中可给出多个权限方式，其间用逗号隔开。例如：chmod g+r，o+r example 使同组和其他用户对文件example 有读权限。\n数字设定法  直接使用数字表示的权限来更改：\n例： $ chmod 644 mm.txt chgrp命令 功能：改变文件或目录所属的组。\n语法：chgrp ［选项］ group filename\n例：$ chgrp - R book /opt/local /book 改变/opt/local /book/及其子目录下的所有文件的属组为book。\nchown命令 功能：更改某个文件或目录的属主和属组。这个命令也很常用。例如root用户把自己的一个文件拷贝给用户xu，为了让用户xu能够存取这个文件，root用户应该把这个文件的属主设为xu，否则，用户xu无法存取这个文件。\n语法：chown ［选项］ 用户或组 文件\n说明：chown将指定文件的拥有者改为指定的用户或组。用户可以是用户名或用户ID。组可以是组名或组ID。文件是以空格分开的要改变权限的文件列表，支持通配符。\n例：把文件shiyan.c的所有者改为wang。 chown wang shiyan.c "});index.add({'id':163,'href':'/interview/docs/offer/MaxGift/','title':"礼物的最大值",'content':"题目 在一个 m*n 的棋盘中的每一个格都放一个礼物，每个礼物都有一定的价值（价值大于0）.你可以从棋盘的左上角开始拿各种里的礼物，并每次向左或者向下移动一格，直到到达棋盘的右下角。给定一个棋盘及上面个的礼物，请计算你最多能拿走多少价值的礼物？\n比如说现在有一个如下的棋盘:\n在这个棋盘中，按照1，12，5，7，7，16，5的顺序可以拿到总价值最大的礼物。\n解题思路  动态规划，定义 $$f(x,y)$$ 表示x,y点上能获取的最大数 状态转移方程：$$f(x,y)=\\max(f(x-1,y),f(x,y-1))+g(x,y)$$ 可以考虑使用一维数组进行记录  public int maxGift(int[][] matrix) { for (int i = 0; i \u0026lt; matrix.length; i++) { for (int j = 0; j \u0026lt; matrix[i].length; j++) { int a = i \u0026gt; 0 ? matrix[i - 1][j] : 0; int b = j \u0026gt; 0 ? matrix[i][j - 1] : 0; matrix[i][j] += Math.max(a, b); } } System.out.println(Arrays.deepToString(matrix)); return matrix[matrix.length - 1][matrix[0].length - 1]; } "});index.add({'id':164,'href':'/interview/docs/offer/FirstNotRepeatingChar/','title':"第一个只出现一次的字符",'content':"题目 牛客网\n在一个字符串(0\u0026lt;=字符串长度\u0026lt;=10000，全部由字母组成)中找到第一个只出现一次的字符,并返回它的位置, 如果没有则返回 -1（需要区分大小写）.\n解题思路  通过 LinkedHashMap 记录数组顺序，然后计算字符出现的次数 遍历找到第一个只出现 1次 的字符  public int FirstNotRepeatingChar(String str) { LinkedHashMap\u0026lt;Character, Integer\u0026gt; data = new LinkedHashMap\u0026lt;\u0026gt;(); char[] chars = str.toCharArray(); for (char c : chars) { Integer count = data.getOrDefault(c, 0); data.put(c, count + 1); } Character res = null; for (Character c : data.keySet()) { if (data.get(c) == 1) { res = c; break; } } if (res == null) { return -1; } for (int i = 0; i \u0026lt; chars.length; i++) { if (chars[i] == res) { return i; } } return -1; } "});index.add({'id':165,'href':'/interview/docs/leetcode/salary/','title':"第二高的薪水",'content':"头条重点\n题目 编写一个 SQL 查询，获取 Employee 表中第二高的薪水（Salary） 。\n+----+--------+ | Id | Salary | +----+--------+ | 1 | 100 | | 2 | 200 | | 3 | 300 | +----+--------+ 例如上述 Employee 表，SQL查询应该返回 200 作为第二高的薪水。如果不存在第二高的薪水，那么查询应返回 null。\n+---------------------+ | SecondHighestSalary | +---------------------+ | 200 | +---------------------+ 解题思路  子查询  select IFNULL((select Distinct Salary from Employee order by Salary DESC limit 1,1),null) as SecondHighestSalary "});index.add({'id':166,'href':'/interview/docs/leetcode/simplifyPath/','title':"简化路径",'content':"头条重点\n题目 以 Unix 风格给出一个文件的绝对路径，你需要简化它。或者换句话说，将其转换为规范路径。\n在 Unix 风格的文件系统中，一个点（.）表示当前目录本身；此外，两个点 （..） 表示将目录切换到上一级（指向父目录）；两者都可以是复杂相对路径的组成部分。\n请注意，返回的规范路径必须始终以斜杠 / 开头，并且两个目录名之间必须只有一个斜杠 /。最后一个目录名（如果存在）不能以 / 结尾。此外，规范路径必须是表示绝对路径的最短字符串。\n示例 1： 输入：\u0026quot;/home/\u0026quot; 输出：\u0026quot;/home\u0026quot; 解释：注意，最后一个目录名后面没有斜杠。 示例 2： 输入：\u0026quot;/../\u0026quot; 输出：\u0026quot;/\u0026quot; 解释：从根目录向上一级是不可行的，因为根是你可以到达的最高级。 示例 3： 输入：\u0026quot;/home//foo/\u0026quot; 输出：\u0026quot;/home/foo\u0026quot; 解释：在规范路径中，多个连续斜杠需要用一个斜杠替换。 示例 4： 输入：\u0026quot;/a/./b/../../c/\u0026quot; 输出：\u0026quot;/c\u0026quot; 示例 5： 输入：\u0026quot;/a/../../b/../c//.//\u0026quot; 输出：\u0026quot;/c\u0026quot; 示例 6： 输入：\u0026quot;/a//b////c/d//././/..\u0026quot; 输出：\u0026quot;/a/b/c\u0026quot; 解题思路  利用栈的特性，将有效路径名压入 当遇到 .. 时 pop 栈 最后按顺序 pop 组成最终路径  public static String simplifyPath(String path) { ArrayDeque\u0026lt;String\u0026gt; stack = new ArrayDeque\u0026lt;\u0026gt;(); String[] split = path.split(\u0026quot;/\u0026quot;); for (String s : split) { if (s.isEmpty()) { continue; } switch (s) { case \u0026quot;..\u0026quot;: stack.pollLast(); break; case \u0026quot;.\u0026quot;: break; default: stack.offerLast(s); } } StringBuilder builder = new StringBuilder(\u0026quot;/\u0026quot;); for (String s : stack) { builder.append(s); builder.append(\u0026quot;/\u0026quot;); } if (builder.length() \u0026gt; 1) { builder.deleteCharAt(builder.length() - 1); } return builder.toString(); } "});index.add({'id':167,'href':'/interview/docs/basic/algo/','title':"算法",'content':"算法 "});index.add({'id':168,'href':'/interview/docs/architecture/base/','title':"系统架构基础",'content':"系统架构基础 分布式系统的最大难点，就是各个节点的状态如何同步。CAP 定理是这方面的基本定理，也是理解分布式系统的起点。\n1998年，加州大学的计算机科学家 Eric Brewer 提出，分布式系统有三个指标。\nConsistency Availability Partition tolerance 它们的第一个字母分别是 C、A、P。Eric Brewer 说，这三个指标不可能同时做到。这个结论就叫做 CAP 定理。\nCAP 它指出对于一个分布式计算系统来说，不可能同时满足以下三点：\n 一致性（Consistency） （等同于所有节点访问同一份最新的数据副本） 可用性（Availability）（每次请求都能获取到非错的响应——但是不保证获取的数据为最新数据） 分区容错性（Partition tolerance）（以实际效果而言，分区相当于对通信的时限要求。系统如果不能在时限内达成数据一致性，就意味着发生了分区的情况，必须就当前操作在C和A之间做出选择。）  数据一致性模型 一些分布式系统通过复制数据来提高系统的可靠性和容错性，并且将数据的不同的副本存放在不同的机器，由于维护数据副本的一致性代价高，因此许多系统采用弱一致性来提高性能，一些不同的一致性模型也相继被提出。\n 强一致性： 要求无论更新操作实在哪一个副本执行，之后所有的读操作都要能获得最新的数据。 弱一致性：用户读到某一操作对系统特定数据的更新需要一段时间，我们称这段时间为“不一致性窗口”。 最终一致性：是弱一致性的一种特例，保证用户最终能够读取到某操作对系统特定数据的更新。  一致性解决方案  分布式事务：两段提交 分布式锁 MQ 消息持久化 重试 幂等 Paxos 算法  服务可用性 可用性，意思是只要收到用户的请求，服务器就必须给出回应。\n高可用解决方案  负载均衡： 降级：当服务器压力剧增的情况下，根据当前业务情况及流量对一些服务和页面有策略的降级，以此释放服务器资源以保证核心任务的正常运行。 熔断：对于目标服务的请求和调用大量超时或失败，这时应该熔断该服务的所有调用，并且对于后续调用应直接返回，从而快速释放资源，确保在目标服务不可用的这段时间内，所有对它的调用都是立即返回，不会阻塞的。再等到目标服务好转后进行接口恢复。 流量控制： 异地多活：  熔断是减少由于下游服务故障对自己的影响；而降级则是在整个系统的角度上，考虑业务整体流量，保护核心业务稳定。\n分区容错性 大多数分布式系统都分布在多个子网络。每个子网络就叫做一个区（partition）。分区容错的意思是，区间通信可能失败。比如，一台服务器放在中国，另一台服务器放在美国，这就是两个区，它们之间可能无法通信。\n般来说，分区容错无法避免，因此可以认为 CAP 的 P 总是成立。CAP 定理告诉我们，剩下的 C 和 A 无法同时做到。\n"});index.add({'id':169,'href':'/interview/docs/basic/database/index/','title':"索引",'content':"索引 基本概念 在数据库中，索引的含义与日常意义上的“索引”一词并无多大区别（想想小时候查字典），它是用于提高数据库表数据访问速度的数据库对象。\n 索引可以避免全表扫描。多数查询可以仅扫描少量索引页及数据页，而不是遍历所有数据页。 对于非聚集索引，有些查询甚至可以不访问数据页。 聚集索引可以避免数据插入操作集中于表的最后一个数据页。 一些情况下，索引还可用于避免排序操作。  索引的存储 一条索引记录中包含的基本信息包括：键值（即你定义索引时指定的所有字段的值）+逻辑指针（指向数据页或者另一索引页）。\n当你为一张空表创建索引时，数据库系统将为你分配一个索引页，该索引页在你插入数据前一直是空的。此页此时既是根结点，也是叶结点。每当你往表中插入一行数据，数据库系统即向此根结点中插入一行索引记录。当根结点满时，数据库系统大抵按以下步骤进行分裂：\n 创建两个儿子结点 将原根结点中的数据近似地拆成两半，分别写入新的两个儿子结点 根结点中加上指向两个儿子结点的指针  通常状况下，由于索引记录仅包含索引字段值（以及4-9字节的指针），索引实体比真实的数据行要小许多，索引页相较数据页来说要密集许多。一个索引页可以存储数量更多的索引记录，这意味着在索引中查找时在I/O上占很大的优势，理解这一点有助于从本质上了解使用索引的优势。\n索引的分类  汉语字典的正文本身就是一个聚集索引。比如，我们要查“安”字，就会很自然地翻开字典的前几页，因为“安”的拼音是“an”，而按照拼音排序汉字的字典是以英文字母“a”开头并以“z”结尾的，那么“安”字就自然地排在字典的前部。如果您翻完了所有以“a”开头的部分仍然找不到这个字，那么就说明您的字典中没有这个字；同样的，如果查“张”字，那您也会将您的字典翻到最后部分，因为“张”的拼音是“zhang”。也就是说，字典的正文部分本身就是一个目录，您不需要再去查其他目录来找到您需要找的内容。正文内容本身就是一种按照一定规则排列的目录称为“聚集索引”。\n  如果您认识某个字，您可以快速地从自动中查到这个字。但您也可能会遇到您不认识的字，不知道它的发音，这时候，您就不能按照刚才的方法找到您要查的字，而需要去根据“偏旁部首”查到您要找的字，然后根据这个字后的页码直接翻到某页来找到您要找的字。但您结合“部首目录”和“检字表”而查到的字的排序并不是真正的正文的排序方法，比如您查“张”字，我们可以看到在查部首之后的检字表中“张”的页码是672页，检字表中“张”的上面是“驰”字，但页码却是63页，“张”的下面是“弩”字，页面是390页。很显然，这些字并不是真正的分别位于“张”字的上下方，现在您看到的连续的“驰、张、弩”三字实际上就是他们在非聚集索引中的排序，是字典正文中的字在非聚集索引中的映射。我们可以通过这种方式来找到您所需要的字，但它需要两个过程，先找到目录中的结果，然后再翻到您所需要的页码。\n 聚集索引 表数据按照索引的顺序来存储的。对于聚集索引，叶子结点即存储了真实的数据行，不再有另外单独的数据页。在聚集索引中，叶结点也即数据结点，所有数据行的存储顺序与索引的存储顺序一致。\n在一张表上只能创建一个聚集索引，因为真实数据的物理顺序只可能是一种。如果一张表没有聚集索引，那么它被称为“堆集”（Heap）。这样的表中的数据行没有特定的顺序，所有的新行将被添加的表的末尾位置。\n非聚集索引 表数据存储顺序与索引顺序无关。对于非聚集索引，叶结点包含索引字段值及指向数据页数据行的逻辑指针，该层紧邻数据页，其行数量与数据表行数据量一致。\n非聚集索引与聚集索引相比：\n 叶子结点并非数据结点 叶子结点为每一真正的数据行存储一个“键-指针”对 叶子结点中还存储了一个指针偏移量，根据页指针及指针偏移量可以定位到具体的数据行。 类似的，在除叶结点外的其它索引结点，存储的也是类似的内容，只不过它是指向下一级的索引页的。  索引失效 索引并不是时时都会生效的，比如以下几种情况，将导致索引失效：\n  如果条件中有or，即使其中有条件带索引也不会使用。 \u0026gt;要想使用or，又想让索引生效，只能将or条件中的每个列都加上索引\n  对于多列索引，不是使用的第一部分，则不会使用索引。\n  like查询是以%开头。\n  如果列类型是字符串，那一定要在条件中将数据使用引号引用起来，否则不使用索引。\n  如果 mysql 估计使用全表扫描要比使用索引快，则不使用索引。例如，使用\u0026lt;\u0026gt;、not in 、not exist，对于这三种情况大多数情况下认为结果集很大，MySQL就有可能不使用索引。\n  索引设计的原则   表的某个字段值得离散度越高，该字段越适合选作索引的关键字。主键字段以及唯一性约束字段适合选作索引的关键字，原因就是这些字段的值非常离散。\n  占用存储空间少的字段更适合选作索引的关键字。例如，与字符串相比，整数字段占用的存储空间较少，因此，较为适合选作索引关键字。\n  存储空间固定的字段更适合选作索引的关键字。与 text 类型的字段相比， char 类型的字段较为适合选作索引关键字。\n  Where 子句中经常使用的字段应该创建索引，分组字段或者排序字段应该创建索引，两个表的连接字段应该创建索引。\n  更新频繁的字段不适合创建索引，不会出现在 where 子句中的字段不应该创建索引。\n  最左前缀原则。\n  尽量使用前缀索引。\n  总结 聚集索引是一种稀疏索引，数据页上一级的索引页存储的是页指针，而不是行指针。而对于非聚集索引，则是密集索引，在数据页的上一级索引页它为每一个数据行存储一条索引记录。\n与非聚集索引相比，聚集索引有着更快检索速度、更快的字段排序。\n在MySQL中InnoDB按照主键进行聚集，如果没有定义主键，InnoDB会试着使用唯一的非空索引来代替。如果没有这种索引，InnoDB就会定义隐藏的主键然后在上面进行聚集，但是主键和聚集索引是不等价的。在InnoDB中Normal索引即非聚集索引。\n参考链接  MySQL 索引设计概要 MySQL 索引性能分析概要  "});index.add({'id':170,'href':'/interview/docs/java/concurrent/interrupt/','title':"线程中断",'content':"线程中断 中断不是类似 linux 里面的命令 kill -9 pid，不是说我们中断某个线程，这个线程就停止运行了。中断代表线程状态，每个线程都关联了一个中断状态，是一个 true 或 false 的 boolean 值，初始值为 false。\n关于中断状态，我们需要重点关注 Thread 类中的以下几个方法：\n// Thread 类中的实例方法，持有线程实例引用即可检测线程中断状态 public boolean isInterrupted() {} // Thread 中的静态方法，检测调用这个方法的线程是否已经中断 // 注意：这个方法返回中断状态的同时，会将此线程的中断状态重置为 false // 所以，如果我们连续调用两次这个方法的话，第二次的返回值肯定就是 false 了 public static boolean interrupted() {} // Thread 类中的实例方法，用于设置一个线程的中断状态为 true public void interrupt() {} 我们说 中断一个线程，其实就是设置了线程的 interrupted status 为 true，至于说被中断的线程怎么处理这个状态，那是那个线程自己的事。如以下代码：\nwhile (!Thread.interrupted()) { doWork(); System.out.println(\u0026quot;我做完一件事了，准备做下一件，如果没有其他线程中断我的话\u0026quot;); }  这种代码就是会响应中断的，它会在干活的时候先判断下中断状态，不过，除了 JDK 源码外，其他用中断的场景还是比较少的，毕竟 JDK 源码非常讲究。\n 当然，中断除了是线程状态外，还有其他含义，否则也不需要专门搞一个这个概念出来了。如果线程处于以下三种情况，那么当线程被中断的时候，能自动感知到：\n  来自 Object 类的 wait()、wait(long)、wait(long, int)，来自 Thread 类的 join()、join(long)、join(long, int)、sleep(long)、sleep(long, int)\n 这几个方法的相同之处是，方法上都有: throws InterruptedException 如果线程阻塞在这些方法上（我们知道，这些方法会让当前线程阻塞），这个时候如果其他线程对这个线程进行了中断，那么这个线程会从这些方法中立即返回，抛出 InterruptedException 异常，同时重置中断状态为 false。\n   实现了 InterruptibleChannel 接口的类中的一些 I/O 阻塞操作，如 DatagramChannel 中的 connect 方法和 receive 方法等\n 如果线程阻塞在这里，中断线程会导致这些方法抛出 ClosedByInterruptException 并重置中断状态。\n   Selector 中的 select 方法\n 一旦中断，方法立即返回\n   对于以上 3 种情况是最特殊的，因为他们能自动感知到中断（这里说自动，当然也是基于底层实现），并且在做出相应的操作后都会重置中断状态为 false。\n那是不是只有以上 3 种方法能自动感知到中断呢？不是的，如果线程阻塞在 LockSupport.park(Object obj) 方法，也叫挂起，这个时候的中断也会导致线程唤醒，但是唤醒后不会重置中断状态，所以唤醒后去检测中断状态将是 true。\nInterruptedException 概述 它是一个特殊的异常，不是说 JVM 对其有特殊的处理，而是它的使用场景比较特殊。通常，我们可以看到，像 Object 中的 wait() 方法，ReentrantLock 中的 lockInterruptibly() 方法，Thread 中的 sleep() 方法等等，这些方法都带有 throws InterruptedException，我们通常称这些方法为阻塞方法（blocking method）。\n阻塞方法一个很明显的特征是，它们需要花费比较长的时间（不是绝对的，只是说明时间不可控），还有它们的方法结束返回往往依赖于外部条件，如 wait 方法依赖于其他线程的 notify，lock 方法依赖于其他线程的 unlock 等等。\n当我们看到方法上带有 throws InterruptedException 时，我们就要知道，这个方法应该是阻塞方法，我们如果希望它能早点返回的话，我们往往可以通过中断来实现。\n除了几个特殊类（如 Object，Thread等）外，感知中断并提前返回是通过轮询中断状态来实现的。我们自己需要写可中断的方法的时候，就是通过在合适的时机（通常在循环的开始处）去判断线程的中断状态，然后做相应的操作（通常是方法直接返回或者抛出异常）。当然，我们也要看到，如果我们一次循环花的时间比较长的话，那么就需要比较长的时间才能感知到线程中断了。\n处理中断 一旦中断发生，我们接收到了这个信息，然后怎么去处理中断呢？本小节将简单分析这个问题。\n我们经常会这么写代码：\ntry { Thread.sleep(10000); } catch (InterruptedException e) { // ignore } // go on 当 sleep 结束继续往下执行的时候，我们往往都不知道这块代码是真的 sleep 了 10 秒，还是只休眠了 1 秒就被中断了。这个代码的问题在于，我们将这个异常信息吞掉了。（对于 sleep 方法，我相信大部分情况下，我们都不在意是否是中断了，这里是举例）\nAQS 的做法很值得我们借鉴，我们知道 ReentrantLock 有两种 lock 方法：\npublic void lock() { sync.lock(); } public void lockInterruptibly() throws InterruptedException { sync.acquireInterruptibly(1); } 前面我们提到过，lock() 方法不响应中断。如果 thread1 调用了 lock() 方法，过了很久还没抢到锁，这个时候 thread2 对其进行了中断，thread1 是不响应这个请求的，它会继续抢锁，当然它不会把“被中断”这个信息扔掉。我们可以看以下代码：\npublic final void acquire(int arg) { if (!tryAcquire(arg) \u0026amp;\u0026amp; acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) // 我们看到，这里也没做任何特殊处理，就是记录下来中断状态。  // 这样，如果外层方法需要去检测的时候，至少我们没有把这个信息丢了  selfInterrupt();// Thread.currentThread().interrupt(); } 而对于 lockInterruptibly() 方法，因为其方法上面有 throws InterruptedException ，这个信号告诉我们，如果我们要取消线程抢锁，直接中断这个线程即可，它会立即返回，抛出 InterruptedException 异常。\n在并发包中，有非常多的这种处理中断的例子，提供两个方法，分别为响应中断和不响应中断，对于不响应中断的方法，记录中断而不是丢失这个信息。如 Condition 中的两个方法就是这样的：\nvoid await() throws InterruptedException; void awaitUninterruptibly(); 实例分析 有以下代码：\nsynchronized (this) { while (client == null) { try { this.wait(); } catch (InterruptedException e) { LOGGER.error(\u0026#34;InterruptedException:{}\u0026#34;, e); Thread.currentThread().interrupt(); } } } 上面的代码会造成什么问题？仔细分析可以发现，代码中如果抛出 InterruptedException，就会陷入死循环中，导致异常日志打爆。为什么会这样呢？首先我们来看下这两个方法：\n wait(): if any thread interrupted the current thread before or while the current thread was waiting for a notification. The interrupted status of the current thread is cleared when this exception is thrown. Thread.currentThread().interrupt():If none of the previous conditions hold then this thread's interrupt status will be set.  wait() 在当前线程有中断标志位时抛出中断异常；而 interrupt() 如果当前线程没有在wait()等阻塞操作，则标记中断。这样就陷入死循环，无限的打印 ERROR 日志。正确的处理 InterruptedException 是很重要的。\n"});index.add({'id':171,'href':'/interview/docs/leetcode/LRUCache/','title':"缓存机制",'content':"头条重点\n题目 运用你所掌握的数据结构，设计和实现一个 LRU (最近最少使用) 缓存机制。它应该支持以下操作： 获取数据 get 和 写入数据 put 。\n获取数据 get(key) - 如果密钥 (key) 存在于缓存中，则获取密钥的值（总是正数），否则返回 -1。 写入数据 put(key, value) - 如果密钥不存在，则写入其数据值。当缓存容量达到上限时，它应该在写入新数据之前删除最近最少使用的数据值，从而为新的数据值留出空间。\n进阶:\n你是否可以在 O(1) 时间复杂度内完成这两种操作？\n示例: LRUCache cache = new LRUCache( 2 /* 缓存容量 */ ); cache.put(1, 1); cache.put(2, 2); cache.get(1); // 返回 1 cache.put(3, 3); // 该操作会使得密钥 2 作废 cache.get(2); // 返回 -1 (未找到) cache.put(4, 4); // 该操作会使得密钥 1 作废 cache.get(1); // 返回 -1 (未找到) cache.get(3); // 返回 3 cache.get(4); // 返回 4 解题思路 class LRUCache extends LinkedHashMap\u0026lt;Integer, Integer\u0026gt;{ private final int capacity; public LRUCache(int capacity) { super(capacity*, 0.75f, true); this.capacity = capacity; } public int get(int key) { Integer integer = super.get(key); return integer == null ? -1 : integer; } public void put(int key, int value) { super.put(key, value); } @Override protected boolean removeEldestEntry(Map.Entry\u0026lt;Integer, Integer\u0026gt; eldest) { return size() \u0026gt; this.capacity; } } "});index.add({'id':172,'href':'/interview/docs/leetcode/validUtf8/','title':"编码验证",'content':"头条重点\n题目 UTF-8 中的一个字符可能的长度为 1 到 4 字节，遵循以下的规则：\n 对于 1 字节的字符，字节的第一位设为0，后面7位为这个符号的unicode码。 对于 n 字节的字符 (n \u0026gt; 1)，第一个字节的前 n 位都设为1，第 n+1 位设为0，后面字节的前两位一律设为10。剩下的没有提及的二进制位，全部为这个符号的unicode码。  这是 UTF-8 编码的工作方式：\n Char. number range | UTF-8 octet sequence (hexadecimal) | (binary) --------------------+--------------------------------------------- 0000 0000-0000 007F | 0xxxxxxx 0000 0080-0000 07FF | 110xxxxx 10xxxxxx 0000 0800-0000 FFFF | 1110xxxx 10xxxxxx 10xxxxxx 0001 0000-0010 FFFF | 11110xxx 10xxxxxx 10xxxxxx 10xxxxxx 给定一个表示数据的整数数组，返回它是否为有效的 utf-8 编码。\n注意：输入是整数数组。只有每个整数的最低 8 个有效位用来存储数据。这意味着每个整数只表示 1 字节的数据。\n示例 1: data = [197, 130, 1], 表示 8 位的序列: 11000101 10000010 00000001. 返回 true 。 这是有效的 utf-8 编码，为一个2字节字符，跟着一个1字节字符。 解题思路 class Solution { public boolean validUtf8(int[] data) { int totalByteCount = 0; for (int item : data) { if (totalByteCount == 0) { totalByteCount = totalByteCount(item); if (totalByteCount == -1) { return false; } totalByteCount--; continue; } //10xxxxxx检查 if ((item \u0026amp; 0xC0) != 0x80) { return false; } totalByteCount--; } return totalByteCount == 0; } private int totalByteCount(int i) { if ((i \u0026amp; 0x80) == 0) { return 1; } if ((i \u0026amp; 0xE0) == 0xC0) { return 2; } if ((i \u0026amp; 0xF0) == 0xE0) { return 3; } if ((i \u0026amp; 0xF8) == 0xF0) { return 4; } return -1; } } "});index.add({'id':173,'href':'/interview/docs/basic/net/','title':"网络分层",'content':"网络分层 OSI    层 功能     应用层 网络进程到应用程序。针对特定应用规定各层协议、时序、表示等，进行封装 。在端系统中用软件来实现，如HTTP等   表示层 数据表示形式，加密和解密，把机器相关的数据转换成独立于机器的数据。规定数据的格式化表示 ，数据格式的转换等   会话层 主机间通讯，管理应用程序之间的会话。规定通信时序 ；数据交换的定界、同步，创建检查点等   传输层 在网络的各个节点之间可靠地分发数据包。所有传输遗留问题；复用；流量；可靠   网络层 在网络的各个节点之间进行地址分配、路由和（不一定可靠的）分发报文。路由（ IP寻址）；拥塞控制。   数据链路层 一个可靠的点对点数据直链。检错与纠错（CRC码）；多路访问；寻址   物理层 一个（不一定可靠的）点对点数据直链。定义机械特性；电气特性；功能特性；规程特性    "});index.add({'id':174,'href':'/interview/docs/basic/net/protocol/','title':"网络协议",'content':"底层网络协议 ARP（地址解析协议） 基本功能为透过目标设备的IP地址，查询目标设备的MAC地址，以保证通信的顺利进行。在每台安装有TCP/IP协议的电脑或路由器里都有一个ARP缓存表，表里的IP地址与MAC地址是一对应的。\n当发送数据时，主机A会在自己的ARP缓存表中寻找是否有目标IP地址。如果找到就知道目标MAC地址为（00-BB-00-62-C2-02），直接把目标MAC地址写入帧里面发送就可；如果在ARP缓存表中没有找到相对应的IP地址，主机A就会在网络上发送一个 广播（ARP request），目标MAC地址是“FF.FF.FF.FF.FF.FF”，这表示向同一网段内的所有主机发出这样的询问：“192.168.38.11的MAC地址是什么？”网络上其他主机并不响应ARP询问，只有主机B接收到这个帧时，才向主机A做出这样的回应（ARP response）：“192.168.38.11的MAC地址是（00-BB-00-62-C2-02）”。这样，主机A就知道主机B的MAC地址，它就可以向主机B发送信息。同时它还更新自己的ARP缓存表，下次再向主机B发送信息时，直接从ARP缓存表里查找就可。ARP缓存表采用老化机制，在一段时间内如果表中的某一行没有使用，就会被删除，这样可以大大减少ARP缓存表的长度，加快查询速度。\n 当发送主机和目的主机不在同一个局域网中时，即便知道目的主机的MAC地址，两者也不能直接通信，必须经过路由转发才可以。所以此时，发送主机通过ARP协议获得的将不是目的主机的真实MAC地址，而是一台可以通往局域网外的路由器的MAC地址。于是此后发送主机发往目的主机的所有帧，都将发往该路由器，通过它向外发送。这种情况称为ARP代理（ARP Proxy）。\n ICMP（互联网控制消息协议） 它 用于TCP/IP网络中发送控制消息，提供可能发生在通信环境中的各种问题反馈，通过这些信息，令管理者可以对所发生的问题作出诊断，然后采取适当的措施解决。它与传输协议最大的不同：它一般不用于在两点间传输数据，而常常 用于返回的错误信息或是分析路由。\nICMP控制的内容包括但不仅限于：echo响应（ping）、目标网络不可达、目标端口不可达、禁止访问的网络、拥塞控制、重定向、TTL超时\u0026hellip;\n路由选择协议 路由选择协议分为：静态的和动态的。Internet中使用的是动态路由选择协议，在Internet的概念中，将整个互联网划分为许多个小的自治系统（AS）。AS的最主要的特征：一个AS对其他AS表现出的是一个单一 和一致的路由选择策略。\n由于AS的存在，路由选择协议又分为两种：\n 内部网关协议（IGP）：即在一个AS内部使用的路由选择协议，而这与互联网中其他AS选用什么路由协议无关。比如：OSPF 外部网关协议（EGP）：若源主机和目的主机不再同一个AS中，就需要使用一种协议将路由选择信息传递到另一个AS中，这就是EGP。比如：BGP。  OSPF（开放式最短路径优先） OSPF属于内部网关协议（IGP）的一种，使用Dijkstra提出的最短路径算法。\nOSPF提出了“区域（Area）”的概念，一个网络可以由单一区域或者多个区域组成。其中，一个特别的区域被称为骨干区域（Backbone Area），该区域是整个OSPF网络的核心区域，并且所有其他的区域都与之直接连接。所有的内部路由都通过骨干区域传递到其他非骨干区域。所有的区域都必须直接连接到骨干区域，如果不能创建直接连接，那么可以通过虚拟链路（Virtual-link）和骨干区域创建虚拟连接。\n划分区域的优点：\n  将洪泛法的范围限制在一个区域中。\n  减少每个区域内部路由信息交换的通信量。\n  OSPF使用的是分布式链路状态协议，使用 洪泛法向该路由器所有的相邻路由器发送信息。最终整个区域的所有路由器都得到一个这个信息的副本。这个副本就是 链路状态数据库（LSDB）用来保存当前网络拓扑结构，路由器上属于同一区域的链路状态数据库是相同的（属于多个区域的路由器会为每个区域维护一份链路状态数据库）。\n  OSPF使用 **“代价（Cost）”**作为路由度量。\n  只有当链路发生变化时才会更新信息。\n   如果同一个目的网络有多条路径，OSPF协议可以进行 负载均衡。\n BGP（边界网关协议） 由于BGP是工作在AS之间的协议，并且各个AS的情况复杂，所以 BGP只是力求找到一个可以到达目的网络且比较好的路由，而并不是寻找一条最佳路由。每一个AS都应该有一个**“BGP发言人“**，一般来说，两个BGP发言人是通过一个共享网络连接在一起的，BGP发言人往往是**BGP边界路由**，但也可以不是。\n一个BGP发言人与其他AS的BGP发言人要交换路由信息，首先要建立TCP连接，然后在此连接上交换BGP报文以建立BGP会话。当BGP发言人交换了路由信息后，就构造自治系统连通图，最后通过该图来进行路由选择。\nDHCP（动态主机设置协议） DHCP是一个局域网的网络协议，使用UDP协议工作，主要有两个用途：\n 用于内部网络或网络服务供应商自动分配IP地址给用户 用于内部网络管理员作为对所有电脑作中央管理的手段  动态主机设置协议（DHCP）是一种使网络管理员能够集中管理和自动分配IP网络地址的通信协议。在IP网络中，每个连接Internet的设备都需要分配唯一的IP地址。DHCP使网络管理员能从中心结点监控和分配IP地址。当某台计算机移到网络中的其它位置时，能自动收到新的IP地址。\nDHCP使用了 租约 的概念，或称为计算机IP地址的有效期。租用时间是不定的，主要取决于用户在某地连接Internet需要多久，这对于教育行业和其它用户频繁改变的环境是很实用的。通过较短的租期，DHCP能够在一个计算机比可用IP地址多的环境中动态地重新配置网络。DHCP支持为计算机分配静态地址，如需要永久性IP地址的Web服务器。\nNAT（地址转换协议） NAT是一种 在IP封包通过路由器或防火墙时重写来源IP地址或目的IP地址的技术。这种技术被普遍使用在有多台主机但只通过一个公有IP地址访问因特网的私有网络中。\n"});index.add({'id':175,'href':'/interview/docs/offer/ReverseSentence/','title':"翻转单词顺序列",'content':"题目 牛客网\n牛客最近来了一个新员工 Fish ，每天早晨总是会拿着一本英文杂志，写些句子在本子上。同事Cat对Fish写的内容颇感兴趣，有一天他向Fish借来翻看，但却读不懂它的意思。例如，“student. a am I”。后来才意识到，这家伙原来把句子单词的顺序翻转了，正确的句子应该是“I am a student.”。Cat对一一的翻转这些单词顺序可不在行，你能帮助他么？\n解题思路 public String ReverseSentence(String str) { if(str == null || str.trim().equals(\u0026quot;\u0026quot;)) return str; String[] split = str.split(\u0026quot; \u0026quot;); StringBuilder builder = new StringBuilder(); for (int i = split.length - 1; i \u0026gt;= 0; i--) { builder.append(split[i]); if (i != 0) builder.append(\u0026quot; \u0026quot;); } return builder.toString(); } "});index.add({'id':176,'href':'/interview/docs/leetcode/reverseWords/','title':"翻转字符串里的单词",'content':"题目 给定一个字符串，逐个翻转字符串中的每个单词。\n示例 1： 输入: \u0026quot;the sky is blue\u0026quot; 输出: \u0026quot;blue is sky the\u0026quot; 说明：\n 无空格字符构成一个单词。 输入字符串可以在前面或者后面包含多余的空格，但是反转后的字符不能包括。 如果两个单词间有多余的空格，将反转后单词间的空格减少到只含一个。  解题思路  按空格拆分字符串为字符串数组 t 逆序遍历字符串数组 t，并组成新的字符串  public String reverseWords(String s) { String trimed = s.trim(); String[] split = trimed.split(\u0026quot; \u0026quot;); StringBuilder builder = new StringBuilder(); for (int i = split.length - 1; i \u0026gt;= 0; i--) { String t = split[i]; if (t.trim().isEmpty()) { continue; } builder.append(t).append(\u0026quot; \u0026quot;); } return builder.toString().trim(); } "});index.add({'id':177,'href':'/interview/docs/offer/MaxProfit/','title':"股票的最大利润",'content':"题目 一只股票在某些时间节点的价格为{9,11,8,5,7,12,16,14}。如果我们能在价格为 5 的时候买入并在价格为 16 时卖出，则能获得最大的利润为 11.\n解题思路  要先买入才能卖出，先找最低价格点 再找最低价格之后的最高价格，用 maxProfit 表示最大利润  public int maxProfit(int[] nums) { if (nums == null || nums.length == 0) return 0; int min = Integer.MAX_VALUE; int maxProfit = 0; for (int i = 0; i \u0026lt; nums.length; i++) { min = Math.min(min, nums[i]); maxProfit = Math.max(maxProfit, nums[i] - min); } return maxProfit; } "});index.add({'id':178,'href':'/interview/docs/offer/IsNumeric/','title':"表示数值的字符串",'content':"题目 请实现一个函数用来判断字符串是否表示数值（包括整数和小数）。例如，字符串\u0026quot;+100\u0026rdquo;,\u0026ldquo;5e2\u0026rdquo;,\u0026quot;-123\u0026rdquo;,\u0026ldquo;3.1416\u0026quot;和\u0026rdquo;-1E-16\u0026quot;都表示数值。 但是\u0026quot;12e\u0026rdquo;,\u0026ldquo;1a3.14\u0026rdquo;,\u0026ldquo;1.2.3\u0026rdquo;,\u0026ldquo;+-5\u0026quot;和\u0026quot;12e+4.3\u0026quot;都不是。\n解题思路  数字符合 A[.[B]][e|EC] 和 .B[e|EC] 的表达式，其中 A 表示整数部分，B 表示小数部分，C 表示指数部分 A 可以有正负，但是 B 没有 e|E 之前、之后都必须有数字  public boolean isNumeric(char[] str) { if (str == null || str.length == 0) return false; int index = scanInteger(str, 0); boolean numeric = index != 0; //小数 if (index \u0026lt; str.length \u0026amp;\u0026amp; str[index] == '.') { index++; int pre = index; index = scanUnsignedInteger(str, index); //1. 小数可以没有整数部分 //2. 小数后可以没有数字 //3. 小数后可以有数字 numeric |= index != pre; } //指数 if (index \u0026lt; str.length \u0026amp;\u0026amp; (str[index] == 'e' || str[index] == 'E')) { index++; int pre = index; index = scanInteger(str, index); //1. e或者E之前必须有数字 //2. e或者E之后必须有数字 numeric \u0026amp;= index != pre; } return numeric \u0026amp;\u0026amp; index == str.length; } private int scanInteger(char[] str, int s) { if (s \u0026lt; str.length \u0026amp;\u0026amp; (str[s] == '+' || str[s] == '-')) s++; return scanUnsignedInteger(str, s); } private int scanUnsignedInteger(char[] str, int s) { while (s \u0026lt; str.length \u0026amp;\u0026amp; str[s] \u0026gt;= '0' \u0026amp;\u0026amp; str[s] \u0026lt;= '9') { s++; } return s; } "});index.add({'id':179,'href':'/interview/docs/basic/os/arch/','title':"计算机体系结构",'content':"计算机体系结构 冯·诺依曼体系结构   计算机处理的数据和指令一律用二进制数表示\n  顺序执行程序\n计算机运行过程中，把要执行的程序和处理的数据首先存入主存储器（内存），计算机执行程序时，将自动地并按顺序从主存储器中取出指令一条一条地执行，这一概念称作顺序执行程序。\n  计算机硬件由运算器、控制器、存储器、输入设备和输出设备五大部分组成。\n  数据的机内表示 二进制表示 机器数 由于计算机中符号和数字一样，都必须用二进制数串来表示，因此，正负号也必须用0,1来表示。\n原码 原码用第一位表示符号, 其余位表示值. 比如如果是8位二进制:\n[+1]原 = 0000 0001 [-1]原 = 1000 0001 第一位是符号位. 因为第一位是符号位, 所以8位二进制数的取值范围就是: [1111 1111 , 0111 1111] 即 [-127 , 127] 原码是人脑最容易理解和计算的表示方式\n反码  正数的反码是其本身 负数的反码是在其原码的基础上, 符号位不变，其余各个位取反.  [+1] = [00000001]原 = [00000001]反 [-1] = [10000001]原 = [11111110]反 可见如果一个反码表示的是负数，人脑无法直观的看出来它的数值， 通常要将其转换成原码再计算。\n补码  正数的补码就是其本身 负数的补码是在其原码的基础上, 符号位不变, 其余各位取反, 最后+1。 (即在反码的基础上+1)  [+1] = [00000001]原 = [00000001]反 = [00000001]补 [-1] = [10000001]原 = [11111110]反 = [11111111]补 1+（-1)= 00000001 + 11111111 = 00000000 = 0 对于负数, 补码表示方式也是人脑无法直观看出其数值的. 通常也需要转换成原码在计算其数值.\n 在计算机系统中，数值一律用补码来表示和存储。原因在于，使用补码，可以将符号位和数值域统一处理；同时，加法和减法也可以统一处理。此外，补码与原码相互转换，其运算过程是相同的，不需要额外的硬件电路。\n 定点数与浮点数 定点数是小数点固定的数。在计算机中没有专门表示小数点的位，小数点的位置是约定默认的。一般固定在机器数的最低位之后，或是固定在符号位之后。前者称为定点纯整数，后者称为定点纯小数。\n 定点数表示法简单直观，但是 数值表示的范围太小，运算时容易产生溢出。\n 浮点数是小数点的位置可以变动的数。为增大数值表示范围，防止溢出，采用浮点数表示法。浮点表示法类似于十进制中的科学计数法。\n在计算机上，通常使用2为基数的幂数来表式。一个浮点数a由两个数m和e来表示：a = m × b^e。在任意一个这样的系统中，我们选择一个基数b（记数系统的基）和精度p（即使用多少位来存储）。m （即尾数）是形如±d.ddd...ddd的p位数（每一位是一个介于0到b-1之间的整数，包括0和b-1）。如果m的第一位是非0整数，m称作正规化的。e是指数。\n| 数符± | 阶码e | 尾数m | 数符表示尾数的符号位，阶码表示幂次，尾数表示规格化后的小数值。\n 32位单精度：单精度二进制小数，使用32位存储。1 8 23 位长 64位双精度：双精度二进制小数，使用64位存储。1 11 52 位长  位、字节、字   位(Bit)：电子计算机中最小的数据单位。每一位的状态只能是0或1。\n  字节(Byte)：8个二进制位构成1个字节，它是存储空间的基本计量单位。1个字节可以储存1个英文字母或者半个汉字，换句话说，1个汉字占据2个字节的存储空间。\n  字(Word)：由若干个字节构成，字的位数叫做字长，不同档次的机器有不同的字长。例如一台8位机，它的1个字就等于1个字节，字长为8位。如果是一台16位机，那么，它的1个字就由2个字节构成，字长为16位。字是计算机进行数据处理和运算的单位。\n  字节序 字节顺序是指占内存多于一个字节类型的数据在内存中的存放顺序，通常有小端、大端两种字节顺序。\n  小端字节序：低字节数据存放在内存低地址处，高字节数据存放在内存高地址处。\n  大端字节序：高字节数据存放在低地址处，低字节数据存放在高地址处。\n  基于X86平台的PC机是小端字节序的，而有的嵌入式平台则是大端字节序的。所有网络协议也都是采用大端字节序的方式来传输数据的。所以有时我们也会把大端字节序方式称之为网络字节序。\n比如数字 0x12345678 在两种不同字节序CPU中的存储顺序如下所示： Big Endian 低地址 高地址 ----------------------------------------------------\u0026gt; +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | 12 | 34 | 56 | 78 | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ Little Endian 低地址 高地址 ----------------------------------------------------\u0026gt; +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | 78 | 56 | 34 | 12 | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ 从上面两图可以看出，采用Big Endian方式存储数据是符合我们人类的思维习惯的。 联合体union的存放顺序是所有成员都从低地址开始存放，利用该特性，就能判断CPU对内存采用Little-endian还是Big-endian模式读写。\n字节对齐 现代计算机中内存空间都是按照字节划分的，从理论上讲似乎对任何类型的变量的访问可以从任何地址开始，但实际情况是在访问特定类型变量的时候经常在特定的内存地址访问，这就需要各种类型数据按照一定的规则在空间上排列，而不是顺序的一个接一个的排放，这就是对齐。\n  为什么要进行字节对齐？\n 某些平台只能在特定的地址处访问特定类型的数据; 最根本的原因是效率问题，字节对齐能提高存取数据的速度。    比如有的平台每次都是从偶地址处读取数据,对于一个int型的变量,若从偶地址单元处存放,则只需一个读取周期即可读取该变量，但是若从奇地址单元处存放，则需要2个读取周期读取该变量。\n  字节对齐的原则\n  数据成员对齐规则：结构体或联合体的数据成员，第一个数据成员放在 offset 为0的地方，以后每个数据成员存储的起始位置要从该成员大小或者成员的子成员大小（只要该成员有子成员，比如说是数组，结构体等）的整数倍开始（比如int在32位机为4字节,则要从4的整数倍地址开始存储）。\n  结构体作为成员：如果一个结构里有某些结构体成员，则结构体成员要从其内部最大元素大小的整数倍地址开始存储。(struct a里存有struct b,b里有char,int ,double等元素,那b应该从8的整数倍开始存储。)\n  收尾工作：结构体的总大小，也就是sizeof的结果，必须是其内部最大成员的整数倍，不足的要补齐。\n    "});index.add({'id':180,'href':'/interview/docs/basic/os/device/','title':"设备管理",'content':"设备管理 外部设备分为两大类：\n 存储型设备：以存储大量信息和快速检索为目标，在系统中存储持久性信息。 I/O型设备：如显示器、打印机等。  I/O硬件原理 I/O系统 通常把I/O设备及其接口线路、控制部件、通道和管理软件称为I/O系统，把计算机的内存和设备介质之间的信息传送操作称为I/O操作。可按照不同方式对设备进行分类：按I/O操作特性分为输入型设备、输出型设备和存储型设备；按I/O信息交换单位分为字符设备和块设备。\n 输入、输出型设备通常是字符设备，存储型设备通常是块设备。\n 存储型设备又分为顺序存储设备和直接存取设备。前者严格依赖信息的物理位置进行读写和定位，如磁带。后者的特点是存取任何一个物理块所需要的时间几乎不依赖于此信息所处的位置，如磁盘。\nI/O控制方式 轮询方式 轮询方式又称程序直接控制方式，使用查询指令测试设备控制器的忙闲状态位，确定内存和设备是否能交换数据。轮询方式采用三条指令：查询指令，查询设备是否就绪；读写指令，当设备就绪时执行数据交换；转移指令，当设备未就绪时执行转移指令指向查询指令继续查询。可见，在这种方式下CPU和设备只能串行工作。\n中断方式 在这种方式下CPU和设备之间传输数据的过程如下：\n  进程发出启动I/O指令，CPU加载控制信息到设备控制器的寄存器，然后进程继续执行不涉及本次I/O数据的任务，或放弃CPU等待设备I/O操作完成。\n  设备控制器检查寄存器的内容，按照I/O指令的要求执行相应I/O操作，一旦传输完成，设备控制器发出I/O中断请求信号。\n  CPU收到并响应I/O中断后，转向设备的I/O中断处理程序执行。\n  中断处理程序执行数据读取操作，将I/O缓冲寄存器的内容写入内存，操作结束后退出中断处理程序，返回发生中断前的状态。\n  进程调度程序在适当的时候让得到数据的进程恢复执行。\n  在I/O中断方式中，如果设备控制器的数据缓冲区较小，当缓冲器装满后便会发生中断，那么在数据传输过程中发生中断次数会很多，这样就消耗了大量CPU时间。\nDMA方式 虽然中断方式提高了CPU利用率，但是在响应中断请求后必须停止现行程序，转入中断处理程序并参与数据传输操作。在DMA(Direct Memory Access)方式中，内存和设备之间有一条数据通路成块地传送数据，无须CPU干预，实际数据传输操作由DMA直接完成。为实现DMA，至少需要以下逻辑部件：\n  内存地址寄存器：存放内存中需要交换数据的地址，DMA传送之前由程序送入首地址；DMA传送过程中，每次交换数据都把地址寄存器的内容加1。\n  字计数器：记录传送数据的总字数，每次传送一个字就把字计数器减1。\n  数据缓冲寄存器或数据缓冲区：暂存每次传送的数据。\n  设备地址寄存器：存放I/O信息的地址，如磁盘的柱面号。\n  中断机制和控制逻辑：用于向CPU提出I/O中断请求及CPU发来的I/O命令，管理DMA的传送过程。\n  通道方式 通道又称I/O处理器，能完成内存和设备之间的信息传送，与CPU并行地执行操作。采用I/O通道设计后，I/O操作过程如下：CPU在执行主程序时遇到I/O请求，启动在指定通道上选址的设备，一旦启动成功，通道开始控制设备进行操作，这时CPU就可以执行其他任务并与通道并行工作，直到I/O操作完成；当通道发出I/O操作结束中断时，处理器才响应并停止当前工作，转向I/O操作结束事件。\n"});index.add({'id':181,'href':'/interview/docs/fromwork/spring/design-partten/','title':"设计模式",'content':"设计模式  代理模式：AOP 单例模式：默认 Bean 为单例 工厂模式：BeanFactory IOC：依赖倒置 or 依赖注入 MVC：spring web 模版方法模式：JdbcTemplate  "});index.add({'id':182,'href':'/interview/docs/offer/reOrderArray/','title':"调整数组顺序使奇数位于偶数前面",'content':"题目 牛客网\n输入一个整数数组，实现一个函数来调整该数组中数字的顺序，使得所有的奇数位于数组的前半部分，所有的偶数位于数组的后半部分，并保证奇数和奇数，偶数和偶数之间的相对位置不变。\n解题思路  需要保证排序的稳定性 采用冒泡算法进行排序  public void reOrderArray(int[] array) { if (array.length \u0026lt;= 1) { return; } for (int i = array.length - 1; i \u0026gt;= 0; i--) { for (int j = i; j \u0026lt; array.length - 1; j++) { if (array[j] % 2 == 0 \u0026amp;\u0026amp; array[j + 1] % 2 == 1) swap(array, j, j + 1); } } } private void swap(int[] array, int a, int b) { int t = array[a]; array[a] = array[b]; array[b] = t; } "});index.add({'id':183,'href':'/interview/docs/basic/algo/skip_list/','title':"跳跃表",'content':"跳跃表 跳跃列表是一种数据结构。它允许快速查询一个有序连续元素的数据链表。跳跃列表的平均查找和插入时间复杂度都是 O(log n) ，优于普通队列的 O(n)。\n快速查询是通过维护一个多层次的链表，且每一层链表中的元素是前一层链表元素的子集。一开始时，算法在最稀疏的层次进行搜索，直至需要查找的元素在该层两个相邻的元素中间。这时，算法将跳转到下一个层次，重复刚才的搜索，直到找到需要查找的元素为止。跳过的元素的方法可以是 随机性选择 或 确定性选择，其中前者更为常见。\n在查找目标元素时，从顶层列表、头元素起步。算法沿着每层链表搜索，直至找到一个大于或等于目标的元素，或者到达当前层列表末尾。如果该元素等于目标元素，则表明该元素已被找到；如果该元素大于目标元素或已到达链表末尾，则退回到当前层的上一个元素，然后转入下一层进行搜索。\n跳跃列表不像平衡树等数据结构那样提供对最坏情况的性能保证：由于用来建造跳跃列表采用随机选取元素进入更高层的方法，在小概率情况下会生成一个不平衡的跳跃列表（最坏情况例如最底层仅有一个元素进入了更高层，此时跳跃列表的查找与普通列表一致）。但是在实际中它通常工作良好，随机化平衡方案也比平衡二叉查找树等数据结构中使用的确定性平衡方案容易实现。跳跃列表在并行计算中也很有用：插入可以在跳跃列表不同的部分并行地进行，而不用对数据结构进行全局的重新平衡。\n跳跃表插入一个元素：\n实现 因为跳跃列表中的元素可以在多个列表中，所以每个元素可以有多于一个指针。跳跃列表的插入和删除的实现与普通的链表操作类似，但高层元素必须在进行多个链表中进行插入或删除。\npackage io.github.hadyang.leetcode.algo; import lombok.Getter; import lombok.Setter; import java.util.Arrays; import java.util.Random; /** * @author haoyang.shi */ public class SkipList\u0026lt;K extends Comparable\u0026lt;K\u0026gt;, V\u0026gt; { @Getter @Setter static final class Node\u0026lt;K extends Comparable\u0026lt;K\u0026gt;, V\u0026gt; { private K key; private V value; private Node\u0026lt;K, V\u0026gt; up, down, pre, next; Node(K key, V value) { this.key = key; this.value = value; } @Override public String toString() { return \u0026quot;Node{\u0026quot; + \u0026quot;key=\u0026quot; + key + \u0026quot;, value=\u0026quot; + value + \u0026quot;, hashcode=\u0026quot; + hashCode() + \u0026quot;, up=\u0026quot; + (up == null ? \u0026quot;null\u0026quot; : up.hashCode()) + \u0026quot;, down=\u0026quot; + (down == null ? \u0026quot;null\u0026quot; : down.hashCode()) + \u0026quot;, pre=\u0026quot; + (pre == null ? \u0026quot;null\u0026quot; : pre.hashCode()) + \u0026quot;, next=\u0026quot; + (next == null ? \u0026quot;null\u0026quot; : next.hashCode()) + '}'; } } private Node\u0026lt;K, V\u0026gt; head;//k,v都是NULL private Integer levels = 0; private Integer length = 0; private Random random = new Random(System.currentTimeMillis()); public SkipList() { createNewLevel(); } public void put(K key, V value) { if (key == null || value == null) { return; } Node\u0026lt;K, V\u0026gt; newNode = new Node\u0026lt;\u0026gt;(key, value); insertNode(newNode); } private void insertNode(Node\u0026lt;K, V\u0026gt; newNode) { Node\u0026lt;K, V\u0026gt; curNode = findNode(newNode.getKey()); if (curNode.getKey() == null) { insertNext(curNode, newNode); } else if (curNode.getKey().compareTo(newNode.getKey()) == 0) { //update curNode.setValue(newNode.getValue()); return; } else { insertNext(curNode, newNode); } int currentLevel = 1; Node\u0026lt;K, V\u0026gt; oldTop = newNode; while (random.nextInt(100) \u0026lt; 50) { Node\u0026lt;K, V\u0026gt; newTop = new Node\u0026lt;\u0026gt;(newNode.getKey(), null); if (currentLevel \u0026gt;= levels) { createNewLevel(); } while (curNode.getPre() != null \u0026amp;\u0026amp; curNode.getUp() == null) { curNode = curNode.getPre(); } if (curNode.getUp() == null) { continue; } curNode = curNode.getUp(); Node\u0026lt;K, V\u0026gt; curNodeNext = curNode.getNext(); curNode.setNext(newTop); newTop.setPre(curNode); newTop.setDown(oldTop); oldTop.setUp(newTop); newTop.setNext(curNodeNext); oldTop = newTop; currentLevel++; } } private void createNewLevel() { Node\u0026lt;K, V\u0026gt; newHead = new Node\u0026lt;\u0026gt;(null, null); if (this.head == null) { this.head = newHead; this.levels++; return; } this.head.setUp(newHead); newHead.setDown(this.head); this.head = newHead; this.levels++; } private void insertNext(Node\u0026lt;K, V\u0026gt; curNode, Node\u0026lt;K, V\u0026gt; newNode) { Node\u0026lt;K, V\u0026gt; curNodeNext = curNode.getNext(); newNode.setNext(curNodeNext); if (curNodeNext != null) { curNodeNext.setPre(newNode); } curNode.setNext(newNode); newNode.setPre(curNode); this.length++; } public V get(K key) { Node\u0026lt;K, V\u0026gt; node = findNode(key); if (key.equals(node.getKey())) { return node.getValue(); } return null; } private Node\u0026lt;K, V\u0026gt; findNode(K key) { Node\u0026lt;K, V\u0026gt; curNode = this.head; for (; ; ) { while (curNode.getNext() != null \u0026amp;\u0026amp; curNode.getNext().getKey().compareTo(key) \u0026lt;= 0) { curNode = curNode.getNext(); } if (curNode.getDown() != null) { curNode = curNode.getDown(); } else { break; } } return curNode; } public void print() { Node\u0026lt;K, V\u0026gt; curI = this.head; String[][] strings = new String[levels][length + 1]; for (String[] string : strings) { Arrays.fill(string, \u0026quot;0\u0026quot;); } while (curI.getDown() != null) { curI = curI.getDown(); } System.out.println(\u0026quot;levels:\u0026quot; + levels + \u0026quot;_\u0026quot; + \u0026quot;length:\u0026quot; + length); int i = 0; while (curI != null) { Node\u0026lt;K, V\u0026gt; curJ = curI; int j = levels - 1; while (curJ != null) { strings[j][i] = String.valueOf(curJ.getKey()); if (curJ.getUp() == null) { break; } curJ = curJ.getUp(); j--; } if (curI.getNext() == null) { break; } curI = curI.getNext(); i++; } for (String[] string : strings) { System.out.println(Arrays.toString(string)); } } public static void main(String[] args) { SkipList\u0026lt;Integer, String\u0026gt; skipList = new SkipList\u0026lt;\u0026gt;(); skipList.put(2, \u0026quot;B\u0026quot;); skipList.put(1, \u0026quot;A\u0026quot;); skipList.put(3, \u0026quot;C\u0026quot;); skipList.print(); System.out.println(skipList.get(2)); } } "});index.add({'id':184,'href':'/interview/docs/java/operator/','title':"运算符",'content':"运算符优先级 优先级从上到下依次递减，最上面具有最高的优先级，逗号操作符具有最低的优先级。\n相同优先级中，按结合顺序计算。大多数运算是从左至右计算，只有三个优先级是从右至左结合的，它们是单目运算符、条件运算符、赋值运算符。\n基本的优先级需要记住：\n 指针最优，单目运算优于双目运算。如正负号。 先乘除（模），后加减。 先算术运算，后移位运算，最后位运算。请特别注意：1 \u0026lt;\u0026lt; 3 + 2 \u0026amp; 7等价于 (1 \u0026lt;\u0026lt; (3 + 2)) \u0026amp; 7. 逻辑运算最后计算。  优先级表    运算符 结合性     [ ] . ( ) (方法调用) 从左向右   ! ~ ++ -- +(一元运算) -(一元运算) 从右向左   * / % 从左向右   + -　 从左向右   \u0026lt;\u0026lt; \u0026gt;\u0026gt; \u0026gt;\u0026gt;\u0026gt; 从左向右   \u0026lt; \u0026lt;= \u0026gt; \u0026gt;= instanceof 从左向右   == != 从左向右   \u0026amp; 从左向右   ^ 从左向右   | 从左向右   \u0026amp;\u0026amp; 从左向右   || 从左向右   ?: 从右向左   = += -= *= /= %= \u0026amp;= |= ^= \u0026lt;\u0026lt;= \u0026gt;\u0026gt;= \u0026gt;\u0026gt;= 从右向左   , 从左到右     无符号右移运算符 \u0026gt;\u0026gt;\u0026gt;，无符号右移的规则只记住一点：忽略了符号位扩展，0 补最高位。无符号右移规则和右移运算是一样的，只是填充时不管左边的数字是正是负都用 0 来填充，无符号右移运算只针对负数计算，因为对于正数来说这种运算没有意义。无符号右移运算符 \u0026gt;\u0026gt;\u0026gt; 只是对 32 位和 64 位的值有意义\n "});index.add({'id':185,'href':'/interview/docs/basic/database/join/','title':"连接",'content':"连接 在数据库原理中，关系运算包含 选择、投影、连接 这三种运算。相应的在SQL语句中也有表现，其中Where子句作为选择运算，Select子句作为投影运算，From子句作为连接运算。\n连接运算是从两个关系的笛卡尔积中选择属性间满足一定条件的元组，在连接中最常用的是等值连接和自然连接。\n 等值连接：关系R、S,取两者笛卡尔积中属性值相等的元组，不要求属性相同。比如 R.A=S.B 自然连接（内连接）：是一种特殊的等值连接，它要求比较的属性列必须是相同的属性组，并且把结果中重复属性去掉。  -- 关系R -- +----+--------+ -- | A | B | C | -- +----+--------+ -- | a1 | b1 | 5 | -- | a1 | b2 | 6 | -- | a2 | b3 | 8 | -- | a2 | b4 | 12| -- +----+--------+  -- 关系S -- +----+----+ -- | B | E | -- +----+----+ -- | b1 | 3 | -- | b2 | 7 | -- | b3 | 10 | -- | b3 | 2 | -- | b5 | 2| -- +----+----+ 自然连接 R \u0026amp; S的结果为：\n-- +----+----+-----+----+ -- | A | B | C | E | -- +----+----+-----+----+ -- | a1 | b1 | 5 | 3 | -- | a1 | b2 | 6 | 7 | -- | a2 | b3 | 8 | 10 | -- | a2 | b3 | 8 | 2 | -- +----+----+-----+----+ 两个关系在做自然连接时，选择两个关系在公共属性上值相等的元组构成新的关系。此时关系R中某些元组有可能在S中不存在公共属性上相等的元组，从而造成R中这些元组在操作时被舍弃，同样，S中某些元组也可能被舍弃。这些舍弃的元组被称为 悬浮元组。\n如果把悬浮元组也保存在结果中，而在其他属性上置为NULL，那么这种连接就成为 外连接，如果只保留左边关系R中的悬浮元组就叫做 左外连接（左连接），如果只保留右边关系S中的悬浮元组就叫做 右外连接（右连接）。\nJoin Join 用于多表中字段之间的联系，语法如下：\n... FROM table1 INNER|LEFT|RIGHT JOIN table2 ON conditiona -- table1:左表；table2:右表。 JOIN 按照功能大致分为如下三类：\n INNER JOIN（内连接，或等值连接）：取得两个表中存在连接匹配关系的记录。 LEFT JOIN（左连接）：取得左表（table1）完全记录，即是右表（table2）并无对应匹配记录。 RIGHT JOIN（右连接）：与 LEFT JOIN 相反，取得右表（table2）完全记录，即是左表（table1）并无匹配对应记录。  在下面的示例中使用以下数据：\nmysql\u0026gt; select A.id,A.name,B.name from A,B where A.id=B.id; -- +----+-----------+-------------+ -- | id | name | name | -- +----+-----------+-------------+ -- | 1 | Pirate | Rutabaga | -- | 2 | Monkey | Pirate | -- | 3 | Ninja | Darth Vader | -- | 4 | Spaghetti | Ninja | -- +----+-----------+-------------+ -- 4 rows in set (0.00 sec) Inner Join 内连接，也叫等值连接，inner join产生同时符合A和B的一组数据。\nmysql\u0026gt; select * from A inner join B on A.name = B.name; -- +----+--------+----+--------+ -- | id | name | id | name | -- +----+--------+----+--------+ -- | 1 | Pirate | 2 | Pirate | -- | 3 | Ninja | 4 | Ninja | -- +----+--------+----+--------+ Left Join mysql\u0026gt; select * from A left join B on A.name = B.name; -- 或者：select * from A left outer join B on A.name = B.name;  -- +----+-----------+------+--------+ -- | id | name | id | name | -- +----+-----------+------+--------+ -- | 1 | Pirate | 2 | Pirate | -- | 2 | Monkey | NULL | NULL | -- | 3 | Ninja | 4 | Ninja | -- | 4 | Spaghetti | NULL | NULL | -- +----+-----------+------+--------+ -- 4 rows in set (0.00 sec) left join，（或left outer join:在Mysql中两者等价，推荐使用left join）左连接从左表(A)产生一套完整的记录，与匹配的记录(右表(B))。如果没有匹配，右侧将包含null。\n如果想只从左表(A)中产生一套记录，但不包含右表(B)的记录，可以通过设置where语句来执行，如下：\nmysql\u0026gt; select * from A left join B on A.name=B.name where A.id is null or B.id is null; -- +----+-----------+------+------+ -- | id | name | id | name | -- +----+-----------+------+------+ -- | 2 | Monkey | NULL | NULL | -- | 4 | Spaghetti | NULL | NULL | -- +----+-----------+------+------+ -- 2 rows in set (0.00 sec) 根据上面的例子可以求差集，如下：\nSELECT * FROM A LEFT JOIN B ON A.name = B.name WHERE B.id IS NULL union SELECT * FROM A right JOIN B ON A.name = B.name WHERE A.id IS NULL; -- +------+-----------+------+-------------+ -- | id | name | id | name | -- +------+-----------+------+-------------+ -- | 2 | Monkey | NULL | NULL | -- | 4 | Spaghetti | NULL | NULL | -- | NULL | NULL | 1 | Rutabaga | -- | NULL | NULL | 3 | Darth Vader | -- +------+-----------+------+-------------+  union ：用于合并多个 select 语句的结果集，并去掉重复的值。 union all ：作用和 union 类似，但不会去掉重复的值。\n Cross join 交叉连接，得到的结果是两个表的乘积，即笛卡尔积。\n实际上，在 MySQL 中（仅限于 MySQL） CROSS JOIN 与 INNER JOIN 的表现是一样的，在不指定 ON 条件得到的结果都是笛卡尔积，反之取得两个表完全匹配的结果。\nINNER JOIN 与 CROSS JOIN 可以省略 INNER 或 CROSS 关键字，因此下面的 SQL 效果是一样的：\n... FROM table1 INNER JOIN table2 ... FROM table1 CROSS JOIN table2 ... FROM table1 JOIN table2 "});index.add({'id':186,'href':'/interview/docs/offer/FindGreatestSumOfSubArray/','title':"连续子数组的最大和",'content':"题目 牛客网\n例如:{6,-3,-2,7,-15,1,2,2},连续子向量的最大和为8(从第0个开始,到第3个为止)。给一个数组，返回它的最大连续子序列的和，你会不会被他忽悠住？(子向量的长度至少是1)\n解题思路 通过动态规划计算最大和，$$f(i)$$ 定义为以第 $$i$$ 个数字结尾的子数组的最大和，那么 $$max(f(i))$$ 就有以下公式：\n$$ max(f(i))=\\begin{cases} num[i] \u0026amp; i=0 or f(i)\u0026lt;0\\\nnum[i]+f(i) \u0026amp; i\\ne0 and f(i)\u0026gt;0 \\end{cases} $$\npublic int FindGreatestSumOfSubArray(int[] array) { if (array == null || array.length == 0) { return 0; } int max = array[0]; int sum = 0; for (int a : array) { if (sum + a \u0026gt; a) { sum += a; } else { sum = a; } if (sum \u0026gt; max) { max = sum; } } return max; } "});index.add({'id':187,'href':'/interview/docs/offer/reConstructBinaryTree/','title':"重建二叉树",'content':"[](https://www.nowcoder.com/practice/8a19cbe657394eeaac2f6ea9b0f6fcf6?tpId=13\u0026amp;tqId=11157\u0026amp;tPage=1\u0026amp;rp=1\u0026amp;ru=%2Fta%2Fcoding-interviews\u0026amp;qru=%2Fta%2Fcoding-interviews%2Fquestion-ranking)\n输入某二叉树的前序遍历和中序遍历的结果，请重建出该二叉树。假设输入的前序遍历和中序遍历的结果中都不含重复的数字。例如输入前序遍历序列{1,2,4,7,3,5,6,8}和中序遍历序列{4,7,2,1,5,3,8,6}，则重建二叉树并返回。\n解题思路  通过前序遍历找到 root 节点 那么在 中序遍历中 root 节点的左侧则是左子树，右侧是右子树 依次类推，递归生成节点的左子树和右子树 构建过程由下往上  public TreeNode reConstructBinaryTree(int[] pre, int[] in) { Map\u0026lt;Integer, Integer\u0026gt; preIndex = new HashMap\u0026lt;\u0026gt;(); for (int i = 0; i \u0026lt; pre.length; i++) { preIndex.put(pre[i], i); } return buildTree(preIndex, in, 0, in.length - 1); } private TreeNode buildTree(Map\u0026lt;Integer, Integer\u0026gt; preIndex, int[] in, int start, int end) { if (start == end) { return new TreeNode(in[start]); } int indexOfRoot = start; for (int i = start; i \u0026lt;= end; i++) { if (preIndex.get(in[i]) \u0026lt; preIndex.get(in[indexOfRoot])) { indexOfRoot = i; } } TreeNode root = new TreeNode(in[indexOfRoot]); if (start \u0026lt;= indexOfRoot - 1) root.left = buildTree(preIndex, in, start, indexOfRoot - 1); if (indexOfRoot + 1 \u0026lt;= end) root.right = buildTree(preIndex, in, indexOfRoot + 1, end); return root; } "});index.add({'id':188,'href':'/interview/docs/offer/FindKthToTail/','title':"链表中倒数第",'content':"题目 牛客网\n输入一个链表，输出该链表中倒数第k个结点。\n解题思路  两个指针，快指针先走 k 步，然后慢指针在向前移动，当快指针遍历结束，慢指针指向倒数第 k 个节点 需要考虑倒数 k 个节点不存在的情况  public ListNode FindKthToTail(ListNode head, int k) { if (head == null) { return null; } ListNode cursor = head; ListNode cursorK = head; int i = 0; while (cursorK != null) { cursorK = cursorK.next; if (i \u0026gt;= k) { cursor = cursor.next; } i++; } if (i \u0026lt; k) { return null; } return cursor; } "});index.add({'id':189,'href':'/interview/docs/offer/EntryNodeOfLoop/','title':"链表中环的入口结点",'content':"题目 给一个链表，若其中包含环，请找出该链表的环的入口结点，否则，输出null。\n解题思路  首先通过 快慢指针（快：每次走两步；慢：每次走一步）确定是否有环 当有环时，再从头节点出发，与快指针按 相同速度 向前移动，当 cursor = fast 则找到环入口  public ListNode EntryNodeOfLoop(ListNode pHead) { if (pHead == null || pHead.next == null) return null; ListNode fast = pHead, slow = pHead; while (fast.next != null) { slow = slow.next; fast = fast.next.next; if (fast == slow) break; } if (fast != slow) return null; ListNode cursor = pHead; while (cursor != fast) { cursor = cursor.next; fast = fast.next; } return cursor; } "});index.add({'id':190,'href':'/interview/docs/offer/mirror-tree/','title':"镜像二叉树",'content':"题目 镜像二叉树\n操作给定的二叉树，将其变换为源二叉树的镜像。\n输入描述:\n二叉树的镜像定义：源二叉树 8 / \\ 6 10 / \\ / \\ 5 7 9 11 镜像二叉树 8 / \\ 10 6 / \\ / \\ 11 9 7 5 解题思路  从上到下进行左右节点交换  public void Mirror(TreeNode root) { if (root == null) return; TreeNode temp = root.left; root.left = root.right; root.right = temp; Mirror(root.left); Mirror(root.right); } "});index.add({'id':191,'href':'/interview/docs/java/collection/','title':"集合框架",'content':"集合框架 Java集合框架提供了数据持有对象的方式，提供了对数据集合的操作。Java集合框架位于java.util包下，主要有三个大类：Collection、Map接口以及对集合进行操作的工具类。\nCollection  List  ArrayList：线程不同步。默认初始容量为10，当数组大小不足时增长率为当前长度的50%。 Vector：线程同步。默认初始容量为10，当数组大小不足时增长率为当前长度的100%。它的同步是通过Iterator方法加synchronized实现的。 Stack：线程同步。继承自Vector，添加了几个方法来完成栈的功能。 LinkedList：线程不同步。双端队列形式。   Set：Set是一种不包含重复元素的Collection，Set最多只有一个null元素。  HashSet：线程不同步，内部使用HashMap进行数据存储，提供的方法基本都是调用HashMap的方法，所以两者本质是一样的。集合元素可以为NULL。 NavigableSet：添加了搜索功能，可以对给定元素进行搜索：小于、小于等于、大于、大于等于，放回一个符合条件的最接近给定元素的 key。 TreeSet：线程不同步，内部使用NavigableMap操作。默认元素“自然顺序”排列，可以通过Comparator改变排序。 EnumSet：线程不同步。内部使用Enum数组实现，速度比HashSet快。只能存储在构造函数传入的枚举类的枚举值。    Map  HashMap：线程不同步。根据key的hashcode进行存储，内部使用静态内部类Node的数组进行存储，默认初始大小为16，每次扩大一倍。当发生Hash冲突时，采用拉链法（链表）。可以接受为null的键值(key)和值(value)。JDK 1.8中：当单个桶中元素个数大于等于8时，链表实现改为红黑树实现；当元素个数小于6时，变回链表实现。由此来防止hashCode攻击。 LinkedHashMap：保存了记录的插入顺序，在用Iterator遍历LinkedHashMap时，先得到的记录肯定是先插入的. 也可以在构造时用带参数，按照应用次数排序。在遍历的时候会比HashMap慢，不过有种情况例外，当HashMap容量很大，实际数据较少时，遍历起来可能会比LinkedHashMap慢，因为LinkedHashMap的遍历速度只和实际数据有关，和容量无关，而HashMap的遍历速度和他的容量有关。 TreeMap：线程不同步，基于 *红黑树- （Red-Black tree）的NavigableMap 实现，能够把它保存的记录根据键排序,默认是按键值的升序排序，也可以指定排序的比较器，当用Iterator 遍历TreeMap时，得到的记录是排过序的。 HashTable：线程安全，HashMap的迭代器(Iterator)是fail-fast迭代器。HashTable不能存储NULL的key和value。 ConcurrentHashmap：支持并发操作的 Hash 表，ConcurrentHashmap 具有和 HashTable 同样的功能，并且具有相应的方法。即使所有操作都是线程安全的，但是并不需要进行加锁。  工具类  Collections、Arrays：集合类的一个工具类/帮助类，其中提供了一系列静态方法，用于对集合中元素进行排序、搜索以及线程安全等各种操作。 Comparable、Comparator：一般是用于对象的比较来实现排序，两者略有区别。   类设计者没有考虑到比较问题而没有实现Comparable接口。这是我们就可以通过使用Comparator，这种情况下，我们是不需要改变对象的。 一个集合中，我们可能需要有多重的排序标准，这时候如果使用Comparable就有些捉襟见肘了，可以自己继承Comparator提供多种标准的比较器进行排序。     "});index.add({'id':192,'href':'/interview/docs/java/oop/','title':"面向对象基础",'content':"面向对象基础 面向对象三要素：封装、继承、多态\n 封装：封装的意义，在于明确标识出允许外部使用的所有成员函数和数据项，或者叫接口。 继承：  继承基类的方法，并做出自己的扩展； 声明某个子类兼容于某基类（或者说，接口上完全兼容于基类），外部调用者可无需关注其差别（内部机制会自动把请求派发dispatch到合适的逻辑）。   多态：基于对象所属类的不同，外部对同一个方法的调用，实际执行的逻辑不同。很显然，多态实际上是依附于继承的第二种含义的。  多态 方法签名：方法名 + 参数列表(参数类型、个数、顺序)\n重写 子类重写父类方法，只有实例方法可以被重写，重写后的方法必须仍为实例方法。成员变量和静态方法都不能被重写，只能被隐藏。\n重写实例方法：超类Parent中有实例方法A，子类child定义了与A 相同签名和子集返回类型 的实例方法B，子类对象ChildObj只能调用自己的实例方法B。\n 方法的重写（override）两同两小一大原则：\n   方法名相同，参数类型相同    子类返回类型小于等于父类方法返回类型    子类抛出异常小于等于父类方法抛出异常    子类访问权限大于等于父类方法访问权限   注意：\n  不能重写static静态方法。(形式上可以写，但本质上不是重写，属于下面要讲的隐藏)\n  重写方法可以改变其它的方法修饰符，如final,synchronized,native。不管被重写方法中有无final修饰的参数，重写方法都可以增加、保留、去掉这个参数的 final 修饰符(参数修饰符不属于方法签名)。\n  重载 在同一个类中，有多个方法名相同，参数列表不同（参数个数不同，参数类型不同），与方法的返回值无关，与权限修饰符无关。编译器通过对方法签名的识别即可静态编译出不同的方法。这也是java中重载与重写的区别之一。\n 重载只是一种语言特性，与多态无关，与面向对象也无关。多态是为了实现接口重用。\n Java中方法是可以和类名同名的，和构造方法唯一的区别就是，构造方法没有返回值。\n隐藏 隐藏与覆盖在形式上极其类似(语法规则)，但有着本质的区别：只有成员变量(不管是不是静态)和静态方法可以被隐藏。\n成员变量 超类 Parent 中有成员变量 A ，子类 Child 定义了与 A 同名的成员变量 B ，子类对象 ChildObj 调用的是自己的成员变量 B。如果把子类对象 ChildObj 转换为超类对象 ParentObj ，ParentObj 调用的是超类的成员变量 A ！\n  隐藏成员变量时，只要同名即可，可以更改变量类型(无论基本类型还是隐藏类型)\n  不能隐藏超类中的 private 成员变量，换句话说，只能隐藏可以访问的成员变量。\n  隐藏超类成员变量 A 时，可以降低或提高子类成员变量B的访问权限，只要A不是 private。\n  隐藏成员变量与是否静态无关！静态变量可以隐藏实例变量，实例变量也可以隐藏静态变量。\n  可以隐藏超类中的final成员变量。\n  静态方法 超类 Parent 有静态方法 A ，子类 Child 定义了与 A 相同签名和子集返回类型 的静态方法 B ，子类对象 ChildObj 调用的是自己的静态方法 B 。如果把子类对象 ChildObj 转换为超类对象 ParentObj ，ParentObj 调用的是超类的静态方法 A ！\n 隐藏后的方法必须仍为静态方法\n "});index.add({'id':193,'href':'/interview/docs/fromwork/mybatis/question/','title':"面试题",'content':"面试题  #{}是预编译处理，${}是字符串替换。 Mybatis在处理#{}时，会将sql中的#{}替换为?号，调用PreparedStatement的set方法来赋值； Mybatis在处理${}时，就是把${}替换成变量的值。 使用#{}可以有效的防止SQL注入，提高系统安全性。 通常一个Xml映射文件，都会写一个Dao接口与之对应，请问，这个Dao接口的工作原理是什么？Dao接口里的方法，参数不同时，方法能重载吗？ Dao接口，就是人们常说的Mapper接口，接口的全限名，就是映射文件中的namespace的值，接口的方法名，就是映射文件中MappedStatement的id值，接口方法内的参数，就是传递给sql的参数。Mapper接口是没有实现类的，当调用接口方法时，接口全限名+方法名拼接字符串作为key值，可唯一定位一个MappedStatement，举例：com.mybatis3.mappers.StudentDao.findStudentById，可以唯一找到namespace为com.mybatis3.mappers.StudentDao下面id = findStudentById的MappedStatement。在Mybatis中，每一个\u0026lt;select\u0026gt;、\u0026lt;insert\u0026gt;、\u0026lt;update\u0026gt;、\u0026lt;delete\u0026gt;标签，都会被解析为一个MappedStatement对象。 Dao接口里的方法，是不能重载的，因为是全限名+方法名的保存和寻找策略。 Dao接口的工作原理是JDK动态代理，Mybatis运行时会使用JDK动态代理为Dao接口生成代理proxy对象，代理对象proxy会拦截接口方法，转而执行MappedStatement所代表的sql，然后将sql执行结果返回。 参考连接 Mybatis 的常见面试题\n"});index.add({'id':194,'href':'/interview/docs/java/questions/','title':"面试题",'content':"面试题 如何用数组实现队列？ 用数组实现队列时要注意 溢出 现象，这时我们可以采用循环数组的方式来解决，即将数组收尾相接。使用front指针指向队列首位，tail指针指向队列末位。\n 内部类访问局部变量的时候，为什么变量必须加上final修饰？ 因为生命周期不同。局部变量在方法结束后就会被销毁，但内部类对象并不一定，这样就会导致内部类引用了一个不存在的变量。\n所以编译器会在内部类中生成一个局部变量的拷贝，这个拷贝的生命周期和内部类对象相同，就不会出现上述问题。\n但这样就导致了其中一个变量被修改，两个变量值可能不同的问题。为了解决这个问题，编译器就要求局部变量需要被final修饰，以保证两个变量值相同。\n在JDK8之后，编译器不要求内部类访问的局部变量必须被final修饰，但局部变量值不能被修改（无论是方法中还是内部类中），否则会报编译错误。利用javap查看编译后的字节码可以发现，编译器已经加上了final。\n long s = 499999999 * 499999999 在上面的代码中，s的值是多少？ 根据代码的计算结果，s的值应该是-1371654655，这是由于Java中右侧值的计算默认是int类型。\n NIO相关，Channels、Buffers、Selectors NIO(Non-blocking IO)为所有的原始类型提供(Buffer)缓存支持，字符集编码解码解决方案。 Channel ：一个新的原始I/O 抽象。 支持锁和内存映射文件的文件访问接口。提供多路(non-bloking) 非阻塞式的高伸缩性网络I/O 。\n   IO NIO     面向流 面向缓冲   阻塞IO 非阻塞IO   无 选择器    流与缓冲 Java NIO和IO之间第一个最大的区别是，IO是面向流的，NIO是面向缓冲区的。 Java IO面向流意味着每次从流中读一个或多个字节，直至读取所有字节，它们没有被缓存在任何地方。此外，它不能前后移动流中的数据。如果需要前后移动从流中读取的数据，需要先将它缓存到一个缓冲区。\nJava NIO的缓冲导向方法略有不同。数据读取到一个它稍后处理的缓冲区，需要时可在缓冲区中前后移动。这就增加了处理过程中的灵活性。但是，还需要检查是否该缓冲区中包含所有您需要处理的数据。而且，需确保当更多的数据读入缓冲区时，不要覆盖缓冲区里尚未处理的数据。\n阻塞与非阻塞IO Java IO的各种流是阻塞的。这意味着，当一个线程调用read() 或 write()时，该线程被阻塞，直到有一些数据被读取，或数据完全写入。该线程在此期间不能再干任何事情了。 Java NIO的非阻塞模式，是线程向某通道发送请求读取数据，仅能得到目前可用的数据，如果目前没有数据可用时，就什么都不会获取，当然它不会保持线程阻塞。所以直至数据变的可以读取之前，该线程可以继续做其他的事情。 非阻塞写也是如此。所以一个单独的线程现在可以管理多个输入和输出通道。\n选择器（Selectors） Java NIO 的 选择器允许一个单独的线程来监视多个输入通道，你可以注册多个通道使用一个选择器，然后使用一个单独的线程来“选择”通道：这些通道里已经有可以处理的输入，或者选择已准备写入的通道。这种选择机制，使得一个单独的线程很容易来管理多个通道。\n 反射的用途 Java反射机制可以让我们在编译期(Compile Time)之外的运行期(Runtime)检查类，接口，变量以及方法的信息。反射还可以让我们在运行期实例化对象，调用方法，通过调用get/set方法获取变量的值。同时我们也可以通过反射来获取泛型信息，以及注解。还有更高级的应用\u0026ndash;动态代理和动态类加载（ClassLoader.loadclass()）。\n下面列举一些比较重要的方法：\n getFields：获取所有 public 的变量。 getDeclaredFields：获取所有包括 private , protected 权限的变量。 setAccessible：设置为 true 可以跳过Java权限检查，从而访问private权限的变量。 getAnnotations：获取注解，可以用在类和方法上。  获取方法的泛型参数：\nmethod = Myclass.class.getMethod(\u0026quot;setStringList\u0026quot;, List.class); Type[] genericParameterTypes = method.getGenericParameterTypes(); for(Type genericParameterType : genericParameterTypes){ if(genericParameterType instanceof ParameterizedType){ ParameterizedType aType = (ParameterizedType) genericParameterType; Type[] parameterArgTypes = aType.getActualTypeArguments(); for(Type parameterArgType : parameterArgTypes){ Class parameterArgClass = (Class) parameterArgType; System.out.println(\u0026quot;parameterArgClass = \u0026quot; + parameterArgClass); } } } 动态代理：\n//Main.java public static void main(String[] args) { HelloWorld helloWorld=new HelloWorldImpl(); InvocationHandler handler=new HelloWorldHandler(helloWorld); //创建动态代理对象 HelloWorld proxy=(HelloWorld)Proxy.newProxyInstance( helloWorld.getClass().getClassLoader(), helloWorld.getClass().getInterfaces(), handler); proxy.sayHelloWorld(); } //HelloWorldHandler.java public Object invoke(Object proxy, Method method, Object[] args) throws Throwable { Object result = null; //调用之前 doBefore(); //调用原始对象的方法 result=method.invoke(obj, args); //调用之后 doAfter(); return result; } 通过反射获取方法注解的参数：\nClass aClass = TheClass.class; Annotation[] annotations = aClass.getAnnotations(); for(Annotation annotation : annotations){ if(annotation instanceof MyAnnotation){ MyAnnotation myAnnotation = (MyAnnotation) annotation; System.out.println(\u0026quot;name: \u0026quot; + myAnnotation.name()); System.out.println(\u0026quot;value: \u0026quot; + myAnnotation.value()); } }   非静态内部类能定义静态方法吗？ public class OuterClass{ private static float f = 1.0f; class InnerClass{ public static float func(){return f;} } } 以上代码会出现编译错误，因为只有静态内部类才能定义静态方法。\n Lock 和 Synchronized 有什么区别？  使用方法的区别  - **Synchronized**：在需要同步的对象中加入此控制，`synchronized`可以加在方法上，也可以加在特定代码块中，括号中表示需要锁的对象。 - **Lock**：需要显示指定起始位置和终止位置。一般使用`ReentrantLock`类做为锁，多个线程中必须要使用一个`ReentrantLock`类做为对象才能保证锁的生效。且在加锁和解锁处需要通过`lock()`和`unlock()`显示指出。所以一般会在`finally`块中写`unlock()`以防死锁。  性能的区别  `synchronized`是托管给JVM执行的，而`lock`是java写的控制锁的代码。在Java1.5中，`synchronize`是性能低效的。因为这是一个重量级操作，需要调用操作接口，导致有可能加锁消耗的系统时间比加锁以外的操作还多。相比之下使用Java提供的Lock对象，性能更高一些。但是到了Java1.6，发生了变化。`synchronize`在语义上很清晰，可以进行很多优化，有适应自旋，锁消除，锁粗化，轻量级锁，偏向锁等等。导致在Java1.6上`synchronize`的性能并不比Lock差。 - **Synchronized**：采用的是CPU悲观锁机制，即线程获得的是独占锁。独占锁意味着 **其他线程只能依靠阻塞来等待线程释放锁**。而在CPU转换线程阻塞时会引起线程上下文切换，当有很多线程竞争锁的时候，会引起CPU频繁的上下文切换导致效率很低。 - **Lock**：用的是乐观锁方式。所谓乐观锁就是，**每次不加锁而是假设没有冲突而去完成某项操作，如果因为冲突失败就重试，直到成功为止**。乐观锁实现的机制就是`CAS`操作。我们可以进一步研究`ReentrantLock`的源代码，会发现其中比较重要的获得锁的一个方法是`compareAndSetState`。这里其实就是调用的CPU提供的特殊指令。  ReentrantLock：具有更好的可伸缩性：比如时间锁等候、可中断锁等候、无块结构锁、多个条件变量或者锁投票。   float 变量如何与 0 比较？ folat类型的还有double类型的，这些小数类型在趋近于0的时候直接等于0的可能性很小，一般都是无限趋近于0，因此不能用==来判断。应该用|x-0|\u0026lt;err来判断，这里|x-0|表示绝对值，err表示限定误差。\n//用程序表示就是 fabs(x) \u0026lt; 0.00001f  如何新建非静态内部类？ 内部类在声明的时候必须是 Outer.Inner a，就像int a 一样，至于静态内部类和非静态内部类new的时候有点区别：\n Outer.Inner a = new Outer().new Inner()（非静态，先有Outer对象才能 new 内部类） Outer.Inner a = new Outer.Inner()（静态内部类）   Java标识符命名规则 可以包含：字母、数字、$、_(下划线)，不可用数字开头，不能是 Java 的关键字和保留字。\n 你知道哪些JDK中用到的设计模式？   装饰模式：java.io\n  单例模式：Runtime类\n  简单工厂模式：Integer.valueOf方法\n  享元模式：String常量池、Integer.valueOf(int i)、Character.valueOf(char c)\n  迭代器模式：Iterator\n  职责链模式：ClassLoader的双亲委派模型\n  解释器模式：正则表达式java.util.regex.Pattern\n   ConcurrentHashMap如何保证线程安全 JDK 1.7及以前：\nConcurrentHashMap允许多个修改操作并发进行，其关键在于使用了锁分离技术。它使用了多个锁来控制对hash表的不同部分进行的修改。ConcurrentHashMap内部使用段(Segment)来表示这些不同的部分，每个段其实就是一个小的hash table，它们有自己的锁。只要多个修改操作发生在不同的段上，它们就可以并发进行。\n详细参考：\nhttp://www.cnblogs.com/ITtangtang/p/3948786.html\nhttp://qifuguang.me/2015/09/10/[Java并发包学习八]深度剖析ConcurrentHashMap/\nJDK 1.8：\nSegment虽保留，但已经简化属性，仅仅是为了兼容旧版本。\n插入时使用CAS算法：unsafe.compareAndSwapInt(this, valueOffset, expect, update)。 CAS(Compare And Swap)意思是如果valueOffset位置包含的值与expect值相同，则更新valueOffset位置的值为update，并返回true，否则不更新，返回false。插入时不允许key或value为null\n与Java8的HashMap有相通之处，底层依然由“数组”+链表+红黑树；\n底层结构存放的是TreeBin对象，而不是TreeNode对象；\nCAS作为知名无锁算法，那ConcurrentHashMap就没用锁了么？当然不是，当hash值与链表的头结点相同还是会synchronized上锁，锁链表。\n i++在多线程环境下是否存在问题，怎么解决？ 虽然递增操作++i是一种紧凑的语法，使其看上去只是一个操作，但这个操作并非原子的，因而它并不会作为一个不可分割的操作来执行。实际上，它包含了三个独立的操作：读取count的值，将值加1，然后将计算结果写入count。这是一个“读取 - 修改 - 写入”的操作序列，并且其结果状态依赖于之前的状态。所以在多线程环境下存在问题。\n要解决自增操作在多线程环境下线程不安全的问题，可以选择使用Java提供的原子类，如AtomicInteger或者使用synchronized同步方法。\n new与newInstance()的区别   new是一个关键字，它是调用new指令创建一个对象，然后调用构造方法来初始化这个对象，可以使用带参数的构造器\n  newInstance()是Class的一个方法，在这个过程中，是先取了这个类的不带参数的构造器Constructor，然后调用构造器的newInstance方法来创建对象。\n   Class.newInstance不能带参数，如果要带参数需要取得对应的构造器，然后调用该构造器的Constructor.newInstance(Object \u0026hellip; initargs)方法\n  你了解哪些JDK1.8的新特性？   接口的默认方法和静态方法，JDK8允许我们给接口添加一个非抽象的方法实现，只需要使用default关键字即可。也可以定义被static修饰的静态方法。\n  对HashMap进行了改进，当单个桶的元素个数大于6时就会将实现改为红黑树实现，以避免构造重复的hashCode的攻击\n  多并发进行了优化。如ConcurrentHashMap实现由分段加锁、锁分离改为CAS实现。\n  JDK8拓宽了注解的应用场景，注解几乎可以使用在任何元素上，并且允许在同一个地方多次使用同一个注解\n  Lambda表达式\n   你用过哪些JVM参数？   Xms 堆最小值\n  Xmx 堆最大值\n  Xmn: 新生代容量\n  XX:SurvivorRatio 新生代中Eden与Surivor空间比例\n  Xss 栈容量\n  XX:PermSize 方法区初始容量\n  XX:MaxPermSize 方法区最大容量\n  XX:+PrintGCDetails 收集器日志参数\n   如何打破 ClassLoader 双亲委托？ 重写loadClass()方法。\n hashCode() \u0026amp;\u0026amp; equals() hashcode() 返回该对象的哈希码值，支持该方法是为哈希表提供一些优点，例如，java.util.Hashtable 提供的哈希表。\n在 Java 应用程序执行期间，在同一对象上多次调用 hashCode 方法时，必须一致地返回相同的整数，前提是对象上 equals 比较中所用的信息没有被修改（equals默认返回对象地址是否相等）。如果根据 equals(Object) 方法，两个对象是相等的，那么在两个对象中的每个对象上调用 hashCode 方法都必须生成相同的整数结果。\n以下情况不是必需的：如果根据 equals(java.lang.Object) 方法，两个对象不相等，那么在两个对象中的任一对象上调用 hashCode 方法必定会生成不同的整数结果。但是，程序员应该知道，为不相等的对象生成不同整数结果可以提高哈希表的性能。\n实际上，由 Object 类定义的 hashCode 方法确实会针对不同的对象返回不同的整数。（这一般是通过将该对象的内部地址转换成一个整数来实现的，但是 JavaTM 编程语言不需要这种实现技巧I。）\n  hashCode的存在主要是用于查找的快捷性，如 Hashtable，HashMap等，hashCode 是用来在散列存储结构中确定对象的存储地址的；\n  如果两个对象相同，就是适用于 equals(java.lang.Object) 方法，那么这两个对象的 hashCode 一定要相同；\n  如果对象的 equals 方法被重写，那么对象的 hashCode 也尽量重写，并且产生 hashCode 使用的对象，一定要和 equals 方法中使用的一致，否则就会违反上面提到的第2点；\n  两个对象的hashCode相同，并不一定表示两个对象就相同，也就是不一定适用于equals(java.lang.Object) 方法，只能够说明这两个对象在散列存储结构中，如Hashtable，他们“存放在同一个篮子里”。\n   Thread.sleep() \u0026amp; Thread.yield() sleep()和yield()都会释放CPU。\nsleep()使当前线程进入停滞状态，所以执行sleep()的线程在指定的时间内肯定不会执行；yield()只是使当前线程重新回到可执行状态，所以执行yield()的线程有可能在进入到可执行状态后马上又被执行。\nsleep()可使优先级低的线程得到执行的机会，当然也可以让同优先级和高优先级的线程有执行的机会；yield()只能使同优先级的线程有执行的机会。\n"});index.add({'id':195,'href':'/interview/docs/offer/PrintMatrix/','title':"顺时针打印矩阵",'content':"题目 [](https://www.nowcoder.com/practice/9b4c81a02cd34f76be2659fa0d54342a?tpId=13\u0026amp;tqId=11172\u0026amp;rp=1\u0026amp;ru=%2Fta%2Fcoding-interviews\u0026amp;qru=%2Fta%2Fcoding-interviews%2Fquestion-ranking\u0026amp;tPage=1)\n输入一个矩阵，按照从外向里以顺时针的顺序依次打印出每一个数字，例如，如果输入如下4 X 4矩阵： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 则依次打印出数字1,2,3,4,8,12,16,15,14,13,9,5,6,7,11,10.\n解题思路  通过4个指针，表示可打印区域，并对区域进行收缩 非 n*n 的矩阵，对于剩余非 4 边遍历的元素，要考虑边界  public ArrayList\u0026lt;Integer\u0026gt; printMatrix(int[][] matrix) { ArrayList\u0026lt;Integer\u0026gt; res = new ArrayList\u0026lt;\u0026gt;(); if (matrix.length == 0) { return res; } if (matrix.length == 1) { for (int i : matrix[0]) { res.add(i); } return res; } int top = 0, bottom = matrix.length - 1, left = 0, right = matrix[0].length - 1; for (; left \u0026lt;= right \u0026amp;\u0026amp; top \u0026lt;= bottom; ) { if (top == bottom) { for (int i = left; i \u0026lt;= right; i++) { res.add(matrix[top][i]); } break; } if (left == right) { for (int i = top; i \u0026lt;= bottom; i++) { res.add(matrix[i][left]); } break; } for (int p = left; p \u0026lt;= right; p++) { res.add(matrix[top][p]); } top++; for (int p = top; p \u0026lt;= bottom; p++) { res.add(matrix[p][right]); } right--; for (int p = right; p \u0026gt;= left; p--) { res.add(matrix[bottom][p]); } bottom--; for (int p = bottom; p \u0026gt;= top; p--) { res.add(matrix[p][left]); } left++; } return res; } "});index.add({'id':196,'href':'/interview/docs/offer/power/','title':"题目",'content':"牛客网\n给定一个 double 类型的浮点数 base 和 int 类型的整数 exponent 。求 base 的 exponent 次方。\n解题思路  当 n 为偶数时，$$a^n = a^{n/2} * a^{n/2}$$ 当 n 为奇数时，$$a^n = a^{n/2} * a^{n/2} * a$$ 可以利用类似斐波纳切的方式，利用递归来进行求解  public double Power(double base, int exponent) { if (base == 0) { return 0; } if (base == 1) { return 1; } int t_exponent = Math.abs(exponent); double t = PositivePower(base, t_exponent); return exponent \u0026gt; 0 ? t : 1 / t; } private double PositivePower(double base, int exponent) { if (exponent == 0) { return 1; } if (exponent == 1) { return base; } double t = PositivePower(base, exponent \u0026gt;\u0026gt; 1); t *= t; if ((exponent \u0026amp; 0x01) == 1) { t *= base; } return t; } "});index.add({'id':197,'href':'/interview/docs/architecture/concurrent/flow-control/','title':"高并发下的流量控制",'content':"高并发下的流量控制 这个时候如果不做任何保护措施，服务器就会承受很大的处理压力，请求量很高，服务器负载也很高，并且当请求超过服务器承载极限的时候，系统就会崩溃，导致所有人都不能访问。\n为了应用服务的高可用，一个常用的办法是对大流量的请求（秒杀/抢购）进行限流，拦截掉大部分请求，只允许一部分请求真正进入后端服务器，这样就可以防止大量请求造成系统压力过大导致的系统崩溃，从而保护服务正常可用。\n令牌桶(Token Bucket)、漏桶(leaky bucket)和 计数器 算法是最常用的三种限流的算法。\n限流算法 计数器 计数器限流算法也是比较常用的，主要用来限制总并发数。比如限流 qps 为 100 ，算法的实现思路就是从第一个请求进来开始计时，在接下去的 1s 内，每来一个请求，就把计数加 1 ，如果累加的数字达到了 100 ，那么后续的请求就会被全部拒绝。等到 1s 结束后，把计数恢复成 0 ，重新开始计数。\n这种实现方式有一个弊端：如果我在单位时间 1s 内的前 10ms ，已经通过了 100 个请求，那后面的 990ms ，只能眼巴巴的把请求拒绝，这种现象称为 突刺现象。\n漏桶 为了消除 突刺现象，可以采用漏桶算法实现限流，漏桶算法这个名字就很形象，算法内部有一个容器，类似生活用到的漏斗，当请求进来时，相当于水倒入漏斗，然后从下端小口慢慢匀速的流出。不管上面流量多大，下面流出的速度始终保持不变。\n不管服务调用方多么不稳定，通过漏桶算法进行限流，每 10 毫秒处理一次请求。因为处理的速度是固定的，请求进来的速度是未知的，可能突然进来很多请求，没来得及处理的请求就先放在桶里，既然是个桶，肯定是有容量上限，如果桶满了，那么新进来的请求就丢弃。\n在算法实现方面，可以 准备一个队列，用来保存请求，另外通过一个线程池定期从队列中获取请求并执行，可以一次性获取多个并发执行。\n这种算法，在使用过后也存在弊端：无法应对短时间的突发流量，同时它的优点也是可以平滑网络上的突发流量，请求可以被整形成稳定的流量。\n令牌桶 从某种意义上讲，令牌桶算法是对漏桶算法的一种改进，桶算法能够限制请求调用的速率，而令牌桶算法能够在限制调用的平均速率的同时还允许一定程度的突发调用。\n在令牌桶算法中，存在一个桶，用来存放固定数量的令牌。算法中存在一种机制，以一定的速率往桶中放令牌。每次请求调用需要先获取令牌，只有拿到令牌，才有机会继续执行，否则选择选择等待可用的令牌、或者直接拒绝。\n放令牌这个动作是持续不断的进行，如果桶中令牌数达到上限，就丢弃令牌，所以就存在这种情况，桶中一直有大量的可用令牌，这时进来的请求就可以直接拿到令牌执行，比如设置 qps 为 100 ，那么限流器初始化完成一秒后，桶中就已经有 100 个令牌了，这时服务还没完全启动好，等启动完成对外提供服务时，该限流器可以抵挡瞬时的 100 个请求。所以，只有桶中没有令牌时，请求才会进行等待，最后相当于以一定的速率执行。\n实现思路：可以 准备一个队列，用来保存令牌，另外通过一个线程池定期生成令牌放到队列中，每来一个请求，就从队列中获取一个令牌，并继续执行。\n 漏桶 VS 令牌桶：两者主要区别在于“漏桶算法”能够强行限制数据的传输速率，而“令牌桶算法”在能够限制数据的平均传输速率外，还允许某种程度的突发传输。在“令牌桶算法”中，只要令牌桶中存在令牌，那么就允许突发地传输数据直到达到用户配置的门限，所以它适合于具有突发特性的流量。\n 集群限流 Redis 请求窗口  采用redis 的计时和计数方式,在规定的时间窗口期,允许通过的最大请求数量\n 比如为了限制某个资源被每个用户或者商户的访问次数，5s 只能访问 2 次，或者一天只能调用 1000 次，这种需求，单机限流是无法实现的，这时就需要通过集群限流进行实现。\n如何实现？为了控制访问次数，肯定需要一个计数器，而且这个计数器只能保存在第三方服务，比如redis。\n大概思路：每次有相关操作的时候，就向 redis 服务器发送一个 incr 命令，比如需要限制某个用户访问 /index 接口的次数，只需要拼接用户 id 和接口名生成 redis 的 key ，每次该用户访问此接口时，只需要对这个 key 执行 incr 命令，在这个 key 带上过期时间，就可以实现指定时间的访问频率。\nNginx 限流 Nginx按请求速率限速模块使用的是漏桶算法，即能够强行保证请求的实时处理速度不会超过设置的阈值。\nNginx官方版本限制IP的连接和并发分别有两个模块：\n limit_req_zone 用来限制单位时间内的请求数，即速率限制,采用的漏桶算法 \u0026ldquo;leaky bucket\u0026rdquo;。 limit_req_conn 用来限制同一时间连接数，即并发限制。  "});index.add({'id':198,'href':'/interview/docs/basic/os/questions/','title':"Index",'content':"面试题 PE文件 PE文件的全称是Portable Executable，意为可移植的可执行的文件，常见的EXE、DLL、OCX、SYS、COM都是PE文件，PE文件是微软Windows操作系统上的程序文件（可能是间接被执行，如DLL）。\n 什么是活锁？与死锁有和区别？ 活锁指的是 任务或者执行者没有被阻塞，由于某些条件没有满足，导致一直重复尝试，失败，尝试，失败。 活锁和死锁的区别在于，处于活锁的实体是在不断的改变状态，所谓的“活”， 而处于死锁的实体表现为等待；活锁有可能自行解开，死锁则不能。\n活锁应该是一系列进程在轮询地等待某个不可能为真的条件为真。活锁的时候进程是不会blocked，这会导致耗尽CPU资源。\n为解决活锁可以引入一些随机性，例如如果检测到冲突，那么就暂停随机的一定时间进行重试。这回大大减少碰撞的可能性。典型的例子是以太网的CSMA/CD检测机制。\n 直接寻址与间接寻址？ 寻址方式就是处理器根据指令中给出的地址信息来寻找物理地址的方式，是确定本条指令的数据地址以及下一条要执行的指令地址的方法。在操作系统中分为指令寻址和操作数寻址。\n指令寻址：在内存中查找指令的方式。\n 顺序寻址方式：即采用PC计数器来计数指令的顺序； 跳跃寻址方式：下条指令的地址码不是由程序计数器给出，而是由本条指令给出。  操作数寻址：形成操作数的有效地址的方法称为操作数的寻址方式。\n 立即寻址：操作数作为指令的一部分而直接写在指令中； 直接寻址：直接寻址是一种基本的寻址方法。在指令格式的地址的字段中直接指出操作数在内存的地址。由于操作数的地址直接给出而不需要经过某种变换，所以称这种寻址方式为直接寻址方式。 简介寻址：间接寻址是相对直接寻址而言的，在间接寻址的情况下，指令地址字段中的形式地址不是操作数的真正地址，而是操作数地址的指示器，或者说此形式地址单元的内容才是操作数的有效地址。   如何从用户态切换到内核态？  程序请求系统服务，执行系统调用 程序运行期间产生中断事件，运行程序被中断，转向中断处理程序处理 程序运行时产生异常事件，运行程序被打断，转向异常处理程序。  这三种情况都是通过中断机制发生，可以说 中断和异常是用户态到内核态转换的仅有途径。\n 实时操作系统和分时操作系统的区别？  分时操作系统：多个联机用户同时适用一个计算机系统在各自终端上进行交互式会话，程序、数据和命令均在会话过程中提供，以问答方式控制程序运行。系统把处理器的时间划分为时间片轮流分配给各个连接终端。 实时操作系统：当外部时间或数据产生时，能够对其予以接受并以足够快的速度进行处理，所得结果能够在规定时间内控制生产过程或对控制对象作出快速响应，并控制所有实时任务协调的操作系统。因而，提供及时响应和高可靠性是其主要特点。实时操作系统有硬实时和软实时之分，硬实时要求在规定的时间内必须完成操作，这是在操作系统设计时保证的；软实时则只要按照任务的优先级，尽可能快地完成操作即可。我们通常使用的操作系统在经过一定改变之后就可以变成实时操作系统。  下面还要补充一个批处理操作系统：批处理是指用户将一批作业提交给操作系统后就不再干预，由操作系统控制它们自动运行。这种采用批量处理作业技术的操作系统称为批处理操作系统。批处理操作系统分为单道批处理系统和多道批处理系统。批处理操作系统不具有交互性，它是为了提高CPU的利用率而提出的一种操作系统。\n如果某个操作系统兼有批处理、分时和实时处理的全部或两种功能，我们称为通用操作系统。\n"});index.add({'id':199,'href':'/interview/docs/leetcode/zigzagLevelOrder/','title':"Index",'content':""});index.add({'id':200,'href':'/interview/docs/','title':"Docs",'content':""});})();