<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Interview</title>
    <link>https://hadyang.github.io/interview/docs/basic/database/</link>
    <description>Recent content on Interview</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-CN</language>
    <lastBuildDate>Wed, 21 Aug 2019 11:00:41 +0800</lastBuildDate>
    
	<atom:link href="https://hadyang.github.io/interview/docs/basic/database/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>InnoDB</title>
      <link>https://hadyang.github.io/interview/docs/basic/database/innodb/</link>
      <pubDate>Wed, 21 Aug 2019 11:00:41 +0800</pubDate>
      
      <guid>https://hadyang.github.io/interview/docs/basic/database/innodb/</guid>
      <description>InnoDB 数据存储  MySQL 存储格式可通过 SQL：SHOW TABLE STATUS IN {dbName} 查看
 与现有的大多数存储引擎一样，InnoDB 使用页作为磁盘管理的最小单位；数据在 InnoDB 存储引擎中都是按行存储的，每个 16KB 大小的页中可以存放 2-200 行的记录。
当 InnoDB 存储数据时，它可以使用不同的行格式进行存储；MySQL 5.7 版本支持以下格式的行存储方式：
 Antelope 是 InnoDB 最开始支持的文件格式，它包含两种行格式 Compact 和 Redundant ，它最开始并没有名字； Antelope 的名字是在新的文件格式 Barracuda 出现后才起的， Barracuda 的出现引入了两种新的行格式 Compressed 和 Dynamic ；InnoDB 对于文件格式都会向前兼容，而官方文档中也对之后会出现的新文件格式预先定义好了名字：Cheetah、Dragon、Elk 等等。
 两种行记录格式 Compact 和 Redundant 在磁盘上按照以下方式存储：
Compact 和 Redundant 格式最大的不同就是记录格式的第一个部分；在 Compact 中，行记录的第一部分倒序存放了一行数据中列的长度（Length），而 Redundant 中存的是每一列的偏移量（Offset），从总体上上看， Compact 行记录格式相比 Redundant 格式能够减少 20% 的存储空间。
行溢出数据 当 InnoDB 使用 Compact 或者 Redundant 格式存储极长的 VARCHAR 或者 BLOB 这类大对象时，我们并不会直接将所有的内容都存放在数据页节点中，而是将数据中的前 768 个字节存储在数据页中，后面会通过偏移量指向溢出页（off-page），最大768字节的作用是便于创建 前缀索引。溢出页（off-page）不存储在 B+tree 中，使用的是uncompress BLOB page，并且每个字段的溢出都是存储独享。</description>
    </item>
    
    <item>
      <title>MySql</title>
      <link>https://hadyang.github.io/interview/docs/basic/database/mysql/</link>
      <pubDate>Wed, 21 Aug 2019 11:00:41 +0800</pubDate>
      
      <guid>https://hadyang.github.io/interview/docs/basic/database/mysql/</guid>
      <description>MySql 引擎 MVCC InnoDB 支持 MVCC 来提高系统读写并发性能。InnoDB MVCC 的实现基于 Undo log，通过回滚段来构建需要的版本记录。通过 ReadView 来判断哪些版本的数据可见。同时 Purge 线程是通过 ReadView 来清理旧版本数据。
MVCC最大的优势：读不加锁，读写不冲突。在读多写少的OLTP应用中，读写不冲突是非常重要的，极大的增加了系统的并发性能
MYSQL 事务日志 事务日志可以帮助提高事务的效率。使用事务日志，存储引擎在修改表的数据时只需要修改其内存拷贝，再把该修改行为记录到持久在硬盘上的事务日志中，而不用每次都将修改的数据本身持久到磁盘。
事务日志采用的是追加的方式，因此写日志的操作是磁盘上一小块区域内的顺序I/O，而不像随机I/O需要在磁盘的多个地方移动磁头，所以采用事务日志的方式相对来说要快得多。事务日志持久以后，内存中被修改的数据在后台可以慢慢地刷回到磁盘。目前大多数存储引擎都是这样实现的，我们通常称之为预写式日志（Write-Ahead Logging），修改数据需要写两次磁盘。
如果数据的修改已经记录到事务日志并持久化，但数据本身还没有写回磁盘，此时系统崩溃，存储引擎在重启时能够自动恢复这部分修改的数据。
MySQL Innodb中跟数据持久性、原子性有关的日志，有以下几种：Redo Log、Undo Log。
回滚日志 &amp;ndash; Undo Log 想要保证事务的 原子性，就需要在异常发生时，对已经执行的操作进行回滚，而在 MySQL 中，恢复机制是通过回滚日志（undo log）实现的，所有事务进行的修改都会先记录到这个回滚日志中，然后在对数据库中的对应行进行写入。
这个过程其实非常好理解，为了能够在发生错误时撤销之前的全部操作，肯定是需要将之前的操作都记录下来的，这样在发生错误时才可以回滚。
回滚日志除了能够在发生错误或者用户执行 ROLLBACK 时提供回滚相关的信息，它还能够在整个系统发生崩溃、数据库进程直接被杀死后，当用户再次启动数据库进程时，还能够立刻通过查询回滚日志将之前未完成的事务进行回滚，这也就需要回滚日志必须先于数据持久化到磁盘上，是我们需要先写日志后写数据库的主要原因。
回滚日志并不能将数据库物理地恢复到执行语句或者事务之前的样子；它是逻辑日志，当回滚日志被使用时，它只会按照日志逻辑地将数据库中的修改撤销掉，可以理解为，我们在事务中使用的每一条 INSERT 都对应了一条 DELETE ，每一条 UPDATE 也都对应一条相反的 UPDATE 语句。
重做日志 &amp;ndash; Redo Log 与原子性一样，事务的持久性也是通过日志来实现的，MySQL 使用重做日志（redo log）实现事务的持久性，重做日志由两部分组成，一是 内存 中的重做日志缓冲区，因为重做日志缓冲区在内存中，所以它是易失的，另一个就是在 磁盘 上的重做日志文件，它是持久的。
当我们在一个事务中尝试对数据进行修改时，它会先将数据从磁盘读入内存，并更新内存中缓存的数据，然后生成一条重做日志并写入重做日志缓存，当事务真正提交时，MySQL 会将重做日志缓存中的内容刷新到重做日志文件，再将内存中的数据更新到磁盘上，图中的第 4、5 步就是在事务提交时执行的。
在 InnoDB 中，重做日志都是以 512 字节的块的形式进行存储的，同时因为块的大小与磁盘扇区大小相同，所以重做日志的写入可以保证原子性，不会由于机器断电导致重做日志仅写入一半并留下脏数据。</description>
    </item>
    
    <item>
      <title>Redis</title>
      <link>https://hadyang.github.io/interview/docs/basic/database/redis/</link>
      <pubDate>Wed, 21 Aug 2019 11:00:41 +0800</pubDate>
      
      <guid>https://hadyang.github.io/interview/docs/basic/database/redis/</guid>
      <description>Redis 线程模型 Redis 在处理网络请求是使用单线程模型，并通过 IO 多路复用来提高并发。但是在其他模块，比如：持久化，会使用多个线程。
Redis 内部使用文件事件处理器 file event handler，这个文件事件处理器是单线程的，所以 redis 才叫做单线程的模型。它采用 IO 多路复用机制同时监听多个 socket ，将产生事件的 socket 压入内存队列中，事件分派器根据 socket 上的事件类型来选择对应的事件处理器进行处理。
文件事件处理器的结构包含 4 个部分：
 多个 socket IO 多路复用程序 文件事件分派器 事件处理器（连接应答处理器、命令请求处理器、命令回复处理器）  多个 socket 可能会并发产生不同的操作，每个操作对应不同的文件事件，但是 IO 多路复用程序会监听多个 socket ，会将产生事件的 socket 放入队列中排队，事件分派器每次从队列中取出一个 socket ，根据 socket 的事件类型交给对应的事件处理器进行处理。
客户端与 redis 的一次通信过程：
为啥 redis 单线程模型也能效率这么高？  纯内存操作 核心是基于非阻塞的 IO 多路复用机制 单线程反而避免了多线程的频繁上下文切换问题  数据结构 Redis的外围由一个键、值映射的字典构成。与其他非关系型数据库主要不同在于：Redis中值的类型不仅限于 字符串，还支持如下抽象数据类型：
 List：字符串列表 Set：无序不重复的字符串集合 Soret Set：有序不重复的字符串集合 HashTable：键、值都为字符串的哈希表  值的类型决定了值本身支持的操作。Redis支持不同无序、有序的列表，无序、有序的集合间的交集、并集等高级服务器端原子操作。
持久化：  使用快照，一种半持久耐用模式。不时的将数据集以异步方式从内存以RDB格式写入硬盘。 1.</description>
    </item>
    
    <item>
      <title>SQL</title>
      <link>https://hadyang.github.io/interview/docs/basic/database/sql/</link>
      <pubDate>Wed, 21 Aug 2019 11:00:41 +0800</pubDate>
      
      <guid>https://hadyang.github.io/interview/docs/basic/database/sql/</guid>
      <description>SQL语句 CRUD CREATE TABLE CREATE TABLE `user` ( `id` INT AUTO_INCREMENT, `name` VARCHAR (20), PRIMARY KEY (`id`) );  VARCHAR记得指定长度。
 UPDATE UPDATE 表名称 SET 列名称 = 新值 WHERE 列名称 = 某值 INSERT INSERT INTO 表名称 VALUES (值1, 值2,....) INSERT INTO table_name (列1, 列2,...) VALUES (值1, 值2,....) DELETE DELETE FROM 表名称 WHERE 列名称 = 值 修改表结构 ALTER TABLE table_name add column_name datatype ALTER TABLE table_name drop COLUMN column_name ALTER TABLE table_name modify COLUMN column_name datatype MySQL SQL 查询语句执行顺序  (7) - SELECT (8) - DISTINCT &amp;lt;select_list&amp;gt; (1) - FROM &amp;lt;left_table&amp;gt; (3) - &amp;lt;join_type&amp;gt; JOIN &amp;lt;right_table&amp;gt; (2) - ON &amp;lt;join_condition&amp;gt; (4) - WHERE &amp;lt;where_condition&amp;gt; (5) - GROUP BY &amp;lt;group_by_list&amp;gt; (6) - HAVING &amp;lt;having_condition&amp;gt; (9) - ORDER BY &amp;lt;order_by_condition&amp;gt; (10 - LIMIT &amp;lt;limit_number&amp;gt;  关于 SQL 语句的执行顺序，有三个值得我们注意的地方：</description>
    </item>
    
    <item>
      <title>事务</title>
      <link>https://hadyang.github.io/interview/docs/basic/database/transaction/</link>
      <pubDate>Wed, 21 Aug 2019 11:00:41 +0800</pubDate>
      
      <guid>https://hadyang.github.io/interview/docs/basic/database/transaction/</guid>
      <description>事务 事务的特性 所谓事务，它是一个操作序列，这些操作要么都执行，要么都不执行，它是一个不可分割的工作单位。
Atomicity（原子性） 原子性是指事务是一个不可再分割的工作单位，事务中的操作要么都发生，要么都不发生。
Consistency（一致性） 一致性是指在事务开始之前和事务结束以后，数据库的完整性约束没有被破坏。这是说数据库事务不能破坏关系数据的完整性以及业务逻辑上的一致性。
Isolation（隔离性） 多个事务并发访问时，事务之间是隔离的，一个事务不应该影响其它事务运行效果。
这指的是在并发环境中，当不同的事务同时操纵相同的数据时，每个事务都有各自的完整数据空间。由并发事务所做的修改必须与任何其他并发事务所做的修改隔离。事务查看数据更新时，数据所处的状态要么是另一事务修改它之前的状态，要么是另一事务修改它之后的状态，事务不会查看到中间状态的数据。
Durability（持久性） 持久性，意味着在事务完成以后，该事务所对数据库所作的更改便持久的保存在数据库之中，并不会被回滚。
事务隔离级别 数据库是要被广大客户所共享访问的，那么在数据库操作过程中很可能出现以下几种不确定情况：
  丢失修改：两个事务T1，T2读入同一数据并修改，T2提交的结果被T1破坏了，导致T1的修改丢失。（订票系统）
  不可重复读：事务T1读取数据后，事务T2执行更新操作，使T1无法再次读取结果。
   可以通过“读锁”和“写锁”解决不可重复读。读取数据的事务将会禁止写事务（但允许读事务），写事务则禁止任何其他事务。
   读脏数据：事务T1修改某个数据并写回磁盘，事务T2读取同一数据，但T1由于某种原因撤销了，这时T1修改过的数据恢复原来的值，T2读取的数据就与数据库中的数据不一致。
  幻读：事务在操作过程中进行两次查询，第二次查询结果包含了第一次查询中未出现的数据（这里并不要求两次查询SQL语句相同）这是因为在两次查询过程中有另外一个事务插入数据造成的。
  为了避免上面出现几种情况在标准SQL规范中定义了4个事务隔离级别，不同隔离级别对事务处理不同 。
未提交读（Read Uncommitted） 未提交读(READ UNCOMMITTED)是最低的隔离级别。允许脏读(dirty reads)，但不允许更新丢失，事务可以看到其他事务“尚未提交”的修改。
提交读（Read Committed） 允许不可重复读取，但不允许脏读取。这可以通过“瞬间共享读锁”和“排他写锁”实现。读取数据的事务允许其他事务继续访问该行数据，但是未提交的写事务将会禁止其他事务访问该行。
可重复读（Repeatable Read） 禁止不可重复读取和脏读取，但是有时可能出现幻读数据。这可以通过“共享读锁”和“排他写锁”实现。读取数据的事务将会禁止写事务（但允许读事务），写事务则禁止任何其他事务。
可序列化(Serializable) 最高的隔离级别，它要求事务序列化执行，事务只能一个接着一个地执行，不能并发执行。仅仅通过“行级锁”是无法实现事务序列化的，必须通过其他机制保证新插入的数据不会被刚执行查询操作的事务访问到。
隔离级别越高，越能保证数据的完整性和一致性，但是对并发性能的影响也越大。
隔离级别的实现 数据库对于隔离级别的实现就是使用并发控制机制对在同一时间执行的事务进行控制，限制不同的事务对于同一资源的访问和更新，而最重要也最常见的并发控制机制，在这里我们将简单介绍三种最重要的并发控制器机制的工作原理。
锁 锁是一种最为常见的并发控制机制，在一个事务中，我们并不会将整个数据库都加锁，而是只会锁住那些需要访问的数据项， MySQL 和常见数据库中的锁都分为两种，共享锁（Shared）和互斥锁（Exclusive），前者也叫读锁，后者叫写锁。
读锁保证了读操作可以并发执行，相互不会影响，而写锁保证了在更新数据库数据时不会有其他的事务访问或者更改同一条记录造成不可预知的问题。
时间戳 除了锁，另一种实现事务的隔离性的方式就是通过时间戳，使用这种方式实现事务的数据库，例如 PostgreSQL 会为每一条记录保留两个字段；读时间戳中包括了所有访问该记录的事务中的最大时间戳，而记录行的写时间戳中保存了将记录改到当前值的事务的时间戳。
使用时间戳实现事务的隔离性时，往往都会使用乐观锁，先对数据进行修改，在写回时再去判断当前值，也就是时间戳是否改变过，如果没有改变过，就写入，否则，生成一个新的时间戳并再次更新数据，乐观锁其实并不是真正的锁机制，它只是一种思想，在这里并不会对它进行展开介绍。
多版本和快照隔离 通过维护多个版本的数据，数据库可以允许事务在数据被其他事务更新时对旧版本的数据进行读取，很多数据库都对这一机制进行了实现；因为 所有的读操作不再需要等待写锁的释放，所以能够显著地提升读的性能， MySQL 和 PostgreSQL 都对这一机制进行自己的实现，也就是 MVCC ，虽然各自实现的方式有所不同，MySQL 就通过提到的 undo log 实现了 MVCC，保证事务并行执行时能够不等待互斥锁的释放直接获取数据。</description>
    </item>
    
    <item>
      <title>并发控制</title>
      <link>https://hadyang.github.io/interview/docs/basic/database/concurrent/</link>
      <pubDate>Wed, 21 Aug 2019 11:00:41 +0800</pubDate>
      
      <guid>https://hadyang.github.io/interview/docs/basic/database/concurrent/</guid>
      <description>并发控制 如果数据库中的所有事务都是串行执行的，那么它非常容易成为整个应用的性能瓶颈，虽然说没法水平扩展的节点在最后都会成为瓶颈，但是串行执行事务的数据库会加速这一过程；而并发（Concurrency）使一切事情的发生都有了可能，它能够解决一定的性能问题，但是它会带来更多诡异的错误。
引入了并发事务之后，如果不对事务的执行进行控制就会出现各种各样的问题，你可能没有享受到并发带来的性能提升就已经被各种奇怪的问题折磨的欲仙欲死了。
如何控制并发是数据库领域中非常重要的问题之一，不过到今天为止事务并发的控制已经有了很多成熟的解决方案，而这些方案的原理就是这篇文章想要介绍的内容，最为常见的三种并发控制机制：
 悲观并发控制：悲观并发控制其实是最常见的并发控制机制，也就是锁 乐观并发控制：即乐观锁，乐观锁其实并不是一种真实存在的锁 多版本并发控制（MVCC）：与前两者对立的命名不同，MVCC 可以与前两者中的任意一种机制结合使用，以提高数据库的读性能  悲观并发控制 控制不同的事务对同一份数据的获取是保证数据库的一致性的最根本方法，如果我们能够让事务在同一时间对同一资源有着独占的能力，那么就可以保证操作同一资源的不同事务不会相互影响。
最简单的、应用最广的方法就是使用锁来解决，当事务需要对资源进行操作时需要先获得资源对应的锁，保证其他事务不会访问该资源后，再对资源进行各种操作；在悲观并发控制中，数据库程序对于数据被修改持悲观的态度，在数据处理的过程中都会被锁定，以此来解决竞争的问题。
读写锁 为了最大化数据库事务的并发能力，数据库中的锁被设计为两种模式，分别是 共享锁和互斥锁。当一个事务获得共享锁之后，它只可以进行读操作，所以共享锁也叫 读锁 ；而当一个事务获得一行数据的互斥锁时，就可以对该行数据进行读和写操作，所以互斥锁也叫 写锁 。
 共享锁和互斥锁除了限制事务能够执行的读写操作之外，它们之间还有『共享』和『互斥』的关系，也就是多个事务可以同时获得某一行数据的共享锁，但是互斥锁与共享锁和其他的互斥锁并不兼容
 如果当前事务没有办法获取该行数据对应的锁时就会陷入等待的状态，直到其他事务将当前数据对应的锁释放才可以获得锁并执行相应的操作。
两阶段锁协议 两阶段锁协议（2PL）是一种能够保证事务可串行化的协议，它将事务的获取锁和释放锁划分成了增长（Growing）和缩减（Shrinking）两个不同的阶段。
在增长阶段，一个事务可以获得锁但是不能释放锁；而在缩减阶段事务只可以释放锁，并不能获得新的锁，如果只看 2PL 的定义，那么到这里就已经介绍完了，但是它还有两个变种：
 Strict 2PL：事务持有的 互斥锁 必须在提交后再释放； Rigorous 2PL：事务持有的 所有锁 必须在提交后释放；  虽然 锁的使用能够为我们解决不同事务之间由于并发执行造成的问题，但是两阶段锁的使用却引入了另一个严重的问题，死锁；不同的事务等待对方已经锁定的资源就会造成死锁，我们在这里举一个简单的例子：
两个事务在刚开始时分别获取了 draven 和 beacon 资源上面的锁，然后再请求对方已经获得的锁时就会发生死锁，双方都没有办法等到锁的释放，如果没有死锁的处理机制就会无限等待下去，两个事务都没有办法完成。
预防死锁 有两种方式可以帮助我们预防死锁的出现，一种是保证事务之间的等待不会出现环，也就是事务之间的等待图应该是一张有向无环图，没有循环等待的情况或者保证一个事务中想要获得的所有资源都在事务开始时以原子的方式被锁定，所有的资源要么被锁定要么都不被锁定。
但是这种方式有两个问题，在事务一开始时很难判断哪些资源是需要锁定的，同时因为一些很晚才会用到的数据被提前锁定，数据的利用率与事务的并发率也非常的低。一种解决的办法就是按照一定的顺序为所有的数据行加锁，同时与 2PL 协议结合，在加锁阶段保证所有的数据行都是从小到大依次进行加锁的，不过这种方式依然需要事务提前知道将要加锁的数据集。
另一种预防死锁的方法就是使用抢占加事务回滚的方式预防死锁，当事务开始执行时会先获得一个时间戳，数据库程序会根据事务的时间戳决定事务应该等待还是回滚。
锁的粒度 到目前为止我们都没有对不同粒度的锁进行讨论，一直以来我们都讨论的都是数据行锁，但是在有些时候我们希望将多个节点看做一个数据单元，使用锁直接将这个数据单元、表甚至数据库锁定起来。这个目标的实现需要我们在数据库中定义不同粒度的锁：
当我们拥有了不同粒度的锁之后，如果某个事务想要锁定整个数据库或者整张表时只需要简单的锁住对应的节点就会在当前节点加上显示（explicit）锁，在所有的子节点上加隐式（implicit）锁；虽然这种不同粒度的锁能够解决父节点被加锁时，子节点不能被加锁的问题，但是我们没有办法在子节点被加锁时，立刻确定父节点不能被加锁。
在这时我们就需要引入 意向锁 来解决这个问题了，当需要给子节点加锁时，先给所有的父节点加对应的意向锁，意向锁之间是完全不会互斥的，只是用来帮助父节点快速判断是否可以对该节点进行加锁：
这里是一张引入了两种意向锁，意向共享锁 和 意向互斥锁 之后所有的锁之间的兼容关系；到这里，我们通过不同粒度的锁和意向锁加快了数据库的吞吐量。
乐观并发控制 除了悲观并发控制机制 - 锁之外，我们其实还有其他的并发控制机制，乐观并发控制（Optimistic Concurrency Control）。乐观并发控制也叫乐观锁，但是它并不是真正的锁，很多人都会误以为乐观锁是一种真正的锁，然而它只是一种并发控制的思想。
基于时间戳的协议 锁协议按照不同事务对同一数据项请求的时间依次执行，因为后面执行的事务想要获取的数据已将被前面的事务加锁，只能等待锁的释放，所以基于锁的协议执行事务的顺序与获得锁的顺序有关。在这里想要介绍的 基于时间戳的协议能够在事务执行之前先决定事务的执行顺序。</description>
    </item>
    
    <item>
      <title>索引</title>
      <link>https://hadyang.github.io/interview/docs/basic/database/index/</link>
      <pubDate>Wed, 21 Aug 2019 11:00:41 +0800</pubDate>
      
      <guid>https://hadyang.github.io/interview/docs/basic/database/index/</guid>
      <description>索引 基本概念 在数据库中，索引的含义与日常意义上的“索引”一词并无多大区别（想想小时候查字典），它是用于提高数据库表数据访问速度的数据库对象。
 索引可以避免全表扫描。多数查询可以仅扫描少量索引页及数据页，而不是遍历所有数据页。 对于非聚集索引，有些查询甚至可以不访问数据页。 聚集索引可以避免数据插入操作集中于表的最后一个数据页。 一些情况下，索引还可用于避免排序操作。  索引的存储 一条索引记录中包含的基本信息包括：键值（即你定义索引时指定的所有字段的值）+逻辑指针（指向数据页或者另一索引页）。
当你为一张空表创建索引时，数据库系统将为你分配一个索引页，该索引页在你插入数据前一直是空的。此页此时既是根结点，也是叶结点。每当你往表中插入一行数据，数据库系统即向此根结点中插入一行索引记录。当根结点满时，数据库系统大抵按以下步骤进行分裂：
 创建两个儿子结点 将原根结点中的数据近似地拆成两半，分别写入新的两个儿子结点 根结点中加上指向两个儿子结点的指针  通常状况下，由于索引记录仅包含索引字段值（以及4-9字节的指针），索引实体比真实的数据行要小许多，索引页相较数据页来说要密集许多。一个索引页可以存储数量更多的索引记录，这意味着在索引中查找时在I/O上占很大的优势，理解这一点有助于从本质上了解使用索引的优势。
索引的分类  汉语字典的正文本身就是一个聚集索引。比如，我们要查“安”字，就会很自然地翻开字典的前几页，因为“安”的拼音是“an”，而按照拼音排序汉字的字典是以英文字母“a”开头并以“z”结尾的，那么“安”字就自然地排在字典的前部。如果您翻完了所有以“a”开头的部分仍然找不到这个字，那么就说明您的字典中没有这个字；同样的，如果查“张”字，那您也会将您的字典翻到最后部分，因为“张”的拼音是“zhang”。也就是说，字典的正文部分本身就是一个目录，您不需要再去查其他目录来找到您需要找的内容。正文内容本身就是一种按照一定规则排列的目录称为“聚集索引”。
  如果您认识某个字，您可以快速地从自动中查到这个字。但您也可能会遇到您不认识的字，不知道它的发音，这时候，您就不能按照刚才的方法找到您要查的字，而需要去根据“偏旁部首”查到您要找的字，然后根据这个字后的页码直接翻到某页来找到您要找的字。但您结合“部首目录”和“检字表”而查到的字的排序并不是真正的正文的排序方法，比如您查“张”字，我们可以看到在查部首之后的检字表中“张”的页码是672页，检字表中“张”的上面是“驰”字，但页码却是63页，“张”的下面是“弩”字，页面是390页。很显然，这些字并不是真正的分别位于“张”字的上下方，现在您看到的连续的“驰、张、弩”三字实际上就是他们在非聚集索引中的排序，是字典正文中的字在非聚集索引中的映射。我们可以通过这种方式来找到您所需要的字，但它需要两个过程，先找到目录中的结果，然后再翻到您所需要的页码。
 聚集索引 表数据按照索引的顺序来存储的。对于聚集索引，叶子结点即存储了真实的数据行，不再有另外单独的数据页。在聚集索引中，叶结点也即数据结点，所有数据行的存储顺序与索引的存储顺序一致。
在一张表上只能创建一个聚集索引，因为真实数据的物理顺序只可能是一种。如果一张表没有聚集索引，那么它被称为“堆集”（Heap）。这样的表中的数据行没有特定的顺序，所有的新行将被添加的表的末尾位置。
非聚集索引 表数据存储顺序与索引顺序无关。对于非聚集索引，叶结点包含索引字段值及指向数据页数据行的逻辑指针，该层紧邻数据页，其行数量与数据表行数据量一致。
非聚集索引与聚集索引相比：
 叶子结点并非数据结点 叶子结点为每一真正的数据行存储一个“键-指针”对 叶子结点中还存储了一个指针偏移量，根据页指针及指针偏移量可以定位到具体的数据行。 类似的，在除叶结点外的其它索引结点，存储的也是类似的内容，只不过它是指向下一级的索引页的。  索引失效 索引并不是时时都会生效的，比如以下几种情况，将导致索引失效：
  如果条件中有or，即使其中有条件带索引也不会使用。 &amp;gt;要想使用or，又想让索引生效，只能将or条件中的每个列都加上索引
  对于多列索引，不是使用的第一部分，则不会使用索引。
  like查询是以%开头。
  如果列类型是字符串，那一定要在条件中将数据使用引号引用起来，否则不使用索引。
  如果 mysql 估计使用全表扫描要比使用索引快，则不使用索引。例如，使用&amp;lt;&amp;gt;、not in 、not exist，对于这三种情况大多数情况下认为结果集很大，MySQL就有可能不使用索引。
  索引设计的原则   表的某个字段值得离散度越高，该字段越适合选作索引的关键字。主键字段以及唯一性约束字段适合选作索引的关键字，原因就是这些字段的值非常离散。
  占用存储空间少的字段更适合选作索引的关键字。例如，与字符串相比，整数字段占用的存储空间较少，因此，较为适合选作索引关键字。
  存储空间固定的字段更适合选作索引的关键字。与 text 类型的字段相比， char 类型的字段较为适合选作索引关键字。</description>
    </item>
    
    <item>
      <title>连接</title>
      <link>https://hadyang.github.io/interview/docs/basic/database/join/</link>
      <pubDate>Wed, 21 Aug 2019 11:00:41 +0800</pubDate>
      
      <guid>https://hadyang.github.io/interview/docs/basic/database/join/</guid>
      <description>连接 在数据库原理中，关系运算包含 选择、投影、连接 这三种运算。相应的在SQL语句中也有表现，其中Where子句作为选择运算，Select子句作为投影运算，From子句作为连接运算。
连接运算是从两个关系的笛卡尔积中选择属性间满足一定条件的元组，在连接中最常用的是等值连接和自然连接。
 等值连接：关系R、S,取两者笛卡尔积中属性值相等的元组，不要求属性相同。比如 R.A=S.B 自然连接（内连接）：是一种特殊的等值连接，它要求比较的属性列必须是相同的属性组，并且把结果中重复属性去掉。  -- 关系R -- +----+--------+ -- | A | B | C | -- +----+--------+ -- | a1 | b1 | 5 | -- | a1 | b2 | 6 | -- | a2 | b3 | 8 | -- | a2 | b4 | 12| -- +----+--------+  -- 关系S -- +----+----+ -- | B | E | -- +----+----+ -- | b1 | 3 | -- | b2 | 7 | -- | b3 | 10 | -- | b3 | 2 | -- | b5 | 2| -- +----+----+ 自然连接 R &amp;amp; S的结果为：</description>
    </item>
    
  </channel>
</rss>